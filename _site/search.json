[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Syed Ahmad Zaki",
    "section": "",
    "text": "Thanks for dropping by!\n\nMy name is Syed Ahmad Zaki, and i currently work in Pizza Hut Global as Manager, Market Planning.\n\nSimply put, i help my global Pizza Hut Restaurant Development leaders and their franchise partners, identify where they can open their next restaurant, using data analytics and predictive modeling. It’s an exciting venture, where we marry the very best of science (data analytics) and art (my franchise partners’ deep real estate experience) for sustainable net new unit growth.\n\nOn this profile, i’ll share more about myself, as well as the projects that i’ve done thus far. As such, feel free to reach out to me on either LinkedIn or Email."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "More About Me",
    "section": "",
    "text": "I’m truly blessed to have the opportunity to work with global franchise partners and teams in helping them unearth the right locations, using spatial data analytics. Since the field of data analytics (not just spatial) is ever evolving, i’m on the constant search for new ideas, knowledge, and a yearning to expand my analytical skillsets."
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "More About Me",
    "section": "EXPERIENCE",
    "text": "EXPERIENCE\n\n\n\n\nPIZZA HUT ASIA PACIFIC FRANCHISE PTE. LTD.\n\n\n\n\nFEB 2016 – PRESENT\n\n\n\n\nDevelopment Services Manager\n\nAnalysed franchisees’ data and identified franchisees’ growth opportunities using spatial analytics and predictive modeling\nDeveloped competition tracking dashboard on PowerBI to monitor Pizza Hut APAC’s growth performance vs. competitors\nTrained franchise partners on spatial analytics and predictive modeling to improve franchisee’s in-house analytical capabilities\n\nAbout this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html#professional-profile",
    "href": "about.html#professional-profile",
    "title": "More About Me",
    "section": "",
    "text": "APAC business partner with excellent communication and interpersonal skills. Data-driven problem solver with wide-ranging cross-functional background. An avid learner in expanding analytical skillset."
  },
  {
    "objectID": "about.html#experiences",
    "href": "about.html#experiences",
    "title": "More About Me",
    "section": "EXPERIENCES",
    "text": "EXPERIENCES\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nManager, Market Planning\nPIZZA HUT GLOBAL\n\n\nKey thought partner to BMU & Franchisee development leaders on unit growth strategies\nChampion innovative thinking and continuous improvement in the market planning space (restaurant development analytics)\nScale and manage spatial consulting vendors, beyond Asia, to meet growing research demands, as a complement to in-house research studies\nEstablished future-oriented market planning solutions to fulfill growing in-house research needs\nChief cheerleader to a talented team of market planners spread globally\n\n\n\n\n\n\n\n\nJUN 2023 – PRESENT\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDevelopment Services Manager\nPIZZA HUT ASIA PACIFIC FRANCHISE PTE. LTD.\n\n\nLeveraged spatial analytics and predictive statistical modeling on large and disparate datasets through the use of R, GIS and SAS, to unlock restaurant growth insights and 1,000+ opportunities within APAC region\nDefined, identified and partnered with best-in-class spatial consulting vendor to complete growth research studies on seven major Asian countries\nDeveloped semi-automated competition tracking dashboard on PowerBI, with data sourced from web-scraping R scripts\nCo-designed and tested in-house GIS platform, using RShiny, as a GIS SaaS Proof-of-Concept platform\nCollaborated with senior management and working level teams of franchise partners to enhance restaurant growth capabilities\nDeveloped close working relations with a local university to further analytical knowledge and synergies\n\n\n\n\n\n\n\n\nFEB 2016 – MAY 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMITB Analytics Research Projects\nSINGAPORE MANAGEMENT UNIVERSITY (SMU)\n\n\nUncovering retail customer segmentation: A nuanced comparison of clustering algorithms using rough set reduced dataset\nProject partnership with Ninja Van: Improving overall shipper retention rates\nVAST Challenge 2021\n\n\n\n\n\n\n\n\nJAN 2021 - DEC 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDevelopment Specialist\nPIZZA HUT RESTAURANTS ASIA PTE. LTD.\n\n\nManaged financial reporting of Development tools and forecasting of restaurant builds across APAC\nMonitored new restaurants’ financial health against forecasted sales\nAnalysed pre-post restaurant redevelopment effectiveness\nCoordinated “to be Developed” system with Legal and Finance on contract renewal terms\n\n\n\n\n\n\n\n\nJUL 2014 - JAN 2016\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSenior Marketing Analyst / Continuous Improvement Facilitator (Finance Secondment)\nWATLOW SINGAPORE PTE. LTD.\n\n\nActive contributions in Asia’s monthly FP&A budgeting exercise\nConceptualized and established forecasting platform across Asia, for macro-overview on sales and operations planning\nPerformed financial reporting process\n\n\n\n\n\n\n\n\nAPR 2012 - JUN 2014\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarketing Analyst / Continuous Improvement Facilitator\nWATLOW SINGAPORE PTE. LTD.\n\n\nImplemented Saleslogix (CRM system) across Asia, for increased visibility on sales opportunities\nFacilitated cross-functional Lean improvement initiatives\n\n\n\n\n\n\n\n\nOCT 2010 - MAR 2012\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssistant Consultant (Intern)\nSPIRE RESEARCH & CONSULTING\n\n\nAdvised on trends and opportunities in key SG industries, for a foreign development agency\nAdvised on growth areas for a multinational automotive company in Indonesia\n\n\n\n\n\n\n\n\nJUL 2009 - DEC 2009"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "More About Me",
    "section": "EDUCATION",
    "text": "EDUCATION\n\n\n\n\nSINGAPORE MANAGEMENT UNIVERSITY (SMU)\n\n\n\n\n2022\n\n\n\n\nMaster of IT In Business (MITB), Analytics Track\n(SAS Scholarship)"
  },
  {
    "objectID": "about.html#education-certifications",
    "href": "about.html#education-certifications",
    "title": "More About Me",
    "section": "EDUCATION / CERTIFICATIONS",
    "text": "EDUCATION / CERTIFICATIONS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaster of IT in Business (MITB) (Analytics Track, SAS Scholarship)\nSINGAPORE MANAGEMENT UNIVERSITY (SMU)\n\n\n\n\n\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHigh-Potential Leaders: Accelerating Your Impact\nWHARTON EXECUTIVE EDUCATION, PHILADELPHIA\n\n\n\n\n\n\n\n2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChartered Accountant (CA), Singapore\nINSTITUTE OF SINGAPORE CHARTERED ACCOUNTANTS (ISCA), SINGAPORE\n\n\n\n\n\n\n\n2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBachelor of Business Management (BBM), OBHR and Finance\nSINGAPORE MANAGEMENT UNIVERSITY (SMU)\n\n\n\n\n\n\n\n2010"
  },
  {
    "objectID": "books.html",
    "href": "books.html",
    "title": "Books",
    "section": "",
    "text": "Scribing my book reviews here.\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "This portfolio includes the various research work i’ve done, as well as other projects on the side.\n\n\n\n\n\n\n\n\n\n\n\n\n\nUncovering Retail Customer Segmentation From Large Transaction Records: A Nuanced Comparison Of Clustering Algorithms Using Rough Set Reduced Dataset\n\n\n\nClustering\n\n\nk-Means Clustering\n\n\nFuzzy k-Means Clustering\n\n\nGaussian Mixture Model\n\n\nRough Set\n\n\nSAS Enterprise Miner\n\n\nR\n\n\n\n\n\n\n\nSyed Ahmad Zaki Bin Syed Sakaf Al-Attas\n\n\nDec 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nVAST Challenge 2021 (Mini-Challenge 2) Part 1\n\n\n\nFuzzy Matching\n\n\nR\n\n\nsf\n\n\ntmap\n\n\nVAST Challenge\n\n\n\nInvestigating the Mini-Challenge 2 of VAST Challenge 2021\n\n\n\nSyed Ahmad Zaki\n\n\nJul 26, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nVAST Challenge 2021 (Mini-Challenge 2) Part 2\n\n\n\nR\n\n\nsf\n\n\ntmap\n\n\nVAST Challenge\n\n\n\nInvestigating the Mini-Challenge 2 of VAST Challenge 2021: A Continuation\n\n\n\nSyed Ahmad Zaki\n\n\nJul 26, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nVAST Challenge 2021 (Mini-Challenge 2) Part 3\n\n\n\nR\n\n\nsf\n\n\ntmap\n\n\nVAST Challenge\n\n\n\nInvestigating the Mini-Challenge 2 of VAST Challenge 2021: Wrapping Up\n\n\n\nSyed Ahmad Zaki\n\n\nJul 26, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nDataViz Makeover 2\n\n\n\nVisualisation\n\n\nkable\n\n\n\n“Recasting Visualisation To Better Illuminate Underlying Insights (Again Using Data From Singapore’s Department Of Statistics)”\n\n\n\nSyed Ahmad Zaki\n\n\nJun 8, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nDataViz Makeover 1\n\n\n\nVisualisation\n\n\ngt\n\n\ngtExtras\n\n\n\n“Recasting Visualisation To Better Illuminate Underlying Insights (Using Data From Singapore’s Department Of Statistics)”\n\n\n\nSyed Ahmad Zaki\n\n\nMay 26, 2021\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html#professional-profile-1",
    "href": "about.html#professional-profile-1",
    "title": "More About Me",
    "section": "PROFESSIONAL PROFILE",
    "text": "PROFESSIONAL PROFILE\nAPAC business partner with excellent communication and interpersonal skills. Data-driven problem solver with wide-ranging cross-functional background. An avid learner in expanding analytical skillset."
  },
  {
    "objectID": "index.html#hi-im-syed-ahmad-zaki.",
    "href": "index.html#hi-im-syed-ahmad-zaki.",
    "title": "Syed Ahmad Zaki",
    "section": "",
    "text": "Thanks for dropping by!\nI’m currently working in Pizza Hut Asia Pacific Franchise as a Development Services Manager. Simply put, i help my regional franchise partners identify where they can open their next store, using data analytics and predictive modeling. It’s an exciting venture, where we marry the very best of science (data analytics) and art (my franchise partners’ deep real estate experience) for sustainable net new unit growth.\nOn this profile, i’ll share more about myself as well as the projects that i’ve done thus far.\nAs such, feel free to reach out to me on LinkedIn or email."
  },
  {
    "objectID": "index.html#hi-im-syed-ahmad-zaki.-1",
    "href": "index.html#hi-im-syed-ahmad-zaki.-1",
    "title": "Syed Ahmad Zaki",
    "section": "",
    "text": "Thanks for dropping by!\nI’m currently working in Pizza Hut Asia Pacific Franchise as a Development Services Manager. Simply put, i help my regional franchise partners identify where they can open their next store, using data analytics and predictive modeling. It’s an exciting venture, where we marry the very best of science (data analytics) and art (my franchise partners’ deep real estate experience) for sustainable net new unit growth.\nOn this profile, i’ll share more about myself as well as the projects that i’ve done thus far.\nAs such, feel free to reach out to me on LinkedIn or email."
  },
  {
    "objectID": "index.html#hi-there.",
    "href": "index.html#hi-there.",
    "title": "Syed Ahmad Zaki",
    "section": "",
    "text": "Thanks for dropping by!\n\nMy name is Syed Ahmad Zaki, and i currently work in Pizza Hut Global as Manager, Market Planning. Simply put, i help my global Pizza Hut Restaurant Development leaders and their regional franchise partners identify where they can open their next restaurant, using data analytics and predictive modeling. It’s an exciting venture, where we marry the very best of science (data analytics) and art (my franchise partners’ deep real estate experience) for sustainable net new unit growth.\n\nOn this profile, i’ll share more about myself, as well as the projects that i’ve done thus far. As such, feel free to reach out to me on LinkedIn or Email."
  },
  {
    "objectID": "index.html#hi-there",
    "href": "index.html#hi-there",
    "title": "Syed Ahmad Zaki",
    "section": "",
    "text": "Thanks for dropping by!\n\nMy name is Syed Ahmad Zaki, and i currently work in Pizza Hut Global as Manager, Market Planning.\n\nSimply put, i help my global Pizza Hut Restaurant Development leaders and their franchise partners, identify where they can open their next restaurant, using data analytics and predictive modeling. It’s an exciting venture, where we marry the very best of science (data analytics) and art (my franchise partners’ deep real estate experience) for sustainable net new unit growth.\n\nOn this profile, i’ll share more about myself, as well as the projects that i’ve done thus far. As such, feel free to reach out to me on either LinkedIn or Email."
  },
  {
    "objectID": "school/vast2021_1/index.html",
    "href": "school/vast2021_1/index.html",
    "title": "VAST Challenge 2021 (Mini-Challenge 2) Part 1",
    "section": "",
    "text": "Team Member:\nSyed Ahmad Zaki, Singapore Management University (SMU)\n\nStudent Team:\nYes\n\nTools Used:\nQuarto\n\nApproximately how many hours were spent working on this submission in total?\n~200 hours\n\nMay we post your submission in the Visual Analytics Benchmark Repository after VAST Challenge 2021 is complete?\nYes"
  },
  {
    "objectID": "school/vast2021_1/index.html#introduction",
    "href": "school/vast2021_1/index.html#introduction",
    "title": "VAST Challenge 2021 (Mini-Challenge 2) Part 1",
    "section": "1. Introduction",
    "text": "1. Introduction\n\n1.1 Our Mission (Should We Accept It!)\nAs a visual analytics expert assisting law enforcement, your mission is to identify which GASTech employees made which purchases and identify suspicious patterns of behavior. You must cope with uncertainties that result from missing, conflicting, and imperfect data to make recommendations for further investigation.\n\nUse visual analytics to analyze the available data and develop responses to the questions below. In addition, prepare a video that shows how you used visual analytics to solve this challenge. Submission instructions are available here. Entry forms are available for download here."
  },
  {
    "objectID": "school/vast2021_1/index.html#literature-review",
    "href": "school/vast2021_1/index.html#literature-review",
    "title": "VAST Challenge 2021 (Mini-Challenge 2) Part 1",
    "section": "2. Literature Review",
    "text": "2. Literature Review\nBefore we begin our literature review, it’s important that we start by loading all the necessary datasets provided in the VAST Challenge 2021 Mini-Challenge 2.\n\n\n\n\nCode\n# Loading all datasets and images\ncc &lt;- read_csv(\"data/cc_data.csv\")                  # Add credit card data\nloyalty &lt;- read_csv(\"data/loyalty_data.csv\")        # Add loyalty data\nmc2 &lt;- raster(\"data/MC2-tourist_modified.tif\")      # Add tif file as a raster layer\ngps &lt;- read_csv(\"data/gps.csv\")                     # Add gps data\ncar &lt;- read_csv(\"data/car-assignments.csv\")         # Add car assignments\nAbila_st &lt;- st_read(dsn = \"data\", \n                    layer = \"Abila\")\nKronos_sf &lt;- st_as_sf(st_read(dsn = \"data\", \n                              layer = \"Kronos_Island\"))\n\n\nA cursory look at the dataset reveals the following data types:\n\n\n\n\n\n\n\n\n\n\nData\nType\nDescription\n\n\n\n\nCredit Card.csv (cc)\nAspatial\nCredit card txns by timestamp, location and amt\n\n\nLoyalty.csv\nAspatial\nLoyalty card txns by date, location and amt\n\n\nCar Assignment.csv (car)\nAspatial\nCar assignment ID with individuals’ name and role\n\n\nMC2.jpg\nAspatial\nAbila’s map in jpeg format\n\n\nMC2.tif\nGeospatial\nAbila’s map in a geotiff format\n\n\nGPS.csv (gps)\nGeospatial\nGPS points (latlong) by car ID and timestamp\n\n\nAbila\nGeospatial\nAbila’s road network\n\n\nKronos Island\nGeospatial\nPolygon showing Kronos Island’s admin boundary\n\n\n\nAn in-depth look at the dataset reveals the following fields:\n\n\n\n\n\nFile Name\ncc\nloyalty\ngps\ncar\nmc2\n\n\n\n\nFile Type\ncsv\ncsv\ncsv\ncsv\npic\n\n\nCount\n1,490\n1,392\n685,169\n44\n-\n\n\nDate Format\nm/d/y\nm/d/y\nm/d/y\n-\n-\n\n\nTime Format\nh:m\n-\nh:m:s\n-\n-\n\n\nLocation\nYes\nYes\n-\n-\nYes\n\n\nPrice\nYes\nYes\n-\n-\n-\n\n\nlast4ccnum\nYes\n-\n-\n-\n-\n\n\nloyaltynum\n-\nYes\n-\n-\n-\n\n\nID\n-\n-\nYes\nYes\n-\n\n\nLatlong\n-\n-\nYes\n-\n-\n\n\nNames\n-\n-\n-\nYes\n-\n\n\nEmployment Details\n-\n-\n-\nYes\n-\n\n\n\nNot all files have the same fields. While it’s easy to merge gps and car data using its unique ID, there are no unique fields tying the cc and loyalty data together. Thus, merging both cc and loyalty data together would require some form of fuzzy joining logic. Separately, to add to the complexity, we would need to identify the various locations within the gps data, using both the cc and mc2 map.\n\nWith these dataset in mind, the following considerations would need to be addressed.\n\n2.1 Fuzzy Matching\nThere are a few ways to employ fuzzy matching in our dataset. One is to use the native adist function within R, but its processing time leaves much to be desired. The other is to use packages specifically designed for fuzzy matching. One such package that is built for speed in matching similar phrases is stringdist. It uses openMP for parallel computing to speed up its matching of unequal content. The only downside (though it’s hardly a downside) is that it requires the columns of comparison to be housed in the same dataframe. Fuzzyjoin, built on top of stringdist, allows comparison of columns housed in different dataset, and its output include a merging of both datasets.\n\nUnfortunately, deciding on the fuzzy logic package is the easy part. The harder part is to decide on the appropriate fuzzy join logic. Here’s a list of distance metrics currently supported by stringdist:\n\n\n\n\n\n\n\n\n\nMethod Name\nDescription\n\n\n\n\nosa\nOptimal string aligment, (restricted Damerau-Levenshtein distance)\n\n\nlv\nLevenshtein distance (as in R’s native adist).\n\n\ndl\nFull Damerau-Levenshtein distance.\n\n\nhamming\nHamming distance (a and b must have same nr of characters).\n\n\nlcs\nLongest common substring distance\n\n\nqgram\nq-gram distance\n\n\ncosine\ncosine distance between q-gram profiles\n\n\njaccard\nJaccard distance between q-gram profiles\n\n\njw\nJaro, or Jaro-Winkler distance\n\n\n\nOut of the above methods, osa, lv and dl seems most apt, since we’re dealing with phrases of differing lengths and are more concerned with slight edits, realignment, addition and subtraction of letters within these phrases. We’ll rely on the osa method then, since it’s a balance between finding the right edits and speed.\n\n\n2.2 Map Visualisations of GPS Data\nWhile it makes sense to convert our GPS points into spatial lines, using the furnished map pic as a basemap is less than ideal. As shown below, it’s unclear whether the GPS path coincides with the location icons shown on the map. These icons are far too large, and does not reflect whether these GPS points either stopped at or merely drove past these points. At the same time, the GPS path seem to run on a road network, that is not reflected within the furnished map pic.\n\n\n\n\nCode\ngps$Timestamp &lt;- date_time_parse(gps$Timestamp,\n                                 zone = \"UTC\",\n                                 format = \"%m/%d/%Y %H:%M:%S\") # Readjust loyalty timestamp\ngps$day &lt;- as_factor(get_day(gps$Timestamp))                   # Extract day of month and convert to factor data type\ngps$id &lt;- as_factor(gps$id)                                    # Change to factor data type\n\ngps_sf &lt;- st_as_sf(gps, \n                   coords = c(\"long\", \n                              \"lat\"), \n                   crs = 4326)                                 # Changing into a shapefile\n\ngps_path1 &lt;- gps_sf %&gt;% # Grouping gps lines according to id and day of month\n  group_by(id, day) %&gt;%\n  summarize(m = mean(Timestamp),\n            do_union = FALSE,\n            .groups = \"drop\") %&gt;%\n  st_cast(\"LINESTRING\") # Change GPS into a line\n\ntmap_mode(\"view\")\nQ2.2_mc2 &lt;- tm_shape(mc2) +\n              tm_raster(legend.show = FALSE) +\n              # tm_raster(palette = c(\"#979D9D\",\n              #                       # \"#A3A7A7\",\n              #                       # \"#B1B2B2\",\n              #                       # \"#B6B5B3\",\n              #                       # \"#B4AFA9\",\n              #                       \"#B3A9A1\")\n              #        ,alpha = NA\n              #        # ,\n              #        # saturation = 1,\n              #        # interpolate = TRUE,\n              #        # max.value = 255\n              #        ) +\n              # tm_rgb(mc2,                      # tm_rgb used to work in the previous tmap versions, but are now unable to render multiple bands in a single raster layer\n              #        r = 1,\n              #        g = 2,\n              #        b = 3,\n              #        alpha = NA,\n              #        saturation = 1,\n              #        interpolate = TRUE,\n              #        max.value = 255) +\n            tm_shape(gps_path1 %&gt;%\n                     filter(id==1)) +\n              tm_lines()\n\nQ2.2_mc2\n\n\n\n\n\n\n\n\nAt the same time, we are provided with the Abila road network. This granular road network is not reflected at all within the map pic. Taking inspiration from City University London’s entry, they made great use of the Abila road network to create their own map (reproduced below). They then coupled this road network with actual location points that they have derived from the data. We will borrow this visualisation format as a basis for our own visualisation. To bring it a step further, we will recreate this map in an interactive fashion in subsequent sections. For now, let’s prepare the necessary data."
  },
  {
    "objectID": "school/vast2021_1/index.html#introduction-1",
    "href": "school/vast2021_1/index.html#introduction-1",
    "title": "VAST Challenge 2021 (Mini-Challenge 2) Part 1",
    "section": "1. Introduction",
    "text": "1. Introduction\n\n1.1 Our Mission (Should We Accept It!)\nAs a visual analytics expert assisting law enforcement, your mission is to identify which GASTech employees made which purchases and identify suspicious patterns of behavior. You must cope with uncertainties that result from missing, conflicting, and imperfect data to make recommendations for further investigation.\n\nUse visual analytics to analyze the available data and develop responses to the questions below. In addition, prepare a video that shows how you used visual analytics to solve this challenge. Submission instructions are available here. Entry forms are available for download here."
  },
  {
    "objectID": "school/vast2021_1/index.html#literature-review-1",
    "href": "school/vast2021_1/index.html#literature-review-1",
    "title": "VAST Challenge 2021 (Mini-Challenge 2) Part 1",
    "section": "2. Literature Review",
    "text": "2. Literature Review\nBefore we begin our literature review, it’s important that we start by loading all the necessary datasets provided in the VAST Challenge 2021 Mini-Challenge 2.\n\n# Loading all datasets and image\ncc &lt;- readr::read_csv(\"data/cc_data.csv\") # Add credit card data\nloyalty &lt;- readr::read_csv(\"data/loyalty_data.csv\") # Add loyalty data\nmc2 &lt;- raster(\"data/MC2-tourist_modified.tif\") # Add tif file as a raster layer\ngps &lt;- readr::read_csv(\"data/gps.csv\") # Add gps data\ncar &lt;- readr::read_csv(\"data/car-assignments.csv\") # Add car assignments\nAbila_st &lt;- st_read(dsn = \"data\", layer = \"Abila\")\nKronos_sf &lt;- st_as_sf(st_read(dsn = \"data\", layer = \"Kronos_Island\"))\n\nA cursory look at the dataset reveals the following data types:\n\n\n\n\n\n\n\n\nData\nType\nDescription\n\n\n\n\nCredit Card.csv (cc)\nAspatial\nCredit card txns by timestamp, location and amt\n\n\nLoyalty.csv\nAspatial\nLoyalty card txns by date, location and amt\n\n\nCar Assignment.csv (car)\nAspatial\nCar assignment ID with individuals’ name and role\n\n\nMC2.jpg\nAspatial\nAbila’s map in jpeg format\n\n\nMC2.tif\nGeospatial\nAbila’s map in a geotiff format\n\n\nGPS.csv (gps)\nGeospatial\nGPS points (latlong) by car ID and timestamp\n\n\nAbila\nGeospatial\nAbila’s road network\n\n\nKronos Island\nGeospatial\nPolygon showing Kronos Island’s admin boundary\n\n\n\nAn in-depth look at the dataset reveals the following fields:\n\n\n\nFile Name\ncc\nloyalty\ngps\ncar\nmc2\n\n\n\n\nFile Type\ncsv\ncsv\ncsv\ncsv\npic\n\n\nCount\n1,490\n1,392\n685,169\n44\n-\n\n\nDate Format\nm/d/y\nm/d/y\nm/d/y\n-\n-\n\n\nTime Format\nh:m\n-\nh:m:s\n-\n-\n\n\nLocation\nYes\nYes\n-\n-\nYes\n\n\nPrice\nYes\nYes\n-\n-\n-\n\n\nlast4ccnum\nYes\n-\n-\n-\n-\n\n\nloyaltynum\n-\nYes\n-\n-\n-\n\n\nID\n-\n-\nYes\nYes\n-\n\n\nLatlong\n-\n-\nYes\n-\n-\n\n\nNames\n-\n-\n-\nYes\n-\n\n\nEmployment Details\n-\n-\n-\nYes\n-\n\n\n\nNot all files have the same fields. While it’s easy to merge gps and car data using its unique ID, there are no unique fields tying the cc and loyalty data together. Thus, merging both cc and loyalty data together would require some form of fuzzy joining logic. Separately, to add to the complexity, we would need to identify the various locations within the gps data, using both the cc and mc2 map.\nWith these dataset in mind, the following considerations would need to be addressed:\n\n2.1 Fuzzy Matching\nThere are a few ways to employ fuzzy matching in our dataset. One is to use the native adist function within R, but its processing time leaves much to be desired. The other is to use packages specifically designed for fuzzy matching. One such package that is built for speed in matching similar phrases is stringdist. It uses openMP for parallel computing to speed up its matching of unequal content. The only downside (though it’s hardly a downside) is that it requires the columns of comparison to be housed in the same dataframe. Fuzzyjoin, built on top of stringdist, allows comparison of columns housed in different dataset, and its output include a merging of both datasets.\nUnfortunately, deciding on the fuzzy logic package is the easy part. The harder part is to decide on the appropriate fuzzy join logic. Here’s a list of distance metrics currently supported by stringdist:\n\n\n\n\n\n\n\nMethod Name\nDescription\n\n\n\n\nosa\nOptimal string aligment, (restricted Damerau-Levenshtein distance)\n\n\nlv\nLevenshtein distance (as in R’s native adist).\n\n\ndl\nFull Damerau-Levenshtein distance.\n\n\nhamming\nHamming distance (a and b must have same nr of characters).\n\n\nlcs\nLongest common substring distance\n\n\nqgram\nq-gram distance\n\n\ncosine\ncosine distance between q-gram profiles\n\n\njaccard\nJaccard distance between q-gram profiles\n\n\njw\nJaro, or Jaro-Winkler distance\n\n\n\nOut of the above methods, osa, lv and dl seems most apt, since we’re dealing with phrases with differing lengths and are more concerned with slight edits, realignment, addition and subtraction of letters within these phrases. We’ll rely on the osa method since it’s a balance between finding the right edits and speed.\n\n\n2.2 Map Visualisations of GPS Data\nWhile it makes sense to convert our GPS points into spatial lines, using the furnished map pic as a basemap is less than ideal. As shown below, it’s unclear whether the GPS path coincides with the location icons shown on the map. These icons are far too large, and does not reflect whether these GPS points either stopped at or merely drove past these points. At the same time, the GPS path seem to run on a road network, that is not reflected within the furnished map pic.\n\n# gps$Timestamp &lt;- date_time_parse(gps$Timestamp,\n#                 zone = \"UTC\",\n#                 format = \"%m/%d/%Y %H:%M:%S\") # Readjust loyalty timestamp\n# gps$day &lt;- as_factor(get_day(gps$Timestamp)) # Extract day of month and convert to factor data type\n# gps$id &lt;- as_factor(gps$id) # Change to factor data type\n# \n# gps_sf &lt;- st_as_sf(gps, coords = c(\"long\", \"lat\"), crs = 4326) # Changing into a shapefile\n# \n# gps_path1 &lt;- gps_sf %&gt;% # Grouping gps lines according to id and day of month\n#   group_by(id, day) %&gt;%\n#   summarize(m = mean(Timestamp),\n#             do_union=FALSE,\n#             .groups = \"drop\") %&gt;%\n#   st_cast(\"LINESTRING\") # Change GPS into a line\n# \n# tmap_mode(\"view\")\n# Q2.2_mc2 &lt;- tm_shape(mc2) +\n#               tm_rgb(mc2, r = 1,g = 2,b = 3,\n#                    alpha = NA,\n#                    saturation = 1,\n#                    interpolate = TRUE,\n#                    max.value = 255) +\n#             tm_shape(gps_path1 %&gt;%\n#                      filter(id==1)) +\n#               tm_lines()\n# \n# Q2.2_mc2\n\nAt the same time, we are provided with the Abila road network. This granular road network is not reflected at all within the map pic. Taking inspiration from City University London’s entry, they made great use of the Abila road network to create their own map (reproduced below). They then coupled this road network with actual location points that they have derived from the data. We will borrow this visualisation format as a basis for our own visualisation. To bring it a step further, we will recreate this map in an interactive fashion in subsequent sections. For now, let’s prepare the necessary data.\n\n\n\nCity University London’s MC2 Sample Map Entry"
  },
  {
    "objectID": "school/vast2021_1/index.html#data-comprehension",
    "href": "school/vast2021_1/index.html#data-comprehension",
    "title": "VAST Challenge 2021 (Mini-Challenge 2) Part 1",
    "section": "3. Data Comprehension",
    "text": "3. Data Comprehension\nAs always, we review each dataset in greater detail. This is a necessary step in order to accurately prepare the data for subsequent use.\n\nWhile reviewing the four csv data, we immediately noticed a few potential issues:\n1. Date format within the timestamp were in a MM-DD-YYYY H:M format\n2. Katerina’s Cafe contains unique characters, which may cause downstream problems during our analysis\n3. ID and Last4CCNum are treated as regular double numbers, instead of a character type\n4. Names and roles are broken up into multiple columns within the car data\n\nThus, we address these potential issues as well as create and simplify other columns for subsequent ease in analysis.\n\n\n\n\nCode\n#--------------- Cleaning CC data ---------------\n\ncc$timestamp &lt;- date_time_parse(cc$timestamp,\n                zone = \"UTC\",\n                format = \"%m/%d/%Y  %H:%M\")              # Readjust CC timestamp\ncc[grep(\"Katerina\", cc$location),2] &lt;- \"Katerina's Cafe\" # Replace unique characters in Katerina's Cafe\ncc$last4ccnum &lt;- as_factor(cc$last4ccnum)                # Change the column format to nominal format\ncc$hour &lt;- as.numeric(format(cc$timestamp,\"%H\"))         # Create a separate column just for hours in the cc data\ncc$period &lt;- case_when(                                  # Segment hour into 5 separate periods\n  cc$hour &gt;= 21 ~ \"Late Evening 9pm to 11.59pm\",\n  cc$hour &gt;= 18 ~ \"Evening 6pm to 8.59pm\",\n  cc$hour &gt;= 12 ~ \"Afternoon 12noon to 5.59pm\",\n  cc$hour &gt;= 6 ~ \"Morning 6am to 11.59am\",\n  TRUE ~ \"Late Night 12mn to 5.59am\"\n)\n\ncc$period &lt;- factor(cc$period,                           # Order periods accordingly\n                    levels = c(\"Morning 6am to 11.59am\",\n                               \"Afternoon 12noon to 5.59pm\",\n                               \"Evening 6pm to 8.59pm\",\n                               \"Late Evening 9pm to 11.59pm\",\n                               \"Late Night 12mn to 5.59am\"))\n\ncc$dayofmonth &lt;- day(cc$timestamp)                       # Extract day of month from timestamp in a new column\ncc$dayofmonth &lt;- as_factor(cc$dayofmonth)                # Change to nominal format\ncc$weekday &lt;- wday(cc$timestamp, \n                   label = TRUE)                         # Extract day of week from timestamp in a new column\ncc &lt;- tibble::rowid_to_column(cc, \n                              \"ID\")                      # Create a numeric id column\ncc$date &lt;- as.Date(cc$timestamp)                         # Create a separate column just for dates in the cc data\ncc$concat_cc_loyalty &lt;- paste(cc$date,\n                              cc$location,\n                              cc$price)                  # Create a separate column of unique values using concatenated values in the cc data\ncc$concat_cc_spots &lt;- paste(cc$date,\n                            cc$location,\n                            cc$hour)                     # Create a second separate column of unique values using concatenated values in the cc data\ncc$ID &lt;- as_factor(cc$ID)                                # Change the column format to nominal format\n\n#--------------- Cleaning Loyalty data ---------------\n\nloyalty$timestamp &lt;- date_time_parse(loyalty$timestamp,\n                zone = \"UTC\",\n                format = \"%m/%d/%Y\")                     # Readjust loyalty timestamp\nloyalty[grep(\"Katerina\", \n             loyalty$location),\n        2] &lt;- \"Katerina's Cafe\"                          # Replace unique characters in Katerina's Cafe\nloyalty$dayofmonth &lt;- day(loyalty$timestamp)             # Extract day of month from timestamp in a new column\nloyalty$dayofmonth &lt;- as_factor(loyalty$dayofmonth)      # Change to nominal format\nloyalty$weekday &lt;- wday(loyalty$timestamp, \n                        label = TRUE)                    # Extract day of week from timestamp in a new column\nloyalty$concat_loyalty_cc &lt;- paste(loyalty$timestamp,\n                                   loyalty$location,\n                                   loyalty$price)        # Create a separate column of unique values using concatenated values in the loyalty data\nloyalty &lt;- tibble::rowid_to_column(loyalty, \n                                   \"ID\")                 # Create a numeric id column\nloyalty$ID &lt;- as_factor(loyalty$ID)                      # Change the column format to nominal format\n\n#--------------- Cleaning Car Assignment data ---------------\n\ncar$CarID &lt;- as_factor(car$CarID)                        # Change the column format to nominal format\ncar$FullName &lt;- paste(car$FirstName,\n                      car$LastName, \n                      sep = \" \")                         # Create new column with combined first and last name\ncar$RoleNName &lt;- paste(car$CarID, \n                       car$CurrentEmploymentTitle, \n                       car$FullName, \n                       sep = \" \")                        # Create new column with combined ID, Role and Full Name\n\n#--------------- Cleaning GPS data ---------------\n\ngps$date &lt;- as_date(gps$Timestamp)                       # Create a separate column just for dates in the gps data\ngps$hour &lt;- hour(gps$Timestamp)                          # Create a separate column just for hours in the gps data\ngps$period &lt;- case_when(                                 # Segment hour into 5 separate periods\n  gps$hour &gt;= 21 ~ \"Late Evening 9pm to 11.59pm\",\n  gps$hour &gt;= 18 ~ \"Evening 6pm to 8.59pm\",\n  gps$hour &gt;= 12 ~ \"Afternoon 12noon to 5.59pm\",\n  gps$hour &gt;= 6 ~ \"Morning 6am to 11.59am\",\n  TRUE ~ \"Late Night 12mn to 5.59am\"\n)\ngps$period &lt;- factor(gps$period,                         # Order periods accordingly\n                     levels = c(\"Morning 6am to 11.59am\",\n                                \"Afternoon 12noon to 5.59pm\",\n                                \"Evening 6pm to 8.59pm\",\n                                \"Late Evening 9pm to 11.59pm\",\n                                \"Late Night 12mn to 5.59am\"))\ngps$dayofmonth &lt;- day(gps$Timestamp)                     # Extract day of month from timestamp in a new column\ngps$weekday &lt;- wday(gps$Timestamp, \n                    label = TRUE)                        # Extract day of week from timestamp in a new column"
  },
  {
    "objectID": "school/vast2021_1/index.html#data-preparation",
    "href": "school/vast2021_1/index.html#data-preparation",
    "title": "VAST Challenge 2021 (Mini-Challenge 2) Part 1",
    "section": "4. Data Preparation",
    "text": "4. Data Preparation\n\n4.1 Combining Both Credit Card and Loyalty Data Using Fuzzy Join (OSA)\nWe will now attempt to find matching rows between the cc and loyalty data.\n\n\n\n\nCode\ncc_loyalty &lt;- cc %&gt;%                                     # Create a new df showing matches with a max distance difference of 1\n  stringdist_inner_join(loyalty,\n                        by = c(\"concat_cc_loyalty\" = \"concat_loyalty_cc\"),\n                        method = \"osa\",\n                        max_dist = 1,\n                        distance_col = \"distance\")\n\ncc_loyalty_1 &lt;- cc_loyalty %&gt;%                           # Isolate best matching cc and loyalty with more than 2 counts\n    group_by(last4ccnum,\n             loyaltynum) %&gt;%\n    count() %&gt;%\n    filter(n&gt;2) %&gt;%\n    ungroup()\n\ncc_loyalty_duplicate_cc &lt;- cc_loyalty_1 %&gt;%              # Extract duplicates in cc data\n  filter(cc_loyalty_1$last4ccnum == cc_loyalty_1$last4ccnum[duplicated(cc_loyalty_1$last4ccnum)])\n\ncc_loyalty_duplicate_loyalty &lt;- subset(cc_loyalty_1,loyaltynum == \"L6267\" | \n                                       loyaltynum == \"L3288\") # Extract duplicates in loyalty data\n\ncc_loyalty_1$type &lt;- \"unique\"                            # Creating a new type column with 'unique' as value\ncc_loyalty_1[which(cc_loyalty_1$last4ccnum == cc_loyalty_duplicate_cc$last4ccnum),\n             4] &lt;- \"duplicate\"                           # Identifying duplicate in type column\ncc_loyalty_1[which(cc_loyalty_1$loyaltynum ==  \"L6267\" |\n                   cc_loyalty_1$loyaltynum ==  \"L3288\"),\n             4] &lt;- \"duplicate\"                           # Identifying duplicate in type column\n\n\n\n\n4.2 Combining Both GPS and Car Assignment Data\nFirst, we will merge the GPS data with the car assignments. Next, we will isolate GPS points, that have been stationary for at least 10 mins.\n\n\n\n\nCode\ngps_name &lt;- left_join(gps,\n                      car, \n                      by = c(\"id\" = \"CarID\"))            # Merge car assignments to gps data\ngps_name$Timestamp &lt;- as.POSIXct(gps_name$Timestamp, \n                                 format = \"%m/%d/%Y  %H:%M:%S\", \n                                 tz = \"GMT\")             # Timestamp switching to month-day-year format\ngps_name &lt;- gps_name[with(gps_name,\n                          order(id,\n                                Timestamp)),\n                     ]                                   # Sort first by ID in ascending order and then Timestamp by oldest to newest\ngps_name &lt;- gps_name %&gt;%                                 # Add running number in the first column\n  mutate(No = 1:n()) %&gt;%\n  dplyr::select(No, \n                everything())\ngps_name &lt;- gps_name %&gt;%                                 # Create additional column indicating time taken from previous timestamp for same ID\n    mutate(Delta = Timestamp - lag(Timestamp, \n                                   default = first(Timestamp)))\ngps_name$Delta &lt;- as.numeric(gps_name$Delta)             # Convert Delta column to numeric format\ngps_name$Delta_Hours &lt;- round(gps_name$Delta / 60 / 60, \n                              1)                         # Create column to convert Delta seconds into hours with one decimal place\n\nrm(gps)                                                  # Remove unused earlier dataset\n\nspots &lt;- gps_name %&gt;%                                    # Filtering out stationary gps coordinates of more than 10 mins\n  filter(Delta &gt; 600)\nspots$No &lt;- rep(1:2965, \n                times = 1)                               # Redo running number in the first column\n\n\n\n\n4.3 Identifying Stationary GPS Points\nNext, using the map and other data sources, we identify the locations of each of these stationary GPS points. Through a visual inspection of the map, credit card and loyalty data, we found 66 unique locations.\n\n\n\n\nCode\nspots$Location &lt;- 1 # Create a Location column\nspots &lt;- spots %&gt;% mutate( # Create additional column with location names based on latlong\n  Location = case_when(\n    between(lat, 36.05092013, 36.05102938) &\n      between(long, 24.82586806, 24.82598723)  ~ \"Abila Airport\",                                       # 35 features\n    between(lat, 36.07434876, 36.07443715) &\n      between(long, 24.84592966, 24.84598782)  ~ \"Abila Scrapyard\",                                     # 4 features\n    between(lat, 36.06342076, 36.06349309) &\n      between(long, 24.85096457, 24.85103679)  ~ \"Abila Zacharo\",                                       # 66 features\n    between(lat, 36.07712237, 36.07715385) &\n      between(long, 24.87617634, 24.87621582)  ~ \"Ahaggo Museum\",                                       # 5 features\n    between(lat, 36.07522801, 36.07530344) &\n      between(long, 24.85626503, 24.85634849)  ~ \"Albert's Fine Clothing\",                              # 20 features\n    between(lat, 36.08172086, 36.08182543) &\n      between(long, 24.85086882, 24.85096705)  ~ \"Bean There Done That\",                                # 46 features\n    between(lat, 36.05402149, 36.05413903) &\n      between(long, 24.90116515, 24.90128202)  ~ \"Brew've Been Served\",                                 # 106 features\n    between(lat, 36.07332048, 36.07336116) &\n      between(long, 24.86416419, 24.86420583)  ~ \"Brewed Awakenings\",                                   # 36 features\n    between(lat, 36.06582469, 36.065941) &\n      between(long, 24.90097567, 24.90108865)  ~ \"20 Building Control Stenig's Home\",                   # 20 features\n    between(lat, 36.05851786, 36.05860144) &\n      between(long, 24.8808655, 24.88092654)  ~ \"Carlyle Chemical Inc.\",                                # 30 features\n    between(lat, 36.07818062, 36.07821857) &\n      between(long, 24.87211555, 24.8721508)  ~ \"4 CFO Ingrid's Home\",                                  # 27 features\n    between(lat, 36.07682044, 36.07685752) &\n      between(long, 24.8658641, 24.86589901)  ~ \"10 CIO Ada's Home\",                                    # 35 features\n    between(lat, 36.0721156, 36.07215701) &\n      between(long, 24.87458425, 24.8746267)  ~ \"32 COO Orhan's Home\",                                  # 29 features\n    between(lat, 36.07062423, 36.07073983) &\n      between(long, 24.89517609, 24.89526281)  ~ \"Chostus Hotel\",                                       # 11 features\n    between(lat, 36.05462322, 36.05469486) &\n      between(long, 24.88977034, 24.88983886)  ~ \"Coffee Cameleon\",                                     # 29 features\n    between(lat, 36.08954231, 36.08962196) &\n      between(long, 24.86066508, 24.8607611)  ~ \"Desafio Golf Course\",                                  # 10 features\n    between(lat, 36.07292088, 36.07301365) &\n      between(long, 24.88396447, 24.88405897)  ~ \"26 Drill Site Manager Marin's Home\",                  # 26 features\n    between(lat, 36.08442031, 36.08449538) &\n      between(long, 24.86416741, 24.8642387)  ~ \"7 Drill Technician Elsa's Home\",                       # 25 features\n    between(lat, 36.08424703, 36.08432477) &\n      between(long, 24.8563809, 24.8564637)  ~ \"9 Drill Technician Gustav's Home\",                      # 13 features\n    between(lat, 36.0726185, 36.07380904) &\n      between(long, 24.87510166, 24.87613744)  ~ \"28 Drill Technician Isande's Home\",                   # 26 features\n    between(lat, 36.06922564, 36.06931513) &\n      between(long, 24.88416486, 24.88426267)  ~ \"27 Drill Technician Kare's Home\",                     # 20 features\n    between(lat, 36.08542073, 36.08550845) &\n      between(long, 24.86036422, 24.86045943)  ~ \"2 Engineer Lars's Home\",                              # 37 features\n    between(lat, 36.08664252, 36.08672442) &\n      between(long, 24.85756416, 24.85766744)  ~ \"3 Engineer Felix's Home\",                             # 22 features\n    between(lat, 36.07622023, 36.07626546) &\n      between(long, 24.87466429, 24.87471053)  ~ \"35 Environmental Safety Advisor Willem's Home\",       # 33 features\n    between(lat, 36.07212045, 36.07213193) &\n      between(long, 24.84132949, 24.84134818)  ~ \"Frank's Fuel\",                                        # 2 features\n    between(lat, 36.05492145, 36.05503511) &\n      between(long, 24.90176782, 24.90188061)  ~ \"Frydos Autosupply n' More\",                           # 29 features\n    between(lat, 36.04802098, 36.04805422) &\n      between(long, 24.87956497, 24.87957691)  ~ \"GasTech\",                                             # 738 features\n    between(lat, 36.05970763, 36.05981097) &\n      between(long, 24.85797552, 24.8580772)  ~ \"Gelatogalore\",                                         # 47 features\n    between(lat, 36.06034564, 36.06043016) &\n      between(long, 24.85646426, 24.85657454)  ~ \"General Grocer\",                                      # 12 features\n    between(lat, 36.05572125, 36.05584094) &\n      between(long, 24.90246542, 24.90258487)  ~ \"Guy's Gyros\",                                         # 143 features\n    between(lat, 36.06362146, 36.06371539) &\n      between(long, 24.88586605, 24.88595859)  ~ \"Hallowed Grounds\",                                    # 70 features\n    between(lat, 36.07660977, 36.07669909) &\n      between(long, 24.85756408, 24.85764247)  ~ \"Hippokampos\",                                         # 155 features\n    between(lat, 36.08412146, 36.08420924) &\n      between(long, 24.85896842, 24.85905081)  ~ \"11 Hydraulic Technician Axel's Home\",                 # 23 features\n    between(lat, 36.08782802, 36.08793196) &\n      between(long, 24.85627136, 24.8563725)  ~ \"19 Hydraulic Technician Vira's Home\",                  # 24 features\n    between(lat, 36.06641679, 36.06650723) &\n      between(long, 24.88256875, 24.88265687)  ~ \"1 IT Helpdesk Nils's Home\",                           # 31 features\n    between(lat, 36.06729646, 36.06736745) &\n      between(long, 24.87788423, 24.87795559)  ~ \"5 IT Technician Isak's Home\",                         # 21 features\n    between(lat, 36.06722012, 36.06731624) &\n      between(long, 24.8858687, 24.88596759)  ~ \"8 IT Technician Lucas's Home\",                         # 23 features\n    between(lat, 36.06749651, 36.0675518) &\n      between(long, 24.87330651, 24.873366)  ~ \"Jack's Magical Beans\",                                  # 31 features\n    between(lat, 36.06582037, 36.06584879) &\n      between(long, 24.85236427, 24.85241027)  ~ \"Kalami Kafenion\",                                     # 47 features\n    between(lat, 36.05442247, 36.05453641) &\n      between(long, 24.89986596, 24.89998054)  ~ \"Katerina's Cafe\",                                     # 158 features\n    between(lat, 36.05292229, 36.05296701) &\n      between(long, 24.84936915, 24.84941679)  ~ \"Kronos Capital\",                                      # 6 features\n    between(lat, 36.06582196, 36.06587998) &\n      between(long, 24.8497762, 24.84983936)  ~ \"Kronos Mart\",                                          # 9 features\n    between(lat, 36.06523446, 36.06534083) &\n      between(long, 24.83307421, 24.83318494)  ~ \"Kronos Pipe and Irrigation\",                          # 7 features\n    between(lat, 36.06402993, 36.06410072) &\n      between(long, 24.84137818, 24.84144338)  ~ \"Maximum Iron and Steel\",                              # 9 features\n    between(lat, 36.05840347, 36.05849041) &\n      between(long, 24.88546548, 24.88553455)  ~ \"Nationwide Refinery\",                                 # 41 features\n    between(lat, 36.05859158, 36.05859887) &\n      between(long, 24.85790261, 24.85799357)  ~ \"Octavio's Office Supplies\",                           # 3 features\n    between(lat, 36.05192066, 36.05197575) &\n      between(long, 24.87076418, 24.87082137)  ~ \"Ouzeri Elian\",                                        # 67 features\n    between(lat, 36.06764972, 36.06775002) &\n      between(long, 24.90243213, 24.9025445)  ~ \"34 Perimeter Control Edvard's Home\",                   # 20 features\n    between(lat, 36.06324941, 36.06330782) &\n      between(long, 24.85226894, 24.8523291)  ~ \"Roberts and Sons\",                                     # 9 features\n    between(lat, 36.05942407, 36.05952152) &\n      between(long, 24.89476557, 24.8948649)  ~ \"Shared Home A - 6 Linnea 25 Kanon 29 Bertrand\",        # 72 features\n    between(lat, 36.06332304, 36.06343537) &\n      between(long, 24.89607033, 24.89617856)  ~ \"Shared Home B - 14 Lidelse 18 Birgitta 21 Hennie\",    # 60 features\n    between(lat, 36.06242283, 36.06253955) &\n      between(long, 24.89877023, 24.89888179)  ~ \"Shared Home C - 17 Sven 24 Minke 33 Brand\",           # 68 features\n    between(lat, 36.05842222, 36.05853828) &\n      between(long, 24.90096522, 24.90107874)  ~ \"Shared Home D - 22 Adra 23 Varja 30 Felix\",           # 73 features\n    between(lat, 36.0603222, 36.06044736) &\n      between(long, 24.90556693, 24.90569385)  ~ \"Shared Home E - 13 Inga 15 Loreto 16 Isia 21 Hennie\", # 85 features\n    between(lat, 36.05282139, 36.05288367) &\n      between(long, 24.86856868, 24.8686314)  ~ \"Shoppers' Delight\",                                    # 17 features\n    between(lat, 36.06772112, 36.06784956) &\n      between(long, 24.89906521, 24.89917328)  ~ \"12 Site Control Hideki's Home\",                       # 21 features\n    between(lat, 36.05409586, 36.05420832) &\n      between(long, 24.90806584, 24.90817838)  ~ \"Stewart and Sons Fabrication\",                        # 36 features\n    between(lat, 36.06774029, 36.06776587) &\n      between(long, 24.87148791, 24.87150031)  ~ \"U-Pump\",                                              # 4 features\n    between(lat, 36.05012433, 36.05021624) &\n      between(long, 24.9003978, 24.90047475)  ~ \"Anonymous Site 1\",                                     # 6 features\n    between(lat, 36.06314781, 36.06324321) &\n      between(long, 24.90010823, 24.90018668)  ~ \"Anonymous Site 2\",                                    # 7 features\n    between(lat, 36.05893131, 36.05900826) &\n      between(long, 24.89277554, 24.89284962)  ~ \"Anonymous Site 3\",                                    # 7 features\n    between(lat, 36.08061881, 36.08067087) &\n      between(long, 24.84681621, 24.84688282)  ~ \"Anonymous Site 4\",                                    # 7 features\n    between(lat, 36.06944928, 36.0695319) &\n      between(long, 24.84147082, 24.84157048)  ~ \"Anonymous Site 5\",                                    # 8 features\n    between(lat, 36.05149231, 36.05253234) &\n      between(long, 24.87495168, 24.87611086)  ~ \"Anonymous Site 6\",                                    # 13 features\n    between(lat, 36.05543848, 36.05657576) &\n      between(long, 24.86618187, 24.86735)  ~ \"Anonymous Site 7\",                                       # 7 features\n    between(lat, 36.07099038, 36.07200089) &\n      between(long, 24.86869468, 24.86985682)  ~ \"Anonymous Site 8\",                                    # 10 features\n    ))\n\nspots$concat_spots_cc &lt;- paste(spots$date,\n                               spots$Location,\n                               spots$hour)         # Create a separate column of unique values using concatenated values in the distilled GPS data\nspots_median &lt;- spots %&gt;%                          # Extract the median lat & long coordinates of locations\n  group_by(Location) %&gt;%\n  summarise(lat.median = median(lat), \n            long.median = median(long), \n            .groups = \"drop\") %&gt;%\n  filter(!is.na(Location)) %&gt;%                     # Exclude remaining few unmatched locations\n  ungroup()\n\nspots_median &lt;- spots_median %&gt;%                   # Add additional column to classify locations into major buckets\n  mutate(Location.Type = case_when(\n    Location %in% c(\"Anonymous Site 1\",\n                    \"Anonymous Site 2\",\n                    \"Anonymous Site 3\",\n                    \"Anonymous Site 4\",\n                    \"Anonymous Site 5\",\n                    \"Anonymous Site 6\",\n                    \"Anonymous Site 7\",\n                    \"Anonymous Site 8\") ~ \"Unknown\",\n    Location %in% c(\"Bean There Done That\",\n                    \"Brew've Been Served\",\n                    \"Brewed Awakenings\",\n                    \"Coffee Cameleon\",\n                    \"Jack's Magical Beans\",\n                    \"Hallowed Grounds\") ~ \"Coffee Cafe\",\n    Location %in% c(\"Abila Zacharo\",\n                    \"Gelatogalore\",\n                    \"Guy's Gyros\",\n                    \"Hippokampos\",\n                    \"Kalami Kafenion\",\n                    \"Katerina's Cafe\",\n                    \"Ouzeri Elian\") ~ \"Food Joints\",\n    Location %in% c(\"GasTech\") ~ \"HQ\",\n    Location %in% c(\"1 IT Helpdesk Nils's Home\",\n                    \"10 CIO Ada's Home\",\n                    \"11 Hydraulic Technician Axel's Home\",\n                    \"12 Site Control Hideki's Home\",\n                    \"19 Hydraulic Technician Vira's Home\",\n                    \"2 Engineer Lars's Home\",\n                    \"20 Building Control Stenig's Home\",\n                    \"26 Drill Site Manager Marin's Home\",\n                    \"27 Drill Technician Kare's Home\",\n                    \"28 Drill Technician Isande's Home\",\n                    \"3 Engineer Felix's Home\",\n                    \"32 COO Orhan's Home\",\n                    \"34 Perimeter Control Edvard's Home\",\n                    \"35 Environmental Safety Advisor Willem's Home\",\n                    \"4 CFO Ingrid's Home\",\n                    \"5 IT Technician Isak's Home\",\n                    \"7 Drill Technician Elsa's Home\",\n                    \"8 IT Technician Lucas's Home\",\n                    \"9 Drill Technician Gustav's Home\",\n                    \"Shared Home A - 6 Linnea 25 Kanon 29 Bertrand\",\n                    \"Shared Home B - 14 Lidelse 18 Birgitta 21 Hennie\",\n                    \"Shared Home C - 17 Sven 24 Minke 33 Brand\",\n                    \"Shared Home D - 22 Adra 23 Varja 30 Felix\",\n                    \"Shared Home E - 13 Inga 15 Loreto 16 Isia 21 Hennie\") ~ \"Residential\",\n    Location %in% c(\"Abila Scrapyard\",\n                    \"Carlyle Chemical Inc.\",\n                    \"Kronos Pipe and Irrigation\",\n                    \"Maximum Iron and Steel\",\n                    \"Nationwide Refinery\",\n                    \"Stewart and Sons Fabrication\") ~ \"Industrial\",\n    Location %in% c(\"Ahaggo Museum\",\n                    \"Albert's Fine Clothing\",\n                    \"Kronos Mart\",\n                    \"Octavio's Office Supplies\",\n                    \"Shoppers' Delight\",\n                    \"General Grocer\",\n                    \"Roberts and Sons\") ~ \"Leisure & Shopping\",\n    Location %in% c(\"Abila Airport\",\n                    \"Chostus Hotel\",\n                    \"Desafio Golf Course\",\n                    \"Kronos Capital\") ~ \"Complex\",\n    Location %in% c(\"Frank's Fuel\",\n                    \"Frydos Autosupply n' More\",\n                    \"U-Pump\") ~ \"Transport\",\n    ))\nspots_median_sf &lt;- st_as_sf(spots_median, \n                            coords = c(\"long.median\", \n                                       \"lat.median\"), \n                            crs = 4326)                # Changing into a shapefile\n\n\n\n\n4.4 Create Custom Map\nUsing the identified spots, we will create our custom map using the tmap package, as well as use the Abila road network.\n\n\n\n\nCode\nAbila_st_union &lt;- st_union(Abila_st)          # Dissolve Abila road network\nAbila_st_proj &lt;- st_transform(Abila_st_union, \n                              crs = 3857)     # Transform to necessary projection\nAbila_st_buffer &lt;- st_buffer(Abila_st_proj, \n                             dist = 25, \n                             nQuadSegs = 5, ) # Create a buffer around the dissolved Abila road network\n\nrm(Abila_st,\n   Abila_st_union,\n   Abila_st_proj)                             # Remove unused earlier dataset\n\ngps_path &lt;- gps_sf %&gt;%                        # Creating a movement path\n  group_by(id, \n           day) %&gt;%\n  summarize(m = mean(Timestamp),\n              do_union=FALSE,\n              .groups = \"drop\") %&gt;%\n  left_join(dplyr::select(car,\n                          CarID,\n                          RoleNName), \n            by = c(\"id\" = \"CarID\")) %&gt;%        # Add in RoleNName column\n  ungroup() %&gt;%\n  st_cast(\"LINESTRING\")\n\nlong.sea &lt;- c(24.91075,                        # Create blue polygon as background to mimic sea\n              24.91075,\n              24.8232,\n              24.8232,\n              24.91075)\nlat.sea &lt;- c(36.09543,\n             36.0445,\n             36.0445,\n             36.09543,\n             36.09543)\nsea &lt;- data.frame(long.sea, \n                  lat.sea)\n\nrm(gps_sf,\n   long.sea,\n   lat.sea,\n   car)                                        # Remove unused earlier dataset\n\nsea_sf &lt;- st_as_sf(sea, coords = c(\"long.sea\", \"lat.sea\"))\nst_crs(sea_sf) &lt;- 4326\nsea_poly&lt;- st_sf(\n  st_cast(\n    st_combine(sea_sf$geometry),\n    \"POLYGON\"\n  ))\n\nrm(sea,\n   sea_sf)                                     # Remove unused earlier dataset\n\nKronos_sf_small &lt;- st_crop(Kronos_sf,          # Clip a smaller Kronos island around Abila\n                           c(xmin = 24.8232, \n                             xmax = 24.91075, \n                             ymin = 36.0445, \n                             ymax = 36.09543))\n\nrm(Kronos_sf)                                  # Remove unused earlier dataset\n\ntmap_mode(\"view\")\n\ncustom_tmap &lt;- tm_shape(sea_poly) +\n  tm_polygons(col=\"lightblue\") +\ntm_shape(Kronos_sf_small) +\n  tm_polygons(col = \"beige\") +\ntm_shape(Abila_st_buffer) +\n  tm_polygons(col = \"white\") +\ntm_shape(gps_path %&gt;% filter(id==2)) +\n  tm_lines(id = \"RoleNName\") +\ntm_shape(spots_median_sf) +\n  tm_dots(col = \"Location.Type\",\n          id = \"Location\",                    # Bold in group\n          popup.vars = \"Location Type:\" ==\"Location.Type\",\n          size = 0.2)\n\ncustom_tmap"
  },
  {
    "objectID": "school/vast2021_1/index.html#data-exploration-analysis",
    "href": "school/vast2021_1/index.html#data-exploration-analysis",
    "title": "VAST Challenge 2021 (Mini-Challenge 2) Part 1",
    "section": "5. Data Exploration Analysis",
    "text": "5. Data Exploration Analysis\nHere we will answer the VAST Challenge questions.\n\n5.1 Question 1 And Its Answers\nUsing just the credit and loyalty card data, identify the most popular locations, and when they are popular. What anomalies do you see? What corrections would you recommend to correct these anomalies? Please limit your answer to 8 images and 300 words.\n\n5.1.1 Popular Locations Seem To Revolve Ard Food And Beverages\nFood and beverage places, such as Brew’ve Been Served, Guy’s Gyros, Hallowed Grounds etc. seem to be the more popular locations, as highlighted in the dark grey tiles below.\n\n\n\n\nCode\ncc_calendar &lt;- cc %&gt;%\n  count(dayofmonth, \n        location)                                           # Group and tally by day of month and location\ncc_calendar$dayofmonth &lt;- as_factor(cc_calendar$dayofmonth) # CHange day of month to factor type\n\n# Create calendar heatmap using ggplot and geom_tile\nQ5.1.1 &lt;- ggplot(complete(cc_calendar, \n                          dayofmonth, \n                          location), \n                 aes(x = dayofmonth, \n                     y = location)) +\n  geom_tile(aes(fill = n), \n            color = \"white\", \n            size = 0.1) +\n  scale_fill_gradient(low = \"light grey\", \n                      high = \"black\", \n                      na.value = \"light grey\") +\n  scale_y_discrete(expand = expansion(add = 1.6),\n                   limits = rev) +\n  labs(title = \"Calendar Heatmap of Location Visit Frequency (From CC Data) By Date\",\n       subtitle = \"Food and coffee outlets seem to be the most frequented, based on credit card data\",\n       x = \"Day of Month\",\n       fill = \"Frequency Of Visit\") +\n  theme_bw() +\n  theme(axis.ticks = element_blank(),\n        panel.border = element_blank(),\n        panel.spacing = unit(0.1, \"cm\"),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        text = element_text(size=7),\n        axis.title.x = element_text(vjust = -3),\n        axis.title.y = element_blank(),\n        legend.position = \"bottom\")\n\nQ5.1.1\n\n\n\n\n\n\n\n5.1.2 Kronos Mart’s Txns Suggest Tampering\nThe txn dates of Kronos Mart differ by exactly one day, comparing either side of the credit card and loyalty data. Investigation needed to ascertain true transaction dates of Kronos Mart’s books, perhaps through receipt verification. We will compare the GPS points in section 5.2.1.\n\n\n\n\nCode\nQ5.1.2_cc &lt;- cc %&gt;%\n  filter(location == \"Kronos Mart\") %&gt;%                                # Filter out only Kronos Mart cc txns\n  dplyr::select(dayofmonth, \n                price, \n                location) %&gt;%                                          # Select only three columns\n  group_by(dayofmonth) %&gt;%                                             # Group by day of month\n  summarise(cc_data = sum(price), \n              .groups = \"drop\") %&gt;%                                    # Create cc_data column which sums prices\n  ungroup()\n\nQ5.1.2_loyalty &lt;- loyalty %&gt;%                                          # Same methodology as above but done on loyalty data\n  filter(location == \"Kronos Mart\") %&gt;%\n  dplyr::select(dayofmonth, \n                price, \n                location) %&gt;%\n  group_by(dayofmonth) %&gt;%\n  summarise(loyalty_data = sum(price), \n            .groups = \"drop\") %&gt;%\n  ungroup()\n\nQ5.1.2_combined &lt;- data.frame(dayofmonth = c(6:19))                    # Create new df with the 14 days\nQ5.1.2_combined$dayofmonth &lt;- as_factor(Q5.1.2_combined$dayofmonth)    # Change to factor type\nQ5.1.2_combined &lt;- Q5.1.2_combined %&gt;%                                 # Merge df to manipulated cc and loyalty data\n  left_join(Q5.1.2_cc, \n            by = \"dayofmonth\") %&gt;%\n  left_join(Q5.1.2_loyalty, \n            by = \"dayofmonth\")\n\nrm(Q5.1.2_cc,\n   Q5.1.2_loyalty)                                                     # Remove unused earlier dataset\n\nQ5.1.2_combined$cc_data[is.na(Q5.1.2_combined$cc_data)] &lt;- 0           # Replace NA with 0\nQ5.1.2_combined$loyalty_data[is.na(Q5.1.2_combined$loyalty_data)] &lt;- 0 # Replace NA with 0\n\nQ5.1.2_combined &lt;- melt(Q5.1.2_combined, \n                        id.vars = \"dayofmonth\", \n                        variable.name = \"source\")                      # Change from wide to long format\n\n# Create area graph on both cc and loyalty data\nQ5.1.2 &lt;- ggplot(Q5.1.2_combined, \n                 aes(dayofmonth, \n                     value, \n                     group = source)) +\n  geom_area(aes(colour = source, \n                fill = source),\n            size = 1) +\n  geom_point() +\n  geom_text(data = subset(Q5.1.2_combined, \n                          value != 0),\n            aes(label = round(value,\n                              0),\n            group = source),\n            vjust = -1,\n            size = 3) +\n  facet_grid(source~.) +\n  ylim(0,\n       500) +\n  labs(\"title\" = \"Kronos Mart's Suspicious Delayed Transactions\",\n       \"subtitle\" = \"Loyalty transactions in Kronos Mart recorded one day earlier than in credit card\") +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        panel.grid.major.y = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        legend.position = \"none\")\n\nQ5.1.2\n\n\n\n\n\n\n\n5.1.3 Unknown Locations Need To Be Verified. Existing Data Only Able To Approx. Type Of Location\nAscertaining the location of unknown locations such as Hippokampos and Abila Zacharo seem tricky, given that their location names do not describe its very nature. Thus, we’re forced to rely on their time-based transactions to approximate the nature of their locations.\n\n\n\n\nCode\n# Create ridgeline plot to see activity by location across the hours of the day\nQ5.1.3 &lt;- ggplot(cc,\n                 aes(x = hour,\n                     y = location,\n                     fill = after_stat(x)\n                     )) +\n                 geom_density_ridges_gradient(scale = 3,\n                                              rel_min_height = 0.001) +\n  scale_x_continuous(breaks = 0:24) +\n  scale_y_discrete(limits = rev) +\n  scale_fill_viridis_c(name = \"ABC\", \n                       option = \"A\") +\n  theme_ridges(font_size = 7, \n               grid = TRUE) +\n  theme(legend.position = \"none\") +\n  labs(title = \"Uncovering Location Type Beyond Ambiguous Location Names Using Credit Card Data\",\n       subtitle = \"High Noon Txns Suggests Abila Zacharo and Hippokampos As Food Outlets\")\n\nQ5.1.3\n\n\n\n\n\n\n\n5.1.4 Selected Coffee Chain Txns Occur Only At Selected Hours Within The Credit Card Data\nCoffee chains usually open for longer than just the three hours we see in the data, given the traditionally low beverage costing.\n\n\n\n\nCode\nQ5.1.4_cc &lt;- cc %&gt;%                                     # Merge cc with location type data from spots median\n  left_join(dplyr::select(spots_median,Location, \n                          Location.Type), \n            by = c(\"location\" = \"Location\")) %&gt;%\n  filter(Location.Type == \"Coffee Cafe\") %&gt;%            # Filter only Coffee Cafe location type\n  dplyr::select(location, \n                hour, \n                price) %&gt;%\n  group_by(location, \n           hour) %&gt;%\n  summarise(coffee_money = sum(price), \n            .groups = \"drop\") %&gt;%\n  ungroup() %&gt;%\n  dcast(hour ~ location, \n        value.var = \"coffee_money\")                     # Change from long to wide format\n\n\nQ5.1.4_cc$hour &lt;- as_factor(Q5.1.4_cc$hour)             # Change to factor type\n\nQ5.1.4_combined &lt;- data.frame(hour = c(1:24))           # Create new dataframe with hours\nQ5.1.4_combined$hour &lt;- as_factor(Q5.1.4_combined$hour) # Change to factor type\nQ5.1.4_combined &lt;- Q5.1.4_combined %&gt;%\n  left_join(Q5.1.4_cc, \n            by = \"hour\")\n\nrm(Q5.1.4_cc)                                           # Remove unused earlier dataset\n\nQ5.1.4_combined &lt;-melt(Q5.1.4_combined, \n                       id.vars = \"hour\", \n                       variable.name = \"coffee_place\")  # Change from wide to long format\n\n# Create a clock-like visualisation using ggplot\nQ5.1.4 &lt;- ggplot(Q5.1.4_combined, aes(hour, \n                                      value, \n                                      fill = coffee_place)) +\n  geom_bar(stat = \"identity\") +\n  coord_polar(theta = \"x\") +\n  labs(title = \"Daily CC Txns At Coffee Chains Restricted To Only Three Hours\",\n       subtitle = \"Three Coffee Chains Have CC Txns Only At Noon\") +\n  xlab(\"\") +\n  ylab(\"\") +\n  theme(\n        axis.ticks = element_blank(),\n        axis.text.y = element_blank(),\n        panel.background = element_blank(),\n        panel.grid.major.x = element_line(colour = \"grey\"),\n        axis.text.x = element_text(size = 15),\n        legend.title=element_blank())\n\nQ5.1.4\n\n\n\n\n\n\n\n5.1.5 CC Data Showed No Vehicle Movements On Weekends Before 12 Noon\nThere were no credit card transactions on weekends before 12 noon. This is odd, considering F&B outlets generally had strong sales on weekday mornings. One would assume for this trend to continue on weekend mornings as well.\n\n\n\n\nCode\nQ5.1.5_weekday &lt;- data.frame(\"weekday\" = unique(cc[c('weekday')])) %&gt;%\n  slice(rep(1:n(), \n            each = 5))                                          # Create new data frame based on unique values in cc\nQ5.1.5_period &lt;- data.frame(\"period\" = unique(cc[c('period')])) # Create new data frame based on unique values in loyalty\nQ5.1.5_period$period &lt;- factor(Q5.1.5_period$period,\n                                levels = c(\"Morning 6am to 11.59am\",\n                                           \"Afternoon 12noon to 5.59pm\",\n                                           \"Evening 6pm to 8.59pm\",\n                                           \"Late Evening 9pm to 11.59pm\",\n                                           \"Late Night 12mn to 5.59am\"))\nQ5.1.5_period &lt;- as.data.frame(lapply(Q5.1.5_period,\n                                      rep,7))\nQ5.1.5_combined &lt;- cbind(Q5.1.5_weekday,\n                         Q5.1.5_period)\n\nrm(Q5.1.5_weekday,\n   Q5.1.5_period)                                              # Remove unused earlier dataset\n\nQ5.1.5_cc &lt;- cc %&gt;%\n  group_by(weekday,\n           period) %&gt;%\n  tally() %&gt;%\n  ungroup()\n\nQ5.1.5_combined &lt;- Q5.1.5_combined %&gt;%\n  left_join(Q5.1.5_cc, \n            by = c(\"weekday\"=\"weekday\",\n                   \"period\"=\"period\"))\nQ5.1.5_combined$id &lt;- seq(1, \n                          nrow(Q5.1.5_combined))\nQ5.1.5_combined[36:63,] &lt;- NA\nQ5.1.5_combined[36,4] &lt;- 5.1\nQ5.1.5_combined[37,4] &lt;- 5.2\nQ5.1.5_combined[38,4] &lt;- 5.3\nQ5.1.5_combined[39,4] &lt;- 5.4\nQ5.1.5_combined[40,4] &lt;- 10.1\nQ5.1.5_combined[41,4] &lt;- 10.2\nQ5.1.5_combined[42,4] &lt;- 10.3\nQ5.1.5_combined[43,4] &lt;- 10.4\nQ5.1.5_combined[44,4] &lt;- 15.1\nQ5.1.5_combined[45,4] &lt;- 15.2\nQ5.1.5_combined[46,4] &lt;- 15.3\nQ5.1.5_combined[47,4] &lt;- 15.4\nQ5.1.5_combined[48,4] &lt;- 20.1\nQ5.1.5_combined[49,4] &lt;- 20.2\nQ5.1.5_combined[50,4] &lt;- 20.3\nQ5.1.5_combined[51,4] &lt;- 20.4\nQ5.1.5_combined[52,4] &lt;- 25.1\nQ5.1.5_combined[53,4] &lt;- 25.2\nQ5.1.5_combined[54,4] &lt;- 25.3\nQ5.1.5_combined[55,4] &lt;- 25.4\nQ5.1.5_combined[56,4] &lt;- 30.1\nQ5.1.5_combined[57,4] &lt;- 30.2\nQ5.1.5_combined[58,4] &lt;- 30.3\nQ5.1.5_combined[59,4] &lt;- 30.4\nQ5.1.5_combined[60,4] &lt;- 35.1\nQ5.1.5_combined[61,4] &lt;- 35.2\nQ5.1.5_combined[62,4] &lt;- 35.3\nQ5.1.5_combined[63,4] &lt;- 35.4\n\nrm(Q5.1.5_cc)                                                  # Remove unused earlier dataset\n\nQ5.1.5_combined &lt;- Q5.1.5_combined %&gt;%\n  arrange(id)\nQ5.1.5_combined$id &lt;- seq(1, \n                          nrow(Q5.1.5_combined))\nQ5.1.5_combined$period &lt;- factor(Q5.1.5_combined$period,\n                                levels = c(\"Morning 6am to 11.59am\",\n                                           \"Afternoon 12noon to 5.59pm\",\n                                           \"Evening 6pm to 8.59pm\",\n                                           \"Late Evening 9pm to 11.59pm\",\n                                           \"Late Night 12mn to 5.59am\"))\n\nQ5.1.5_label &lt;- Q5.1.5_combined\nQ5.1.5_number_of_bar &lt;- nrow(Q5.1.5_label)\nQ5.1.5_angle &lt;- 90 - 360 * (Q5.1.5_label$id-0.5) / Q5.1.5_number_of_bar\nQ5.1.5_label$hjust &lt;- ifelse(Q5.1.5_angle &lt; -90, \n                             1, \n                             0)\nQ5.1.5_label$angle &lt;- ifelse(Q5.1.5_angle &lt; -90, \n                             Q5.1.5_angle + 180,\n                             Q5.1.5_angle)\n\nrm(Q5.1.5_angle,\n   Q5.1.5_number_of_bar)                                       # Remove unused earlier dataset\n\nQ5.1.5_base &lt;- Q5.1.5_combined %&gt;%\n  group_by(weekday) %&gt;%\n  summarize(start = min(id), \n            end = max(id) - 4, \n            .groups = \"drop\") %&gt;%\n  rowwise() %&gt;%\n  mutate(title = mean(c(start, \n                        end))) %&gt;%\n  ungroup()\n\nQ5.1.5_grid &lt;- Q5.1.5_base\nQ5.1.5_grid$end &lt;- Q5.1.5_grid$end[ c( nrow(Q5.1.5_grid), \n                                       1:nrow(Q5.1.5_grid) - 1)] + 1\nQ5.1.5_grid$start &lt;- Q5.1.5_grid$start - 1\nQ5.1.5_grid &lt;- Q5.1.5_grid[-1,]\n\n\nQ5.1.5 &lt;- ggplot(Q5.1.5_combined, \n                 aes(x = as_factor(id), \n                     y = n, \n                     fill = period)) +\n  geom_bar(aes(x = as_factor(id),\n               y = n, \n               fill = period), \n           stat = \"identity\", \n           alpha = 0.5) +\n  geom_segment(data = Q5.1.5_grid, \n               aes(x = end, \n                   y = 120, \n                   xend = start, \n                   yend = 120), \n               colour = \"grey\", \n               alpha = 1, \n               size = 0.3 , \n               inherit.aes = FALSE ) +\n  geom_segment(data = Q5.1.5_grid, \n               aes(x = end, \n                   y = 90, \n                   xend = start, \n                   yend = 90), \n               colour = \"grey\", \n               alpha = 1, \n               size = 0.3 , \n               inherit.aes = FALSE ) +\n  geom_segment(data = Q5.1.5_grid, \n               aes(x = end, \n                   y = 60, \n                   xend = start, \n                   yend = 60), \n               colour = \"grey\", \n               alpha = 1, \n               size = 0.3 , \n               inherit.aes = FALSE ) +\n  geom_segment(data = Q5.1.5_grid, \n               aes(x = end, \n                   y = 30, \n                   xend = start, \n                   yend = 30), \n               colour = \"grey\", \n               alpha = 1, \n               size = 0.3 , \n               inherit.aes = FALSE ) +\n  annotate(\"text\", \n           x = rep(max(Q5.1.5_combined$id),\n                   4), \n           y = c(30, \n                 60, \n                 90, \n                 120), \n           label = c(\"30\", \n                     \"60\", \n                     \"90\", \n                     \"120\") , \n           color = \"grey\", \n           size = 3 , \n           angle = 0, \n           fontface = \"bold\", \n           hjust = 1) +\n  geom_bar(aes(x = as_factor(id), \n               y = n, \n               fill = period), \n           stat = \"identity\", \n           alpha = 0.5) +\n  ylim(-100,\n       150) +\n  theme_minimal() +\n  theme(axis.text = element_blank(),\n        axis.title = element_blank(),\n        panel.grid = element_blank(),\n        plot.margin = unit(rep(-1,\n                               4), \n                           \"cm\")) +\n  coord_polar() +\n  geom_text(data = Q5.1.5_label, \n            aes(x = id, \n                y = n + 10, \n                label = n, \n                hjust = hjust), \n            color = \"black\", \n            fontface = \"bold\",\n            alpha = 0.6, \n            size = 3, \n            angle = Q5.1.5_label$angle, \n            inherit.aes = FALSE ) +\n  geom_segment(data = Q5.1.5_base, \n               aes(x = start, \n                   y = -5, \n                   xend = end, \n                   yend = -5), \n               colour = \"black\", \n               alpha = 0.8, \n               size = 0.6, \n               inherit.aes = FALSE ) +\n  geom_text(data = Q5.1.5_base, \n            aes(x = title, \n                y = -18, \n                label = weekday), \n            color = \"black\", \n            fontface = \"bold\",\n            alpha = 0.6, \n            size = 3, \n            inherit.aes = FALSE )\n\nQ5.1.5\n\n\n\n\n\n\n\n5.1.6 1-to-1 Matching of Credit And Loyalty Cards, Except In Selected Instances\nOur original dataset contained 55 credit card numbers and 54 loyalty card numbers respectively. As part of our fuzzy matching, we were able to complete a 1-to-1 match of 49 pairs of credit and loyalty cards. The remaining cards were found to have a 1-to-2 matching relationship. More investigation would need to be done on these 1-to-2 matches.\n\n\n\n\nCode\n# Create new df for labeling\nQ5.1.6_label_cc &lt;- data.frame(\"id\" = 1:54,\n                              \"code\" = as_factor(cc_loyalty_1$last4ccnum))\nQ5.1.6_label_loyalty &lt;- data.frame(\"id\" = 55:108,\n                                   \"code\" = cc_loyalty_1$loyaltynum)\n\nQ5.1.6_label &lt;- bind_rows(Q5.1.6_label_cc,\n                          Q5.1.6_label_loyalty)\n\nrm(Q5.1.6_label_cc,\n   Q5.1.6_label_loyalty) # Remove unused earlier dataset\n\nQ5.1.6_label &lt;- subset(Q5.1.6_label, \n                       select = -1 )\n\n# Create parallel coordinates plot showing relationship between cc and loyalty. Non-unique matches are visualised in black, whereas 1-to-1 matches are shown in grey. Fig.height changed to 9 to show full height of parallel coordinates plot.\nQ5.1.6 &lt;- ggparcoord(cc_loyalty_1,\n                     columns = 1:2,\n                     groupColumn = 4,\n                     showPoints = TRUE,\n                     alphaLines = 1) +\n  geom_text(aes(label= Q5.1.6_label$code),\n            size = 3,\n            nudge_x = 0.07) +\n  scale_color_manual(values=c( \"#172623\", \n                               \"#E8E8E8\")) +\n  theme_minimal() +\n  scale_y_discrete(breaks = NULL) +\n  theme(axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        legend.position = \"bottom\") +\n        labs(title = \"Credit Card and Loyalty Number Mostly Matched One-To-One\",\n             subtitle = \"Two Loyalty Numbers Are Each Attached To Two Different Credit Cards; \\nOne Credit Card Linked To Two Different Loyalty Numbers\")\n\nQ5.1.6\n\n\n\n\n\n\n\n\n5.2 Question 2 And Its Answers\nAdd the vehicle data to your analysis of the credit and loyalty card data. How does your assessment of the anomalies in question 1 change based on this new data? What discrepancies between vehicle, credit, and loyalty card data do you find? Please limit your answer to 8 images and 500 words.\n\n5.2.1 GPS Seems To Validate Kronos Mart’s Loyalty Txns\nIn section 5.1.2 earlier, we mentioned of the possibility of transactional data tampering specifically relating to Kronos Mart. Here, using looking at the GPS data, single visits on 9th, 11th, 12th, 13th, 15th, 16th and three visits on 18th suggests that these GPS visits matches more to loyalty data than the credit card data.\n\n\n\n\nCode\n# Create new df of gps data specifically on Kronos Mart's tracking\nQ5.2.1_gps &lt;- tibble(\"dayofmonth\" = c(6:19)) %&gt;%\n                  left_join(spots %&gt;%\n                              group_by(Location, \n                                       dayofmonth) %&gt;%\n                              tally() %&gt;%\n                              filter(Location == \"Kronos Mart\") %&gt;%\n                              ungroup(), \n                            by = \"dayofmonth\") %&gt;%\n                  mutate(n2 = n) %&gt;%\n                  replace_na(list(n = 0))\n\n# Showcase Kronos Mart's GPS activity via a geom-area visualisation\nQ5.2.1 &lt;- ggplot(Q5.2.1_gps,\n                  aes(x = dayofmonth, \n                      y = n)) +\n  geom_area(size = 1) +\n  geom_point() +\n  geom_text(aes(label = n2), \n            na.rm = TRUE,\n            vjust = -1,\n            size = 3) +\n  scale_x_continuous(breaks = seq(6,\n                                  19,\n                                  1)) +\n  ylim(0,\n       5) +\n  labs(\"title\" = \"GPS Movements to Kronos Mart Validates Loyalty Data\",\n       \"subtitle\" = \"GPS data seem to validate the loyalty data, more than the credit card data\") +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        panel.grid.major.y = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        legend.position = \"none\")\n\nQ5.2.1\n\n\n\n\n\n\n\n5.2.2 GPS Validates F&B Outlets From Earlier Credit Card Data; Other Outlets Seem To Have Unmatched Visits\nVisitations to unknown locations such as Hippokampos and Abila Zacharo during noon / lunch time confirms these locations as food outlets. However, visit frequency to other locations seem to differ:\n- Some locations such as Bean There Done That, Brewed Awakenings and Jack’s Magical Beans has high gps visits in the morning, whereas credit card data shows stronger transactions during lunch time\n- Kronos Mart’s GPS data suggests more activity between 12 noon to midnight, but credit card data suggests more activity between midnight and 12 noon\n\n\n\n\nCode\n# Create ridgeline plot of GPS locations by hours of day, and excluding Home, Anonymous locations and Gastech.\nQ5.2.2 &lt;- ggplot(spots %&gt;%\n                   filter(!grepl(\"Home|Anonymous|GasTech\", \n                                 Location)),\n                 aes(x = hour,\n                     y = Location,\n                     fill = stat(x)\n                     )) +\n                 geom_density_ridges_gradient(scale = 3,\n                                              rel_min_height = 0.001) +\n  scale_x_continuous(breaks = 0:24) +\n  scale_y_discrete(limits = rev) +\n  scale_fill_viridis_c(name = \"ABC\", \n                       option = \"A\") +\n  theme_ridges(font_size = 7, \n               grid = TRUE) +\n  theme(legend.position = \"none\") +\n  labs(title = \"Uncovering Location Type Beyond Ambiguous Location Names Using GPS Data\",\n       subtitle = \"High Noon GPS Activity Continue To Suggest Abila Zacharo and Hippokampos As Food Outlets\")\n\nQ5.2.2\n\n\n\n\n\n\n\n5.2.3 Selected Coffee Chain Activity Still Occur Only At Selected Hours Within The GPS Data\nGPS data shows restricted activity coffee chain activity to just two hours within the day. This differs from credit card data, where it occurs in three hours. Noon activity is missing from GPS data.\n\n\n\n\nCode\n# Merge spots data with Location Type and change from long to wide format\nQ5.2.3_spots &lt;- spots %&gt;%\n  left_join(dplyr::select(spots_median,Location, \n                          Location.Type), \n            by = c(\"Location\" = \"Location\")) %&gt;%\n  filter(Location.Type == \"Coffee Cafe\") %&gt;%\n  count(Location, hour) %&gt;%\n  ungroup() %&gt;%\n  dcast(hour ~ Location,)\n\nQ5.2.3_spots$hour &lt;- as_factor(Q5.2.3_spots$hour)\n\nQ5.2.3_combined &lt;- data.frame(hour = c(1:24))\nQ5.2.3_combined$hour &lt;- as_factor(Q5.2.3_combined$hour)\nQ5.2.3_combined &lt;- Q5.2.3_combined %&gt;%\n  left_join(Q5.2.3_spots, \n            by = \"hour\")\n\nrm(Q5.2.3_spots) # Remove unused earlier dataset\n\nQ5.2.3_combined &lt;-melt(Q5.2.3_combined, \n                       id.vars = \"hour\", \n                       variable.name = \"coffee_place\")\n\nQ5.2.3 &lt;- ggplot(Q5.2.3_combined, \n                 aes(hour, \n                     value, \n                     fill = coffee_place)) +\n  geom_bar(stat = \"identity\") +\n  coord_polar(theta = \"x\") +\n  labs(title = \"Daily GPS Activity At Coffee Chains Restricted To Only Two Hours\",\n       subtitle = \"No More Noon Activity Has Compared To Its CC Txns\") +\n  xlab(\"\") +\n  ylab(\"\") +\n  theme(\n        axis.ticks = element_blank(),\n        axis.text.y = element_blank(),\n        panel.background = element_blank(),\n        panel.grid.major.x = element_line(colour = \"grey\"),\n        axis.text.x = element_text(size = 15),\n        legend.title=element_blank())\n\nQ5.2.3\n\n\n\n\n\n\n\n5.2.4 GPS Showed Some Semblance of Vehicle Movement On Weekends Before Midday\nOn Sat morning, there was around 2,000+ gps points up and about in the city. This differs from a lack of credit card activity during the same period. Looking at the GPS data, these movements revolved around homes and Kronos Capital. Both these locations are not commercial locations, and thus no credit card transaction is to be expected anyways.\n\n\n\n\nCode\nQ5.2.4_weekday &lt;- data.frame(\"weekday\" = unique(cc[c('weekday')])) %&gt;%\n  slice(rep(1:n(), \n            each = 5))\nQ5.2.4_period &lt;- data.frame(\"period\" = unique(cc[c('period')]))\nQ5.2.4_period$period &lt;- factor(Q5.2.4_period$period,\n                                levels = c(\"Morning 6am to 11.59am\",\n                                           \"Afternoon 12noon to 5.59pm\",\n                                           \"Evening 6pm to 8.59pm\",\n                                           \"Late Evening 9pm to 11.59pm\",\n                                           \"Late Night 12mn to 5.59am\"))\nQ5.2.4_period &lt;- as.data.frame(lapply(Q5.2.4_period,\n                                      rep,\n                                      7))\nQ5.2.4_combined &lt;- cbind(Q5.2.4_weekday,\n                         Q5.2.4_period)\n\nrm(Q5.2.4_weekday,\n   Q5.2.4_period) # Remove unused earlier dataset\n\nQ5.2.4_gps &lt;- gps_name %&gt;%\n  group_by(weekday,\n           period) %&gt;%\n  tally() %&gt;%\n  ungroup()\n\nQ5.2.4_combined &lt;- Q5.2.4_combined %&gt;%\n  left_join(Q5.2.4_gps, \n            by = c(\"weekday\" = \"weekday\",\n                   \"period\" = \"period\"))\nQ5.2.4_combined$id &lt;- seq(1, \n                          nrow(Q5.2.4_combined))\nQ5.2.4_combined[36:63,] &lt;- NA\nQ5.2.4_combined[36,4] &lt;- 5.1\nQ5.2.4_combined[37,4] &lt;- 5.2\nQ5.2.4_combined[38,4] &lt;- 5.3\nQ5.2.4_combined[39,4] &lt;- 5.4\nQ5.2.4_combined[40,4] &lt;- 10.1\nQ5.2.4_combined[41,4] &lt;- 10.2\nQ5.2.4_combined[42,4] &lt;- 10.3\nQ5.2.4_combined[43,4] &lt;- 10.4\nQ5.2.4_combined[44,4] &lt;- 15.1\nQ5.2.4_combined[45,4] &lt;- 15.2\nQ5.2.4_combined[46,4] &lt;- 15.3\nQ5.2.4_combined[47,4] &lt;- 15.4\nQ5.2.4_combined[48,4] &lt;- 20.1\nQ5.2.4_combined[49,4] &lt;- 20.2\nQ5.2.4_combined[50,4] &lt;- 20.3\nQ5.2.4_combined[51,4] &lt;- 20.4\nQ5.2.4_combined[52,4] &lt;- 25.1\nQ5.2.4_combined[53,4] &lt;- 25.2\nQ5.2.4_combined[54,4] &lt;- 25.3\nQ5.2.4_combined[55,4] &lt;- 25.4\nQ5.2.4_combined[56,4] &lt;- 30.1\nQ5.2.4_combined[57,4] &lt;- 30.2\nQ5.2.4_combined[58,4] &lt;- 30.3\nQ5.2.4_combined[59,4] &lt;- 30.4\nQ5.2.4_combined[60,4] &lt;- 35.1\nQ5.2.4_combined[61,4] &lt;- 35.2\nQ5.2.4_combined[62,4] &lt;- 35.3\nQ5.2.4_combined[63,4] &lt;- 35.4\n\nrm(Q5.2.4_gps) # Remove unused earlier dataset\n\nQ5.2.4_combined &lt;- Q5.2.4_combined %&gt;%\n  arrange(id)\nQ5.2.4_combined$id &lt;- seq(1, \n                          nrow(Q5.2.4_combined))\nQ5.2.4_combined$period &lt;- factor(Q5.2.4_combined$period,\n                                levels = c(\"Morning 6am to 11.59am\",\n                                           \"Afternoon 12noon to 5.59pm\",\n                                           \"Evening 6pm to 8.59pm\",\n                                           \"Late Evening 9pm to 11.59pm\",\n                                           \"Late Night 12mn to 5.59am\"))\n\nQ5.2.4_label &lt;- Q5.2.4_combined\nQ5.2.4_number_of_bar &lt;- nrow(Q5.2.4_label)\nQ5.2.4_angle &lt;- 90 - 360 * (Q5.2.4_label$id - 0.5) / Q5.2.4_number_of_bar\nQ5.2.4_label$hjust &lt;- ifelse(Q5.2.4_angle &lt; -90, \n                             1, \n                             0)\nQ5.2.4_label$angle &lt;- ifelse(Q5.2.4_angle &lt; -90, \n                             Q5.2.4_angle + 180, \n                             Q5.2.4_angle)\n\nrm(Q5.2.4_angle) # Remove unused earlier dataset\nrm(Q5.2.4_number_of_bar) # Remove unused earlier dataset\n\nQ5.2.4_base &lt;- Q5.2.4_combined %&gt;%\n  group_by(weekday) %&gt;%\n  summarize(start = min(id), \n            end = max(id) - 4, \n            .groups = \"drop\") %&gt;%\n  rowwise() %&gt;%\n  mutate(title = mean(c(start, \n                        end))) %&gt;%\n  ungroup()\n\nQ5.2.4_grid &lt;- Q5.2.4_base\nQ5.2.4_grid$end &lt;- Q5.2.4_grid$end[ c( nrow(Q5.2.4_grid), 1:nrow(Q5.2.4_grid) - 1)] + 1\nQ5.2.4_grid$start &lt;- Q5.2.4_grid$start - 1\nQ5.2.4_grid &lt;- Q5.2.4_grid[-1,]\n\n\nQ5.2.4 &lt;- ggplot(Q5.2.4_combined, \n                 aes(x = as_factor(id), \n                     y = n, \n                     fill = period)) +\n  geom_bar(aes(x = as_factor(id), \n               y = n, \n               fill = period), \n           stat = \"identity\", \n           alpha = 0.5) +\n  geom_segment(data = Q5.2.4_grid, \n               aes(x = end, \n                   y = 80000, \n                   xend = start, \n                   yend = 80000), \n               colour = \"grey\", \n               alpha = 1, \n               size = 0.3 , \n               inherit.aes = FALSE ) +\n  geom_segment(data = Q5.2.4_grid, \n               aes(x = end, \n                   y = 60000, \n                   xend = start, \n                   yend = 60000), \n               colour = \"grey\", \n               alpha = 1, \n               size = 0.3 , \n               inherit.aes = FALSE ) +\n  geom_segment(data = Q5.2.4_grid, \n               aes(x = end, \n                   y = 40000, \n                   xend = start, \n                   yend = 40000), \n               colour = \"grey\", \n               alpha = 1, \n               size = 0.3 , \n               inherit.aes = FALSE ) +\n  geom_segment(data = Q5.2.4_grid, \n               aes(x = end, \n                   y = 20000, \n                   xend = start, \n                   yend = 20000), \n               colour = \"grey\", \n               alpha = 1, \n               size = 0.3 , \n               inherit.aes = FALSE ) +\n  annotate(\"text\", \n           x = rep(max(Q5.2.4_combined$id),\n                   4), \n           y = c(20000, \n                 40000, \n                 60000, \n                 80000), \n           label = c(\"20000\", \n                     \"40000\", \n                     \"60000\", \n                     \"80000\") , \n           color = \"grey\", \n           size = 3 , \n           angle = 0, \n           fontface = \"bold\", \n           hjust = 1) +\n  geom_bar(aes(x = as_factor(id), \n               y = n, \n               fill = period), \n           stat = \"identity\", \n           alpha = 0.5) +\n  ylim(-80000,\n       100000) +\n  theme_minimal() +\n  theme(axis.text = element_blank(),\n        axis.title = element_blank(),\n        panel.grid = element_blank(),\n        plot.margin = unit(rep(-1,\n                               4), \n                           \"cm\")) +\n  coord_polar() +\n  geom_text(data = Q5.2.4_label, \n            aes(x = id, \n                y = n + 10, \n                label = n, \n                hjust = hjust), \n            color = \"black\", \n            fontface = \"bold\",\n            alpha = 0.6, \n            size = 3, \n            angle = Q5.2.4_label$angle, \n            inherit.aes = FALSE ) +\n  geom_segment(data = Q5.2.4_base, \n               aes(x = start, \n                   y = -5, \n                   xend = end, \n                   yend = -5), \n               colour = \"black\", \n               alpha = 0.8, \n               size = 0.6 , \n               inherit.aes = FALSE )  +\n  geom_text(data = Q5.2.4_base, \n            aes(x = title, \n                y = -10000, \n                label = weekday), \n            color = \"black\", \n            fontface = \"bold\",\n            alpha = 0.6, \n            size = 3, \n            inherit.aes = FALSE )\n\nQ5.2.4\n\n\n\n\n\n\n\n\n5.3 Question 3 And Its Answers\nCan you infer the owners of each credit card and loyalty card? What is your evidence? Where are there uncertainties in your method? Where are there uncertainties in the data? Please limit your answer to 8 images and 500 words.\n\n5.3.1 Combining Both Credit Card and Spots Data Using Fuzzy Join (OSA)\nWe will find matching IDs between the credit card and spots data. Spots data are basically GPS points that have remained stationary for more than 10 mins. The main columns of comparison are the day of month, location and hour, and identical rows are determined as a result. From here, we identify the match, by counting the max number of matching rows between credit card and spots data.\n\nThese matching is on a best match basis, and uncertainties lie in the following areas:\n- Spots data may show a visit to that location, but no purchase may have been made\n- Discrepancy in the credit card data’s date or hour may cause ill-matches\n\n\n\n\nCode\ncc_spots &lt;- cc %&gt;% # Create a new df that shows matches with a max distance difference of 0\n  stringdist_inner_join(spots,\n                        by = c(\"concat_cc_spots\" = \"concat_spots_cc\"),\n                        method = \"osa\",\n                        max_dist = 0,\n                        distance_col = \"distance\")\n\ncc_spots_1 &lt;- cc_spots %&gt;% # Isolate best matching cc and spots with more than 2 counts\n  filter(!is.na(FullName)) %&gt;% # Remove unknown drivers\n  group_by(RoleNName,\n           last4ccnum) %&gt;%\n  count() %&gt;%\n  arrange(RoleNName,\n          -n) %&gt;% # Arrange the highest to lowest count in each group\n  ungroup()\n\ncolnames(cc_spots_1)[colnames(cc_spots_1) == \"n\"] = \"matches\" # Rename last column to matches\n\ncc_summary &lt;- cc %&gt;%\n  group_by(last4ccnum) %&gt;%\n  count() %&gt;%\n  ungroup()\n\ncc_spots_1 &lt;- cc_spots_1[!duplicated(cc_spots_1$RoleNName),] # Isolating 1 cc to 1 driver\n\nknitr::kable(cc_spots_1, caption = \"Matched Credit Card to ID And Name\") %&gt;%\n  kable_styling(bootstrap_options = \"striped\",\n                full_width = F) # Output matched table\n\n\n\nMatched Credit Card to ID And Name\n\n\nRoleNName\nlast4ccnum\nmatches\n\n\n\n\n1 IT Helpdesk Nils Calixto\n9551\n23\n\n\n10 SVP/CIO Ada Campo-Corrente\n8332\n20\n\n\n11 Hydraulic Technician Axel Calzas\n1321\n21\n\n\n12 Site Control Hideki Cocinaro\n7108\n25\n\n\n13 Site Control Inga Ferro\n7819\n29\n\n\n14 Engineering Group Manager Lidelse Dedos\n1874\n28\n\n\n15 Site Control Loreto Bodrogi\n3853\n28\n\n\n16 Perimeter Control Isia Vann\n7354\n33\n\n\n17 IT Technician Sven Flecha\n7384\n32\n\n\n18 Geologist Birgitta Frente\n9617\n28\n\n\n19 Hydraulic Technician Vira Frente\n6895\n23\n\n\n2 Engineer Lars Azada\n1415\n21\n\n\n20 Building Control Stenig Fusil\n6816\n27\n\n\n21 Perimeter Control Hennie Osvaldo\n9405\n31\n\n\n22 Badging Office Adra Nubarron\n1286\n26\n\n\n23 Badging Office Varja Lagos\n3484\n31\n\n\n24 Perimeter Control Minke Mies\n4434\n28\n\n\n25 Geologist Kanon Herrero\n2142\n29\n\n\n26 Drill Site Manager Marin Onda\n1310\n32\n\n\n27 Drill Technician Kare Orilla\n3492\n25\n\n\n29 Facilities Group Manager Bertrand Ovan\n3547\n20\n\n\n3 Engineer Felix Balas\n9635\n19\n\n\n30 Security Group Manager Felix Resumir\n6901\n31\n\n\n31 President/CEO Sten Sanjorge Jr.\n5010\n5\n\n\n32 SVP/COO Orhan Strum\n8156\n22\n\n\n33 Drill Technician Brand Tempestad\n9683\n24\n\n\n34 Perimeter Control Edvard Vann\n4795\n25\n\n\n35 Environmental Safety Advisor Willem Vasco-Pais\n2463\n20\n\n\n4 SVP/CFO Ingrid Barranco\n7688\n22\n\n\n5 IT Technician Isak Baza\n6899\n19\n\n\n6 IT Group Manager Linnea Bergen\n7253\n27\n\n\n7 Drill Technician Elsa Orilla\n2540\n19\n\n\n8 IT Technician Lucas Alcazar\n7889\n27\n\n\n9 Drill Technician Gustav Cazar\n1877\n12\n\n\n\n\n\n\n\n\n\n\nContinuing On To Part 2"
  },
  {
    "objectID": "school/vast2021_2/index.html",
    "href": "school/vast2021_2/index.html",
    "title": "VAST Challenge 2021 (Mini-Challenge 2) Part 2",
    "section": "",
    "text": "5.4 Question 4 And Its Answers (A Continuation From Part 1)\nGiven the data sources provided, identify potential informal or unofficial relationships among GASTech personnel. Provide evidence for these relationships. Please limit your response to 8 images and 500 words.\n\n5.4.1 Elsa (ID: 7, Black Line) And Brand (ID: 33, Blue Line) Are Seeing Each Other\nBoth were frequenting the following places together at similar times and for similar durations:\n- Chostus Hotel\n- Frydos Autosupply n’ More\n- Gathering at Engineer’s Lars Home on 10th Jan Late Evening\n- Hippokampos on 15th Jan Afternoon\n- Ouzeri Elian on 6th Jan Afternoon\n\n\n\n\nCode\ntmap_mode(\"view\")\n\nQ5.4.1 &lt;- tm_shape(sea_poly) +\n  tm_polygons(col = \"lightblue\") +\ntm_shape(Kronos_sf_small) +\n  tm_polygons(col = \"beige\") +\ntm_shape(Abila_st_buffer) +\n  tm_polygons(col = \"white\") +\ntm_shape(gps_path %&gt;% filter(id == 7)) + # Extract Elsa's path\n  tm_lines(col = \"black\",\n           lty = 1,\n           id = \"RoleNName\") +\ntm_shape(gps_path %&gt;% filter(id == 33)) + # Extract Brand's path\n  tm_lines(col = \"blue\",\n           lty = 1,\n           id = \"RoleNName\") +  \ntm_shape(spots_median_sf) +\n  tm_dots(col = \"Location.Type\",\n          id = \"Location\",                # Bold in group\n          popup.vars = \"Location Type:\" == \"Location.Type\",\n          size = 0.2)\n\nQ5.4.1\n\n\n\n\n\n\n\n\n\n5.4.2 21 Hennie Osvaldo Has Two Homes, Of Which Are Shared By Other Tenants\nHennie seem to stay in two separate homes on different evenings:\n- Either with Lidelse and Birgitta\n- Or with Inga, Loreto and Isia\n\n\n\n\nCode\ntmap_mode(\"view\")\n\nQ5.4.2 &lt;- tm_shape(sea_poly) +\n  tm_polygons(col = \"lightblue\") +\ntm_shape(Kronos_sf_small) +\n  tm_polygons(col = \"beige\") +\ntm_shape(Abila_st_buffer) +\n  tm_polygons(col = \"white\") +\ntm_shape(gps_path %&gt;% filter(id == 21)) + # Extract Hennie's path\n  tm_lines(col = \"black\",\n           lty = 1,\n           id = \"RoleNName\") +  \ntm_shape(spots_median_sf %&gt;%\n           filter(Location == \"Shared Home B - 14 Lidelse 18 Birgitta 21 Hennie\" | Location == \"Shared Home E - 13 Inga 15 Loreto 16 Isia 21 Hennie\")) +\n  tm_dots(col = \"green\",\n          size = 0.2)\n\nQ5.4.2\n\n\n\n\n\n\n\n\n\n5.4.3 Bertrand (ID: 29, Black Line) And Linnea (ID: 6, Blue Line) Are Seeing Each Other\nAlthough both are staying in the same housing together with Kanon, both seem to frequent the same coffee chain in the mornings and food outlet in the evenings together. Kanon was not present during these meal times.\n- Coffee Cameleon\n- Katerina’s Cafe\n\n\n\n\nCode\ntmap_mode(\"view\")\n\nQ5.4.3 &lt;- tm_shape(sea_poly) +\n  tm_polygons(col = \"lightblue\") +\ntm_shape(Kronos_sf_small) +\n  tm_polygons(col = \"beige\") +\ntm_shape(Abila_st_buffer) +\n  tm_polygons(col = \"white\") +\ntm_shape(gps_path %&gt;% filter(id == 29 & day != 19)) + # Extract Bertrand's path and removed 19th Jan since its single point throws an error in the linestring\n  tm_lines(col = \"black\",\n           lty = 1,\n           id = \"RoleNName\") +\ntm_shape(gps_path %&gt;% filter(id == 6)) + # Extract Linnea's path\n  tm_lines(col = \"blue\",\n           lty = 1,\n           id = \"RoleNName\") +  \ntm_shape(spots_median_sf) +\n  tm_dots(col = \"Location.Type\",\n          id = \"Location\", # Bold in group\n          popup.vars = \"Location Type:\" == \"Location.Type\",\n          size = 0.2)\n\nQ5.4.3\n\n\n\n\n\n\n\n\n\n5.4.4 Lidelse (ID: 14, Black Line) And Birgitta (ID: 18, Blue Line) Are Seeing Each Other\nSimilarly, although both are staying in the same housing together with Hennie, both seem to frequent the same coffee chains in the mornings and food outlets in the afternoon and evenings together. Hennie was not present during these meal times.\n- Guy’s Gyros\n- Bean There Done That\n- Katerina’s Cafe\n- Hallowed Grounds\n\n\n\n\nCode\ntmap_mode(\"view\")\n\nQ5.4.4 &lt;- tm_shape(sea_poly) +\n  tm_polygons(col = \"lightblue\") +\ntm_shape(Kronos_sf_small) +\n  tm_polygons(col = \"beige\") +\ntm_shape(Abila_st_buffer) +\n  tm_polygons(col = \"white\") +\ntm_shape(gps_path %&gt;% filter(id == 14)) + # Extract Lidelse's path\n  tm_lines(col = \"black\",\n           lty = 1,\n           id = \"RoleNName\") +\ntm_shape(gps_path %&gt;% filter(id == 18)) + # Extract Birgitta's path\n  tm_lines(col = \"blue\",\n           lty = 1,\n           id = \"RoleNName\") +  \ntm_shape(spots_median_sf) +\n  tm_dots(col = \"Location.Type\",\n          id = \"Location\",                # Bold in group\n          popup.vars = \"Location Type:\" == \"Location.Type\",\n          size = 0.2)\n\nQ5.4.4\n\n\n\n\n\n\n\n\n\n\nContinuing On To Part 3"
  },
  {
    "objectID": "school/vast2021_3/index.html",
    "href": "school/vast2021_3/index.html",
    "title": "VAST Challenge 2021 (Mini-Challenge 2) Part 3",
    "section": "",
    "text": "5.5 Question 5 And Its Answers (A Continuation From Part 2)\nDo you see evidence of suspicious activity? Identify 1- 10 locations where you believe the suspicious activity is occurring, and why. Please limit your response to 10 images and 500 words.\n\nSuspicious Activities Can Be In The Following Forms:\n\n\nUnknown locations not found on map\n\nGathering of two or more individuals at the same location at the same hour for extended periods\n\nIndividuals frequenting unusual places at abnormal hours\n\n\n\n5.5.1 Presence of Anonymous Locations (Shown As Black Dots On Map)\nThese are locations where there were multiple instances of GPS points remaining stationary for more than 10 mins. These unknown locations do not conform to known locations on the furnished map pic.\n\n\n\n\nCode\ntmap_mode(\"view\")\n\nQ5.5.1 &lt;- tm_shape(mc2) +\n  tm_raster(legend.show = FALSE) +\n  # tm_rgb(mc2,                             # tm_rgb used to work in the previous tmap versions, but are now unable to render multiple bands in a single raster layer\n  #        r = 1, \n  #        g = 2,\n  #        b = 3,\n  #        alpha = NA,\n  #        saturation = 1,\n  #        interpolate = TRUE,\n  #        max.value = 255) +\n  tm_shape(spots_median_sf %&gt;%\n           filter(Location.Type != \"Unknown\")) +\n  tm_dots(col = \"Location.Type\",\n          id = \"Location\", # Bold in group\n          popup.vars = \"Location Type:\" == \"Location.Type\",\n          size = 0.2) +\n  tm_shape(spots_median_sf %&gt;%\n           filter(Location.Type == \"Unknown\")) +\n  tm_dots(col = \"black\",\n          id = \"Location\", # Bold in group\n          popup.vars = \"Location Type:\" == \"Location.Type\",\n          size = 0.2)\n\nQ5.5.1\n\n\n\n\n\n\n\n\n5.5.2 Suspicious Monitoring Of Key Officials’ Homes (by Bodrogi, Vann, Osvaldo and Mies)\nShowcasing only residential points, Bodrogi (ID: 15, black line), Vann (ID: 16, blue line), Osvaldo (ID:21, purple line) and Mies (ID:24, red line) were seen patroling key executives’ houses located near the centre area. (Hover over the lines and points to see the ID and owner of each residence)\n\n\n\n\nCode\ntmap_mode(\"view\")\n\nQ5.5.2 &lt;- tm_shape(sea_poly) +\n  tm_polygons(col = \"lightblue\") +\ntm_shape(Kronos_sf_small) +\n  tm_polygons(col = \"beige\") +\ntm_shape(Abila_st_buffer) +\n  tm_polygons(col = \"white\") +\ntm_shape(gps_path %&gt;% filter(id == 15)) + # Extract Bodrogi's path\n  tm_lines(col = \"black\",\n           lty = 1,\n           id = \"RoleNName\") +\ntm_shape(gps_path %&gt;% filter(id == 16)) + # Extract Vann's path\n  tm_lines(col = \"blue\",\n           lty = 1,\n           id = \"RoleNName\") +  \ntm_shape(gps_path %&gt;% filter(id == 21)) + # Extract Osvaldo's path\n  tm_lines(col = \"purple\",\n           lty = 1,\n           id = \"RoleNName\") +\ntm_shape(gps_path %&gt;% filter(id == 24)) + # Extract Mies's path\n  tm_lines(col = \"red\",\n           lty = 1,\n           id = \"RoleNName\") +  \ntm_shape(spots_median_sf %&gt;%\n           filter(Location.Type == \"Residential\")) +\n  tm_dots(col = \"green\",\n          size = 0.2)\n\nQ5.5.2\n\n\n\n\n\n\n\n\n\n5.5.3 Weird Off-Road Driving by Isande Borrasca\nIt begs the question as to the main cause of Isande’s wayward driving. Though it’s highly unlikely that he veers from side to side throughout his drive, it suggests that his GPS device is either faulty or that it has been tampered to cover his tracks. Relooking at the places he visited, there is little to suggest that he might be a risky character. But nonetheless, his wayward movements remain suspicious.\n\n\n\n\nCode\ntmap_mode(\"view\")\n\nQ5.5.3 &lt;- tm_shape(sea_poly) +\n  tm_polygons(col = \"lightblue\") +\ntm_shape(Kronos_sf_small) +\n  tm_polygons(col = \"beige\") +\ntm_shape(Abila_st_buffer) +\n  tm_polygons(col = \"white\") +\ntm_shape(gps_path %&gt;% filter(id == 28)) + # Extract Isande's path\n  tm_lines(col = \"black\",\n           lty = 1,\n           id = \"RoleNName\") +\ntm_shape(spots_median_sf) +\n  tm_dots(col = \"Location.Type\",\n          id = \"Location\", # Bold in group\n          popup.vars = \"Location Type:\" == \"Location.Type\",\n          size = 0.2)\n\nQ5.5.3\n\n\n\n\n\n\n\n\n\n5.5.4 Possible Suspicious Gathering At Kronos Capital On 18th and 19th Jan\nOn 18th Jan, Bodrogi (ID: 15, black line) met Nubarron (ID: 22, blue line) at Kronos Capital in the afternoon. This location was visited in the morning by Nubarron, as well as Vann (ID: 34, red line) in the evening. Herrero (ID:25, green line) was also stationary for approx. 24 hours in this location on 19th Jan.\n\n\n\n\nCode\ntmap_mode(\"view\")\n\nQ5.5.4 &lt;- tm_shape(sea_poly) +\n  tm_polygons(col = \"lightblue\") +\ntm_shape(Kronos_sf_small) +\n  tm_polygons(col = \"beige\") +\ntm_shape(Abila_st_buffer) +\n  tm_polygons(col = \"white\") +\ntm_shape(gps_path %&gt;% filter(id==15 & day==18)) + # Extract Bodrogi's path on 18th Jan\n  tm_lines(col = \"black\",\n           lty = 1,\n           id = \"RoleNName\") +\ntm_shape(gps_path %&gt;% filter(id==22 & day==18)) + # Extract Nubarron's path on 18th Jan\n  tm_lines(col = \"blue\",\n           lty = 1,\n           id = \"RoleNName\") +  \ntm_shape(gps_path %&gt;% filter(id==34 & day==18)) + # Extract Vann's path on 18th Jan\n  tm_lines(col = \"red\",\n           lty = 1,\n           id = \"RoleNName\") +\ntm_shape(gps_path %&gt;% filter(id==25 & day==19)) + # Extract Herrero's path on 19th Jan\n  tm_lines(col = \"green\",\n           lty = 1,\n           id = \"RoleNName\") +  \ntm_shape(spots_median_sf %&gt;%\n           filter(Location == \"Kronos Capital\")) +\n  tm_dots(col = \"green\",\n          size = 0.2)\n\nQ5.5.4\n\n\n\n\n\n\n\n\n\n5.5.5 Large Gathering At Engineer Lar’s Home on Jan 10 Late Evening\nA large gathering of 13 individuals, from both the IT and Geological department, was spotted in the late evening on 10th Jan.\n\n\n\n\nCode\ntmap_mode(\"view\")\n\nQ5.5.5 &lt;- tm_shape(sea_poly) +\n  tm_polygons(col = \"lightblue\") +\ntm_shape(Kronos_sf_small) +\n  tm_polygons(col = \"beige\") +\ntm_shape(Abila_st_buffer) +\n  tm_polygons(col = \"white\") +\n\n# Extract a multitude of visitors to Lars' Home on Jan 10th Late Evening\ntm_shape(gps_path %&gt;% \n           filter(day==10 & id==1  |\n                            id==2  |\n                            id==5  |\n                            id==6  |\n                            id==7  |\n                            id==8  |\n                            id==9  |\n                            id==11 |\n                            id==14 |\n                            id==18 |\n                            id==19 |\n                            id==25 |\n                            id==33)) +\n  tm_lines(col = \"black\",\n           lty = 1,\n           id = \"RoleNName\") +  \ntm_shape(spots_median_sf %&gt;%\n           filter(Location == \"2 Engineer Lars's Home\")) +\n  tm_dots(col = \"green\",\n          size = 0.2)\n\nQ5.5.5\n\n\n\n\n\n\n\n\n\n\n5.6 Question 6 And Its Answers\nIf you solved this mini-challenge in 2014, how did you approach it differently this year?\n\n5.6.1 Question Not Applicable\nWe did not attempt this mini-challenge in 2014.\n\n#—————————————————–"
  },
  {
    "objectID": "scribbles.html",
    "href": "scribbles.html",
    "title": "Scribbles",
    "section": "",
    "text": "Scribbling my musings from everyday interactions and experiences. Also for me to capture and reuse useful information and knowledge.\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "school/dataviz_1/index.html",
    "href": "school/dataviz_1/index.html",
    "title": "DataViz Makeover 1",
    "section": "",
    "text": "Critic the visualized graph with at least three points from each evaluation criterion (clarity and aesthetics) respectively.\nSuggest alternative graphical presentation to improve the current design:\n\n\n\nProposed alternative data visualization must be in static form.\nProposed design would need to be sketched.\nSupport your design by describing the advantages or which part of the issue(s) your alternative design attempts to overcome.\n\n\nUse Tableau to design the proposed data visualization.\nProvide step-by-step description on how the data visualization was prepared.\nDescribe three major observations revealed by the prepared data visualization.\n\n\n\n\n\nThe write-up of the DataViz Makeover must be in distill or blogdown format, and is required to publish the write-up on Netlify.\nThe DataViz Makeover must be prepared using Tableau Desktop. The final workbook must be uploaded onto Tableau Public."
  },
  {
    "objectID": "school/dataviz_1/index.html#makeover-requirements",
    "href": "school/dataviz_1/index.html#makeover-requirements",
    "title": "DataViz Makeover 1",
    "section": "",
    "text": "Critic the visualized graph with at least three points from each evaluation criterion (clarity and aesthetics) respectively.\nSuggest alternative graphical presentation to improve the current design:\n\n\n\nProposed alternative data visualization must be in static form.\nProposed design would need to be sketched.\nSupport your design by describing the advantages or which part of the issue(s) your alternative design attempts to overcome.\n\n\nUse Tableau to design the proposed data visualization.\nProvide step-by-step description on how the data visualization was prepared.\nDescribe three major observations revealed by the prepared data visualization.\n\n\n\n\n\nThe write-up of the DataViz Makeover must be in distill or blogdown format, and is required to publish the write-up on Netlify.\nThe DataViz Makeover must be prepared using Tableau Desktop. The final workbook must be uploaded onto Tableau Public."
  },
  {
    "objectID": "school/dataviz_1/index.html#original-visualisation-and-its-perceived-objective",
    "href": "school/dataviz_1/index.html#original-visualisation-and-its-perceived-objective",
    "title": "DataViz Makeover 1",
    "section": "2. Original Visualisation and Its Perceived Objective",
    "text": "2. Original Visualisation and Its Perceived Objective\nThe original visualization (shown below) showcases the merchandise trade, for both export and import, of the top six trading countries with Singapore across 2019 to 2020.\n\n\n\nFigure 1: Original Data Visualisation\n\n\nTo better understand how best to recast this visualization, we must first understand the intent of the visualization. Alberto Cairo succinctly sums up the actual intent of the graphic: “what is it that you want people to see”. He went to explain that one should begin not by designing the graphic first but rather, begin by “writing a very long narrative that you want to tell”.\n\nAn attempt on the narrative for the above visualization is as follows:\n\n“The import and export fluctuations within each of its top six trading countries differ greatly between 2019 to 2020, as well as against other top trading countries.”\n\nTo further illuminate the narrative, we propose the following questions that the visualization aims to answer:\n• What are the import and export levels in each of the top six trading countries?\n• How do these import and export levels fluctuate over time?\n• Within the top six countries, who has the highest and lowest import and export levels respectively?\n• (Additional) Do these fluctuations coincide with any event?"
  },
  {
    "objectID": "school/dataviz_1/index.html#critiquing-methodology",
    "href": "school/dataviz_1/index.html#critiquing-methodology",
    "title": "DataViz Makeover 1",
    "section": "3. Critiquing Methodology",
    "text": "3. Critiquing Methodology\nWe attempt to critique the above visualization on two criterions: clarity and aesthetics. Ben Jones (2012) nicely captures these criterions within a two-by-two matrix. The ideal quadrant to be in is Quadrant I, where it is both clear and beautiful.\n\n\n\nFigure 2: Data Visualization Matrix: Clarity vs Aesthetics\n\n\nFrom a definition perspective, aesthetics refers to how pleasing the visualization is to the reader’s eye, while clarity refers to how clear and concise the story or the message behind the visualization is conveyed to the reader."
  },
  {
    "objectID": "school/dataviz_1/index.html#critique-of-visualisation",
    "href": "school/dataviz_1/index.html#critique-of-visualisation",
    "title": "DataViz Makeover 1",
    "section": "4. Critique of Visualisation",
    "text": "4. Critique of Visualisation\nA critique of the above visualisation will be based on the earlier two criterions.\n\n4.1. Clarity:\nInconsistent vertical scale values intra and inter-countries\nThe vertical scales for both the primary and secondary axis denoting the export and import levels are inconsistent with one another for all six countries. For instance in Hong Kong, the export levels are denoted in millions, whereas the import levels are denoted in hundred thousands.\n\nUnmatched vertical scale values intra and inter-countries\nEven when the scales are denoted in millions, as shown in Japan, the export and import levels don’t match up with each other. In the same Japan instance, export seems to have a higher scale level than its import.\n\nInconsistent horizontal time scale values intra and inter-countries\nThe horizontal time scale values are inconsistent for Japan, as compared to the rest of the countries. Japan’s horizontal time scale only showcases the months of 2020, whereas the other five countries showcases the months of both 2019 and 2020.\n\nUnclear order of top six countries\nThere is no clear explanation on how the countries are arranged, and whether its order or arrangement takes into account the level of merchandise trade with Singapore. Can it be considered that HK is a more significant contributor of merchandise trade to Singapore, than US, solely based on its position within the visualisation?\n\n\n\n\n4.2. Aesthetics:\nLegend shows two colour schemes, but visualisation has three\nThe legend clearly asssigns two distinct colours to export and import respectively, but the graph showcases a third colour. This third colour is a result of the overlapping area between both the export and import portions. Having a third undefined colour forces the reader to decipher for him/herself on what it actually means.\n\nRepeating scale names\nThe vertical scales of Export and Import, as well the horizontal scales of ‘Month of Period’ is repeated six times throughout the whole visualisation.\n\nDiffering width\nExcluding Japan which has only a 2020 time period, the rest of the countries with a 2019-2020 time period have differing graph widths. Hong Kong has a narrower graph width than Malaysia, which is located just below it.\n\nLegend takes up too much valuable real estate\nSince the legend position is situated on the top right corner of the visualisation, the space below it on the right side of the visualisation is very much wasted. Relocating this legend either above or below, could have made the visualisation wider."
  },
  {
    "objectID": "school/dataviz_1/index.html#alternative-design",
    "href": "school/dataviz_1/index.html#alternative-design",
    "title": "DataViz Makeover 1",
    "section": "5. Alternative Design",
    "text": "5. Alternative Design\n\n5.1. Suggested Improvements\nThe use of an area chart in the original visualisation is ideal, given the fluctuations between the export and import levels across the time period. As such, the following suggestions are mere tweaks to the original visualisation:\n• Consistent and matching vertical and horizontal scales\n• Countries ordered in terms of overall merchandise trade\n• Keep to two colour schemes\n• Reduce repeated scale names\n• Consistent chart widths\n• Reposition or replace legend\n• Presence of a header/sub-header indicating the storyline of the visualisation (with leading commentaries on visualisation itself)\n\n\n5.2. Finding Inspiration\nReusing the same area charts, we now scour everywhere for great inspiration!\n\nTaking a leaf from history, the visualisation on Commercial and Political Atlas by William Playfair intuitively showcases the difference in levels within the area chart. The use of different colour schemes allow the reader to understand which areas have higher amounts than another. Though it only uses one area chart, it helps that this inspired visualisation is also based on exports and imports, similar to this makeover!\n\n\n\nFigure 3: Historical Visualisation on Export and Import Levels Between England and Two Other Countries (Denmark and Norway)\n\n\n\n\n5.3. Sketch\nThe following sketch strives to find common ground between the original visualisation and its critiques, as well as the historical inspiration above."
  },
  {
    "objectID": "school/dataviz_1/index.html#step-by-step-guide",
    "href": "school/dataviz_1/index.html#step-by-step-guide",
    "title": "DataViz Makeover 1",
    "section": "6. Step-by-Step Guide",
    "text": "6. Step-by-Step Guide\n\n6.1. Data Preparation\nThe original raw data reside within the following links: Merchandise Trade Webpage and Merchandise Trade Excel Download\n\nThe Excel file contains the following three sheets:\n\n\n\n\nCode\nmerc_contents_table &lt;- tibble(\n  `Sheet Name` = c(\"Content\",\n                \"T1\",\n                \"T2\"),\n  Description = c(\"Explaining sheets T1 and T2 contain the “Merchandise Imports By Region/Market, Monthly” and “Merchandise Exports By Region/Market, Monthly” respectively\",\n                 \"Contains the monthly merchandise imports by country/region in dollars (thousands for individual countries or millions for regions) spanning Jan-1976 to Apr-2021\",\n                 \"Contains the monthly merchandise exports by country/region in dollars (thousands for individual countries or millions for regions) spanning Jan-1976 to Apr-2021\")\n  ) |&gt;\n  gt() |&gt;\n  opt_row_striping() |&gt;\n  tab_header(\n    title = \"Contents of Merchandise Trade Excel File\"\n  ) |&gt;  \n  cols_width(\n    `Sheet Name` ~ px(120)\n  ) |&gt;\n  cols_align(\n    align = \"center\",\n    columns = `Sheet Name`\n  ) |&gt;\n  tab_style(style = cell_text(weight = \"bold\"), \n            locations = cells_column_labels()\n            )\n\nmerc_contents_table\n\n\n\n\n\n\n  \n    \n    \n  \n  \n    \n      Contents of Merchandise Trade Excel File\n    \n    \n    \n      Sheet Name\n      Description\n    \n  \n  \n    Content\nExplaining sheets T1 and T2 contain the “Merchandise Imports By Region/Market, Monthly” and “Merchandise Exports By Region/Market, Monthly” respectively\n    T1\nContains the monthly merchandise imports by country/region in dollars (thousands for individual countries or millions for regions) spanning Jan-1976 to Apr-2021\n    T2\nContains the monthly merchandise exports by country/region in dollars (thousands for individual countries or millions for regions) spanning Jan-1976 to Apr-2021\n  \n  \n  \n\n\n\n\nMicrosoft Excel will be utilised first for the following data preparation steps.\n\n\n\n\nCode\ninstruction_table &lt;- tibble(\n  `No.` = c(1:17),\n  Steps = c(\"Using Microsoft Excel file, open the raw data (outputFile.xlsx).\",\n           \"Since we need to make sure that the headers are in the first row, we will remove the first 5 rows in spreadsheet T1. Repeat step in spreadsheet T2.\",\n           \"Remove rows 2 to 8 in both spreadsheet T1 and T2, since these rows showcase cumulative amounts.\",\n           \"Trim the bottom of the table with commentary and no data, by removing rows 115 to 128 in spreadsheet T1. Similarly, remove rows 87 to 100 in T2.\",\n           \"Since the visualization is based on data only between the years 2019 and 2020, we will remove columns B to SW and TV to TY in both spreadsheet T1 and T2.\",\n           \"Since the dates in row 1 are not in proper date format, reformat dates to the first of the month from Jan-2019 onwards. Similarly, do this step on spreadsheet T2. Then rename cell A1 to Country.\",\n           \"Since we would need to differentiate both these spreadsheets after we consolidate them, we would first insert a column in column B and name it “Type”. Then fill the column with the value “Import” in T1.Insert a similar column in T2 but fill the column with the value “Export”.\",\n           \"For subsequent use in Tableau Desktop, we would need to change the table layout from a wide table to a long table. From the Data ribbon, select the button “From Table/Range”.\",\n           \"Ensure that $A$1:$Z$114 is selected and click OK. A Power Query Editor program will then launch.\",\n           \"Highlight the first two columns (Country and Type), and then within the Transform ribbon, select Unpivot Other Columns.\",\n           \"Rename the third and fourth column to Period and Trade_Amt.\",\n           \"To load the results back into Excel, within the Home (tab), select Close & Load. The transformed table will appear in a new worksheet called Table1.\",\n           \"To only extract the country name in the first column, insert a new column in column B and then use the following formula: \\\"=TRIM(LEFT([@Country],SEARCH(\\\"(\\\",[@Country])-1))\\\". To keep these values, copy and paste back these same values only.\",\n           \"Remove column A with the old values. Rename the header of the latest column A to Country. Since the Trade_Amt values are actually in thousands, we would need to reflect this accordingly. Multiply the values within the Trade_Amt column by a thousand. You have now completed the transformation for sheet T1.\",\n           \"Repeat steps 8 to 14 for sheet T2. Ensure that $A$1:$Z$86 is selected.\",\n           \"To create a consolidated file, copy and combine the transformed tables in steps 14 and 15 into a new workbook.\",\n           \"Since we just need the six countries, we will remove the other countries. Highlight columns A to D and go to the Data ribbon and select Filter. Uncheck the six countries (Hong Kong, Japan, Mainland China, Malaysia, Taiwan and US) from the Country filter and delete all rows from row 2 onwards. Then remove filter to repopulate the table with only the six countries. Save this file as \\\"Consol.xlsx\\\". This file will then be used for the visualisation on Tableau Desktop.\"),\n  Screenshots = c(local_image(\"img/1.png\", height = 100),\n                  local_image(\"img/2.png\", height = 100),\n                  local_image(\"img/3.png\", height = 100),\n                  local_image(\"img/4.png\", height = 100),\n                  local_image(\"img/5.png\", height = 100),\n                  local_image(\"img/6.png\", height = 50),\n                  local_image(\"img/7.png\", height = 75),\n                  local_image(\"img/8.png\", height = 75),\n                  local_image(\"img/9.png\", height = 100),\n                  local_image(\"img/10.png\", height = 50),\n                  local_image(\"img/11.png\", height = 50),\n                  local_image(\"img/12.png\", height = 100),\n                  local_image(\"img/13.png\", height = 50),\n                  local_image(\"img/14.png\", height = 75),\n                  local_image(\"img/15.png\", height = 50),\n                  local_image(\"img/16.png\", height = 100),\n                  local_image(\"img/17.png\", height = 200))\n) |&gt;\n  mutate(Screenshots = map(Screenshots, html)) |&gt;\n  gt() |&gt; \n  tab_header(\n    title = \"Data Preparation Steps Using MS Excel\"\n  ) |&gt;  \n  opt_row_striping() |&gt;\n  cols_width(\n    `No.` ~ px(30)\n  ) |&gt; \n  cols_align(\n    align = \"center\",\n    columns = `No.`\n  ) |&gt;\n  tab_style(style = cell_text(weight = \"bold\"), \n            locations = cells_column_labels()\n            )\n\ninstruction_table\n\n\n\n\n\n\n  \n    \n    \n    \n  \n  \n    \n      Data Preparation Steps Using MS Excel\n    \n    \n    \n      No.\n      Steps\n      Screenshots\n    \n  \n  \n    1\nUsing Microsoft Excel file, open the raw data (outputFile.xlsx).\n\n    2\nSince we need to make sure that the headers are in the first row, we will remove the first 5 rows in spreadsheet T1. Repeat step in spreadsheet T2.\n\n    3\nRemove rows 2 to 8 in both spreadsheet T1 and T2, since these rows showcase cumulative amounts.\n\n    4\nTrim the bottom of the table with commentary and no data, by removing rows 115 to 128 in spreadsheet T1. Similarly, remove rows 87 to 100 in T2.\n\n    5\nSince the visualization is based on data only between the years 2019 and 2020, we will remove columns B to SW and TV to TY in both spreadsheet T1 and T2.\n\n    6\nSince the dates in row 1 are not in proper date format, reformat dates to the first of the month from Jan-2019 onwards. Similarly, do this step on spreadsheet T2. Then rename cell A1 to Country.\n\n    7\nSince we would need to differentiate both these spreadsheets after we consolidate them, we would first insert a column in column B and name it “Type”. Then fill the column with the value “Import” in T1.Insert a similar column in T2 but fill the column with the value “Export”.\n\n    8\nFor subsequent use in Tableau Desktop, we would need to change the table layout from a wide table to a long table. From the Data ribbon, select the button “From Table/Range”.\n\n    9\nEnsure that $A$1:$Z$114 is selected and click OK. A Power Query Editor program will then launch.\n\n    10\nHighlight the first two columns (Country and Type), and then within the Transform ribbon, select Unpivot Other Columns.\n\n    11\nRename the third and fourth column to Period and Trade_Amt.\n\n    12\nTo load the results back into Excel, within the Home (tab), select Close & Load. The transformed table will appear in a new worksheet called Table1.\n\n    13\nTo only extract the country name in the first column, insert a new column in column B and then use the following formula: \"=TRIM(LEFT([@Country],SEARCH(\"(\",[@Country])-1))\". To keep these values, copy and paste back these same values only.\n\n    14\nRemove column A with the old values. Rename the header of the latest column A to Country. Since the Trade_Amt values are actually in thousands, we would need to reflect this accordingly. Multiply the values within the Trade_Amt column by a thousand. You have now completed the transformation for sheet T1.\n\n    15\nRepeat steps 8 to 14 for sheet T2. Ensure that $A$1:$Z$86 is selected.\n\n    16\nTo create a consolidated file, copy and combine the transformed tables in steps 14 and 15 into a new workbook.\n\n    17\nSince we just need the six countries, we will remove the other countries. Highlight columns A to D and go to the Data ribbon and select Filter. Uncheck the six countries (Hong Kong, Japan, Mainland China, Malaysia, Taiwan and US) from the Country filter and delete all rows from row 2 onwards. Then remove filter to repopulate the table with only the six countries. Save this file as \"Consol.xlsx\". This file will then be used for the visualisation on Tableau Desktop.\n\n  \n  \n  \n\n\n\n\n\n\n6.2. Data Visualisation\nHaving prepared the data, we will now utilize Tableau Desktop for the eventual visualisation.\n\n\n\n\nCode\ninstruction_table2 &lt;- tibble(\n  `No.` = c(1:29),\n  Steps = c(\"Open Tableau Desktop. Then connect to MS Excel and import the earlier Consol file.\",\n           \"From the bottom bar, select Sheet 1. Then drag Country and Period to Columns, and Trade Amt to Rows.\",\n           \"Let's change the Period to a Month-Year date format. On the left Data bar, select the dropdown menu for Period. Change Data Type to Date.\",\n           \"From the Columns bar at the top of the screen, select the dropdown menu for Period. Select Month-Year.\",\n           \"To differentiate Export and Import using colour, drag Type to Colour Marks\",\n           \"Within the Marks dropdown menu, select Area.\",\n           \"Within the Colour Marks, select 100% Opacity.\",\n           \"Since the Export and Import values should not be stacked on top of each other, we will need to separate them. Select Analysis from the toolbar, then select Stack Marks, then select Off\",\n           \"To know the lowest level of either Export and Import for each month, a calculated field will need to be created. Select Trade Amt from the left Data bar, and create a Calculated Field\",\n           \"Name this calculated field, Lowest_Level. Then use the following formula: {EXCLUDE [Type]:MIN([Trade Amt])}. Check that the calculation is valid.\",\n           \"Drag the newly created Lowest_Level field to the vertical axis of Trade Amt. An icon with two verical bars should appear on the cursor.\",\n           \"Drag Measure Names from Rows at the top onto Detail Marks.\",\n           \"Click on the Detail Marks icon beside Measure Names and select Colour\",\n           \"To hide the Lowest_Level area, select Measure Names in the Marks column, then select Colour marks and Edit Colours. Select the colour White on both Lowest_Level fields\",\n           \"Drag Trade Amt to Rows and change Sum Trade Amt to Line.\",\n           \"Remove Measure Names from the Marks section, by dragging it out of the window.\",\n           \"To combine both graphs, right-click on Trade Amt vertical axis and select Dual Axis.\",\n           \"Right-click on the Trade Amt secondary axis, and select both Synchronize Axis and hide Show Header.\",\n           \"To remove the Country header, select Country at the top and Hide Fields Labels for Columns.\",\n           \"Likewise to hide the Sheet 1 header, select Sheet 1 at the top and select Hide Title.\",\n           \"To remove the horizontal grid lines, select Format and choose Lines. Then within the Format Lines window, select None for Grid Lines within the Rows tab.\",\n           \"We will manually reorder the countries in descending order of trade from left to right. Since Hong Kong is an anomaly with a large export-import gap, we will place it at the right most placement. Reorder the countries by dragging the country names into their respective positions as per the image.\",\n           \"Rename the Y-axis to Merchandise Trade Amt by right-clicking on the Y-axis and select Edit Axis.\",\n           \"Right-click on the horizontal time axis and select Edit Axis. Within the Tick Marks tab, select Fixed major tick marks, input 1/1/2019 as the tick origin and set the Interval at 2.\",\n           \"Then within the General tab, remove Month of Period from the Axis Title. Click OK. \",\n           \"To remove the vertical lines that separate the countries, right click on the visualisation, and select Format. At the top, choose the fourth icon called Borders. Under Column Divider, select Pane and then choose None. \",\n           \"To change the area under the lowest level lines to a grey tinge, select Measure Values within the Marks window and edit colors. For each of the Lowest Level data items, choose Val 245 or HTML color of #f5f5f5.\",\n           \"To change the area above the graphs to the same grey tinge, right-click on the horizontal time axis and select Add Reference Line. Then choose the Band icon, and select Entire Table as the Scope. Within the Band From, input 1/1/2019 and Constant within the value and choose None for the label. Within the Band To, input 1/12/2020 and Constant within the value and choose None for the label. Lastly within Fill, choose More Colors and select Val 245 or HTML color of #f5f5f5. \",\n           \"To change the fonts of the Country Headers, right-click on the countries and select Format. Choose Tableau Semibold for its font, size 18 and choose Bold. Then rename Sheet 1 to Viz and save your work.\"),\n  Screenshots = c(local_image(\"img/18.png\", height = 150),\n                  local_image(\"img/19.png\", height = 100),\n                  local_image(\"img/20.png\", height = 200),\n                  local_image(\"img/21.png\", height = 200),\n                  local_image(\"img/22.png\", height = 150),\n                  local_image(\"img/23.png\", height = 200),\n                  local_image(\"img/24.png\", height = 200),\n                  local_image(\"img/25.png\", height = 200),\n                  local_image(\"img/26.png\", height = 200),\n                  local_image(\"img/27.png\", height = 100),\n                  local_image(\"img/28.png\", height = 150),\n                  local_image(\"img/29.png\", height = 100),\n                  local_image(\"img/30.png\", height = 200),\n                  local_image(\"img/31.png\", height = 100),\n                  local_image(\"img/32.png\", height = 200),\n                  local_image(\"img/33.png\", height = 200),\n                  local_image(\"img/34.png\", height = 200),\n                  local_image(\"img/35.png\", height = 200),\n                  local_image(\"img/36.png\", height = 50),\n                  local_image(\"img/37.png\", height = 100),\n                  local_image(\"img/38.png\", height = 200),\n                  local_image(\"img/39.png\", height = 150),\n                  local_image(\"img/40.png\", height = 200),\n                  local_image(\"img/41.png\", height = 100),\n                  local_image(\"img/42.png\", height = 200),\n                  local_image(\"img/43.png\", height = 300),\n                  local_image(\"img/44.png\", height = 100),\n                  local_image(\"img/45.png\", height = 200),\n                  local_image(\"img/46.png\", height = 200))\n) |&gt;\n  mutate(Screenshots = map(Screenshots, html)) |&gt;\n  gt() |&gt; \n  tab_header(\n    title = \"Data Visualisation Steps Using Tableau Desktop\"\n  ) |&gt;  \n  opt_row_striping() |&gt;\n  cols_width(\n    `No.` ~ px(30)\n  ) |&gt; \n  cols_align(\n    align = \"center\",\n    columns = `No.`\n  ) |&gt;\n  tab_style(style = cell_text(weight = \"bold\"), \n            locations = cells_column_labels()\n            )\n\ninstruction_table2\n\n\n\n\n\n\n  \n    \n    \n    \n  \n  \n    \n      Data Visualisation Steps Using Tableau Desktop\n    \n    \n    \n      No.\n      Steps\n      Screenshots\n    \n  \n  \n    1\nOpen Tableau Desktop. Then connect to MS Excel and import the earlier Consol file.\n\n    2\nFrom the bottom bar, select Sheet 1. Then drag Country and Period to Columns, and Trade Amt to Rows.\n\n    3\nLet's change the Period to a Month-Year date format. On the left Data bar, select the dropdown menu for Period. Change Data Type to Date.\n\n    4\nFrom the Columns bar at the top of the screen, select the dropdown menu for Period. Select Month-Year.\n\n    5\nTo differentiate Export and Import using colour, drag Type to Colour Marks\n\n    6\nWithin the Marks dropdown menu, select Area.\n\n    7\nWithin the Colour Marks, select 100% Opacity.\n\n    8\nSince the Export and Import values should not be stacked on top of each other, we will need to separate them. Select Analysis from the toolbar, then select Stack Marks, then select Off\n\n    9\nTo know the lowest level of either Export and Import for each month, a calculated field will need to be created. Select Trade Amt from the left Data bar, and create a Calculated Field\n\n    10\nName this calculated field, Lowest_Level. Then use the following formula: {EXCLUDE [Type]:MIN([Trade Amt])}. Check that the calculation is valid.\n\n    11\nDrag the newly created Lowest_Level field to the vertical axis of Trade Amt. An icon with two verical bars should appear on the cursor.\n\n    12\nDrag Measure Names from Rows at the top onto Detail Marks.\n\n    13\nClick on the Detail Marks icon beside Measure Names and select Colour\n\n    14\nTo hide the Lowest_Level area, select Measure Names in the Marks column, then select Colour marks and Edit Colours. Select the colour White on both Lowest_Level fields\n\n    15\nDrag Trade Amt to Rows and change Sum Trade Amt to Line.\n\n    16\nRemove Measure Names from the Marks section, by dragging it out of the window.\n\n    17\nTo combine both graphs, right-click on Trade Amt vertical axis and select Dual Axis.\n\n    18\nRight-click on the Trade Amt secondary axis, and select both Synchronize Axis and hide Show Header.\n\n    19\nTo remove the Country header, select Country at the top and Hide Fields Labels for Columns.\n\n    20\nLikewise to hide the Sheet 1 header, select Sheet 1 at the top and select Hide Title.\n\n    21\nTo remove the horizontal grid lines, select Format and choose Lines. Then within the Format Lines window, select None for Grid Lines within the Rows tab.\n\n    22\nWe will manually reorder the countries in descending order of trade from left to right. Since Hong Kong is an anomaly with a large export-import gap, we will place it at the right most placement. Reorder the countries by dragging the country names into their respective positions as per the image.\n\n    23\nRename the Y-axis to Merchandise Trade Amt by right-clicking on the Y-axis and select Edit Axis.\n\n    24\nRight-click on the horizontal time axis and select Edit Axis. Within the Tick Marks tab, select Fixed major tick marks, input 1/1/2019 as the tick origin and set the Interval at 2.\n\n    25\nThen within the General tab, remove Month of Period from the Axis Title. Click OK. \n\n    26\nTo remove the vertical lines that separate the countries, right click on the visualisation, and select Format. At the top, choose the fourth icon called Borders. Under Column Divider, select Pane and then choose None. \n\n    27\nTo change the area under the lowest level lines to a grey tinge, select Measure Values within the Marks window and edit colors. For each of the Lowest Level data items, choose Val 245 or HTML color of #f5f5f5.\n\n    28\nTo change the area above the graphs to the same grey tinge, right-click on the horizontal time axis and select Add Reference Line. Then choose the Band icon, and select Entire Table as the Scope. Within the Band From, input 1/1/2019 and Constant within the value and choose None for the label. Within the Band To, input 1/12/2020 and Constant within the value and choose None for the label. Lastly within Fill, choose More Colors and select Val 245 or HTML color of #f5f5f5. \n\n    29\nTo change the fonts of the Country Headers, right-click on the countries and select Format. Choose Tableau Semibold for its font, size 18 and choose Bold. Then rename Sheet 1 to Viz and save your work.\n\n  \n  \n  \n\n\n\n\n\n\n6.3. Creating the Static Dashboard\nHaving visualised the data, we will now put the final touches on a dashboard.\n\n\n\n\nCode\ninstruction_table3 &lt;- tibble(\n  `No.` = c(1:11),\n  Steps = c(\"Create a new Dashboard. On the top left, select Default Dashboard, and then select Automatic as its size\",\n           \"Let's create a header. Within Objects, click on Floating. Drag Text to the main window. Type 'Singapore's Top Six Trading Countries Greatly Differ in Both Export And Import'. Choose Tableau Semibold font, size 18 and black colour.\",\n           \"To replace the legend, we will colour code the words Export and Import. Highlight the Export word, and choose HTML color #4e79a7. Do the same for the Import word, but this time, choose HTML color #f28e2b\",\n           \"To perfectly align the header, select the text and then the layout column. Follow these dimensions, x of 2, y of 1, w of 1639 and h of 58. Do note that your width might differ from the picture, given your respective screen resolution.\",\n           \"Let's now create a sub-header in similar fashion. Within Objects, click on Floating. Drag Text to the main window. Type 'The import and export fluctuations within each of its top six trading countries differ greatly between 2018 to 2020, as well as against other top trading countries.'. Choose Tableau Book font, size 12 and black colour.\",\n           \"As per before, to perfectly align the sub-header, select the text and then the layout column. Follow these dimensions, x of 2, y of 46, w of 1639 and h of 69.\",\n           \"Next drag the newly created Viz sheet into the dashboard and remove its legend.\",\n           \"To nicely align the Viz sheet, follow these dimensions, x of 2, y of 90, w of 1639 and h of 780.\",\n           \"Reorder the Item Hierarchy such that Viz is on the lowest level.\",\n           \"To annotate our comments, go back to the Viz worksheet, right-click a point on the graph and select Annotate, Point. Then enter the necessary comments and choose size 8 font, and click OK. To remove the background box shading, select None for shading within the Format Annotation window.\",\n           \"Rename your dashboard as Viz1 and then save this dashboard to Tableau Public.\"),\n  Screenshots = c(local_image(\"img/47.png\", height = 200),\n                  local_image(\"img/48.png\", height = 100),\n                  local_image(\"img/49.png\", height = 100),\n                  local_image(\"img/50.png\", height = 50),\n                  local_image(\"img/51.png\", height = 100),\n                  local_image(\"img/52.png\", height = 50),\n                  local_image(\"img/53.png\", height = 100),\n                  local_image(\"img/54.png\", height = 100),\n                  local_image(\"img/55.png\", height = 100),\n                  local_image(\"img/55a.png\", height = 100),\n                  local_image(\"img/56.png\", height = 200))\n) |&gt;\n  mutate(Screenshots = map(Screenshots, html)) |&gt;\n  gt() |&gt; \n  tab_header(\n    title = \"Data Visualisation Steps Using Tableau Desktop\"\n  ) |&gt;  \n  opt_row_striping() |&gt;\n  cols_width(\n    `No.` ~ px(30)\n  ) |&gt; \n  cols_align(\n    align = \"center\",\n    columns = `No.`\n  ) |&gt;\n  tab_style(style = cell_text(weight = \"bold\"), \n            locations = cells_column_labels()\n            )\n\ninstruction_table3\n\n\n\n\n\n\n  \n    \n    \n    \n  \n  \n    \n      Data Visualisation Steps Using Tableau Desktop\n    \n    \n    \n      No.\n      Steps\n      Screenshots\n    \n  \n  \n    1\nCreate a new Dashboard. On the top left, select Default Dashboard, and then select Automatic as its size\n\n    2\nLet's create a header. Within Objects, click on Floating. Drag Text to the main window. Type 'Singapore's Top Six Trading Countries Greatly Differ in Both Export And Import'. Choose Tableau Semibold font, size 18 and black colour.\n\n    3\nTo replace the legend, we will colour code the words Export and Import. Highlight the Export word, and choose HTML color #4e79a7. Do the same for the Import word, but this time, choose HTML color #f28e2b\n\n    4\nTo perfectly align the header, select the text and then the layout column. Follow these dimensions, x of 2, y of 1, w of 1639 and h of 58. Do note that your width might differ from the picture, given your respective screen resolution.\n\n    5\nLet's now create a sub-header in similar fashion. Within Objects, click on Floating. Drag Text to the main window. Type 'The import and export fluctuations within each of its top six trading countries differ greatly between 2018 to 2020, as well as against other top trading countries.'. Choose Tableau Book font, size 12 and black colour.\n\n    6\nAs per before, to perfectly align the sub-header, select the text and then the layout column. Follow these dimensions, x of 2, y of 46, w of 1639 and h of 69.\n\n    7\nNext drag the newly created Viz sheet into the dashboard and remove its legend.\n\n    8\nTo nicely align the Viz sheet, follow these dimensions, x of 2, y of 90, w of 1639 and h of 780.\n\n    9\nReorder the Item Hierarchy such that Viz is on the lowest level.\n\n    10\nTo annotate our comments, go back to the Viz worksheet, right-click a point on the graph and select Annotate, Point. Then enter the necessary comments and choose size 8 font, and click OK. To remove the background box shading, select None for shading within the Format Annotation window.\n\n    11\nRename your dashboard as Viz1 and then save this dashboard to Tableau Public."
  },
  {
    "objectID": "school/dataviz_1/index.html#final-visualisation",
    "href": "school/dataviz_1/index.html#final-visualisation",
    "title": "DataViz Makeover 1",
    "section": "7. Final Visualisation",
    "text": "7. Final Visualisation\n\n7.1. Visualisation Snapshot\nTo gain access to this dashboard on Tableau Public, click on this link. This dashboard is best viewed on a secondary monitor.\n\n\n\n\n7.2. Derived Insights\n\nAmongst the six countries, Hong Kong had the biggest gap in merchandise trade amount between export and import. Singapore’s exports to HK far outstrip HK’s imports into Singapore, with Mar-2020 recording the widest 2 year gap.\n\nThe converse is true for Taiwan. Taiwan’s imports to Singapore consistently outstrip Singapore’s exports to Taiwan, with Oct-2020 recording the widest 2 year gap.\n\nFor the United States, import levels were consistently higher prior to Apr-2020. From this point onwards, export to the United States became consistently higher.\n\nSimilarly for Malaysia, from Dec-2021 onwards, import consistently outgrew export. Prior to this, export and import levels greatly overlapped each other.\n\nThough at different levels, both the export and import levels within Mainland China and Japan fluctuate consistently."
  },
  {
    "objectID": "school/dataviz_2/index.html",
    "href": "school/dataviz_2/index.html",
    "title": "DataViz Makeover 2",
    "section": "",
    "text": "Critic the visualized graph with at least three points from each evaluation criterion (clarity and aesthetics) respectively.\nSuggest alternative graphical presentation to improve the current design:\n\nProposed alternative data visualization must be in static form.\nProposed design would need to be sketched.\nSupport your design by describing the advantages or which part of the issue(s) your alternative design attempts to overcome.\n\nUse Tableau to design the proposed data visualization.\nProvide step-by-step description on how the data visualization was prepared.\nDescribe three major observations revealed by the prepared data visualization.\n\n\n\n\n\nThe study period should be between January 2011-December 2020. No additional data sets are required.\n\nFocus on telling data story using appropriate statistical graphic methods, graphical design principles and interactive techniques, including animation."
  },
  {
    "objectID": "school/dataviz_2/index.html#makeover-requirements",
    "href": "school/dataviz_2/index.html#makeover-requirements",
    "title": "DataViz Makeover 2",
    "section": "",
    "text": "Critic the visualized graph with at least three points from each evaluation criterion (clarity and aesthetics) respectively.\nSuggest alternative graphical presentation to improve the current design:\n\nProposed alternative data visualization must be in static form.\nProposed design would need to be sketched.\nSupport your design by describing the advantages or which part of the issue(s) your alternative design attempts to overcome.\n\nUse Tableau to design the proposed data visualization.\nProvide step-by-step description on how the data visualization was prepared.\nDescribe three major observations revealed by the prepared data visualization.\n\n\n\n\n\nThe study period should be between January 2011-December 2020. No additional data sets are required.\n\nFocus on telling data story using appropriate statistical graphic methods, graphical design principles and interactive techniques, including animation."
  },
  {
    "objectID": "school/dataviz_2/index.html#original-visualisation-and-its-perceived-objective",
    "href": "school/dataviz_2/index.html#original-visualisation-and-its-perceived-objective",
    "title": "DataViz Makeover 2",
    "section": "2. Original Visualisation and Its Perceived Objective",
    "text": "2. Original Visualisation and Its Perceived Objective\nThe original visualization (shown below) showcases the merchandise trade, for both export and import, of ten of Singapore’s major trading partners in 2020.\n\n\n\nUsing the same methodology in our earlier DataViz Makeover 1, the intent of the visualisation would need to be defined first. We attempt the narrative for the above visualization, as follows:\n\n“In 2020, each of our top ten trading countries (Mainland China, Malaysia, EU, United States, Taiwan, Japan, Republic of Korea, Indonesia, Hong Kong and Thailand) differ greatly in their respective export, import as well as total merchandise trade value.”\n\nTo further illuminate the narrative, we propose the following questions that the visualization aim to answer. Given the subject matter, these questions are very similar to those proposed in our earlier makeover:\n\nWhat are the import and export levels in each of the top ten trading countries?\n\nHow do these import and export levels fluctuate over time?\n\nWhich countries are net exporter or net importer at varying periods in time?\n\nWithin the top ten countries, who has the highest and lowest import and export levels respectively?\n\n(Additional) Do these fluctuations coincide with any event?"
  },
  {
    "objectID": "school/dataviz_2/index.html#critiquing-methodology",
    "href": "school/dataviz_2/index.html#critiquing-methodology",
    "title": "DataViz Makeover 2",
    "section": "3. Critiquing Methodology",
    "text": "3. Critiquing Methodology\nThe methodology will remain the same as before, where we will assess on clarity and aesthetics."
  },
  {
    "objectID": "school/dataviz_2/index.html#critique-of-visualisation",
    "href": "school/dataviz_2/index.html#critique-of-visualisation",
    "title": "DataViz Makeover 2",
    "section": "4. Critique of Visualisation",
    "text": "4. Critique of Visualisation\n\n4.1. Clarity:\n\nLack of proper scales on both x and y axis\nThough each bubble has a merchandise trading amount, the x and y axis lacks such a scale. The values within both the merchandise trading amount and the xy axis does not match either. We’re unsure whether the scales refer to millions, billions or even kilograms.\n\n\nBottom insight does not match with the visualisation\nThe bottom insight made reference to the difference in Singapore’s standing between two separate trading partner from 2006 and 2009 respectively. The visualisation however shows the trading values only for the year 2020.\n\n\nAxis names are misleading\n‘Imports’,though situated near to the x-axis, actually refers to the y-axis, and vice-versa for the ‘Exports’ icon. The reader would need to take into account the wordings of ‘net importers’ and ‘net exporters’ to realise that the axis names are somewhat incorrectly placed.\n\n\n\n4.2. Aesthetics:\n\nUnnecessary colours for each country\nThe ten countries each have their own colour scheme to differentiate one from the other. Differentiation has already been achieved from the names of the countries within their respective infographic bubbles.\n\n\nThe bubbles hide behind each other\nSimilar sized bubbles seem to be grouped closely together. Naturally some would be hidden behind others. When the cursor hovers each bubble, the bubbles move forward to display the country name and merchandise trade value. The user wouldn’t have noticed the availability of such interactivity, given the design is very much like a static image.\n\n\nThe infographic bubbles take up too much space\nThe infographic bubbles indicating the name of the trading partner, as well as its total merchandise trade value take up far too much space within the small visualisation."
  },
  {
    "objectID": "school/dataviz_2/index.html#alternative-design-considerations",
    "href": "school/dataviz_2/index.html#alternative-design-considerations",
    "title": "DataViz Makeover 2",
    "section": "5. Alternative Design Considerations",
    "text": "5. Alternative Design Considerations\n\n5.1. Suggested Improvements\nThe use of the bubble chart as per the original visualisation will be continued, given the use of three continuous data (export, import and merchandise trade value). As such, the following suggestions are mere tweaks to the original visualisation:\n\nProper scales on both x and y axis\n\nRightly sized infographic bubbles (if needed)\n\nShowing trade movements on and before 2020 (perhaps from 2011 onwards)\n\nRight level of transparency to show location of bubbles as well as other overlapping bubbles\n\nSeeing the bubble positions in other years before 2020\n\nBeing able to see the export, import and total merchandise trade over time\n\n\n\n5.2. Finding Inspiration\nWhile we intend to reuse the original bubble chart design, we scour the internet for inspiration!\nThe chart that comes to mind when comparing multiple countries across two continuous variable was Dr Hans Rosling’s famous charts on global trends. In his famous Ted talk, he covered two charts across a lengthy time frame: one chart covers the life expectancy in each country against its fertility rate, while the other chart covers the child survival rate in each country against its GDP per capita.\n\n\n\nSeparately, while the original bubble chart design will be reused, it’s not able to show the change across time. This infographic from Upslide below shows the right chart type to consider, based on the nature of data that needs to be presented. Though this infographic is meant for charts within Excel, its application is universal. From the infographic, the best chart to compare non-cyclical data over many periods is the line chart. Though we will use this as a complement to the bubble chart, we would need to have a solution of fitting lines for ten different countries in a visually appealing manner.\n\n\n\nYvan Fornes’s work on visualising ten different categories across ten years (2005 to 2014) seems an apt approach to follow, as we visualise ten different countries’ and their respective export and import volumes across 2011 to 2020.\n\n\n\n\n5.3. Sketch\nThe following sketch strives to find common ground between the original visualisation and its critiques, as well as incorporating the inspirations above."
  },
  {
    "objectID": "school/dataviz_2/index.html#proposed-tableau-visualisation",
    "href": "school/dataviz_2/index.html#proposed-tableau-visualisation",
    "title": "DataViz Makeover 2",
    "section": "6. Proposed Tableau Visualisation",
    "text": "6. Proposed Tableau Visualisation\nClick on this Tableau Public link to view the visualisation."
  },
  {
    "objectID": "school/dataviz_2/index.html#step-by-step-guide",
    "href": "school/dataviz_2/index.html#step-by-step-guide",
    "title": "DataViz Makeover 2",
    "section": "7. Step-by-Step Guide",
    "text": "7. Step-by-Step Guide\n\n6.1. Data Preparation\nThe original raw data reside within the following links: Merchandise Trade Webpage and Merchandise Trade Excel Download\n\nThe Excel file contains the following three sheets:\n\n\n\n\nCode\nSheet_Name &lt;- c(\"Content\",\"T1\",\"T2\")\nDescription &lt;- c(\"Explaining sheets T1 and T2 contain the “Merchandise Imports By Region/Market, Monthly” and “Merchandise Exports By Region/Market, Monthly” respectively\",\n                 \"Contains the monthly merchandise imports by country/region in dollars (thousands for individual countries or millions for regions) spanning Jan-1976 to Apr-2021\",\n                 \"Contains the monthly merchandise exports by country/region in dollars (thousands for individual countries or millions for regions) spanning Jan-1976 to Apr-2021\")\ntable1 &lt;- data.frame(Sheet_Name,Description)\nknitr::kable(table1, booktabs = TRUE,\n  caption = 'Contents of Merchandise Trade Excel File')\n\n\n\nContents of Merchandise Trade Excel File\n\n\n\n\n\n\nSheet_Name\nDescription\n\n\n\n\nContent\nExplaining sheets T1 and T2 contain the “Merchandise Imports By Region/Market, Monthly” and “Merchandise Exports By Region/Market, Monthly” respectively\n\n\nT1\nContains the monthly merchandise imports by country/region in dollars (thousands for individual countries or millions for regions) spanning Jan-1976 to Apr-2021\n\n\nT2\nContains the monthly merchandise exports by country/region in dollars (thousands for individual countries or millions for regions) spanning Jan-1976 to Apr-2021\n\n\n\n\n\nUnlike the earlier makeover, this makeover will utilise Tableau Prep Builder to prepare the data.\n\n\n\n\nCode\nNo. &lt;- c(1:19)\nSteps &lt;- c(\"To bring the data into Tableau Prep Builder, first unzip the output folder. Then load the output Excel file into Prep Builder. Drag spreadsheet T1 into the main window. Rename T1 to Import and start the cleaning node.\",\n           \"To cleanly separate the country names, split the Variables column using the Custom Split and select \\\"(\\\".\",\n           \"After renaming this newly split column as Country, filter using Selected Values. Exclude Africa, America, Asia, Europe, Oceania and Total Merchandise Imports\",\n           \"Add a Pivot node to the Clean 1 node. To pivot just the dates, select all date values from 1976 Apr to 2021 Apr and drag to the central pane. \",\n           \"Then change format to Date.\",\n           \"Rename to Month and Import (Thousand Dollars).\",\n           \"We will now replicate these same steps for the Export file. First, drag spreadsheet T2 into the main window.\",\n           \"Rename T2 to Export. Then copy both the Clean 1 and Pivot nodes and paste them next to the Export node\",\n           \"To connect these nodes together, drag the Export node to the Clean 1(1) node over the Add function.\",\n           \"Since these are export values, replace the Import (Thousand Dollars) to Export (Thousand Dollars) in the Pivot 1(1) node.\",\n           \"To combine both the Import and Export data, drag Pivot 1 to Pivot 1(1) to create a new Join node.\",\n           \"Within the Join Clauses, match both the Country and Month columns to each other. Ensure that the full Join Type is selected.\",\n           \"Attach a new Clean node to the Join 1 node. Since there are duplicate columns, merge both Month columns first. Then merge both Country columns.\",\n           \"We will now need to pivot the export and import fields. We will add a Pivot node to the Clean 3 node. We then drag the Export and Import columns into the centre pane.\",\n           \"We will extract out just the Export and Import words, through the use of the Automatic Split. The newly created column will be renamed as Trade Type.\",\n           \"Most of the trade value are in thousands. We now need to convert this to its proper format. Taking into account that European Union is the only trading partner whose trade value is in the millions, we will create a calculated field using the following formula.\",\n           \"To limit our timeframe, we will add a Clean node. We will then keep only the months between 2011 to 2020.\",\n           \"At the same time, we will select our countries of choice by keeping these ten countries using the Filter function.\",\n           \"After keeping only the Trade Value, Trade Type, Country and Month columns, we will then add the Output node. Save the file in a Tableau hyper format for use in Tableau Desktop.\")\nScreenshots &lt;- c(\"img/a.png\",\n                 \"img/b.png\",\n                 \"img/de.png\",\n                 \"img/f.png\",\n                 \"img/g.png\",\n                 \"img/h.png\",\n                 \"img/i.png\",\n                 \"img/j.png\",\n                 \"img/k.png\",\n                 \"img/l.png\",\n                 \"img/m.png\",\n                 \"img/n.png\",\n                 \"img/o.png\",\n                 \"img/p.png\",\n                 \"img/q.png\",\n                 \"img/r.png\",\n                 \"img/s.png\",\n                 \"img/t.png\",\n                 \"img/u.png\")\n\ntable2 &lt;- data.frame(No.,Steps,Screenshots)\n\n# Add appropriate rmarkdown tagging\ntable2$Screenshots = sprintf(\"![](%s)\", table2$Screenshots)\n\nknitr::kable(table2, booktabs = TRUE,\n  caption = 'Data Preparation Steps Using Tableau Prep Builder')\n\n\n\nData Preparation Steps Using Tableau Prep Builder\n\n\n\n\n\n\n\nNo.\nSteps\nScreenshots\n\n\n\n\n1\nTo bring the data into Tableau Prep Builder, first unzip the output folder. Then load the output Excel file into Prep Builder. Drag spreadsheet T1 into the main window. Rename T1 to Import and start the cleaning node.\n\n\n\n2\nTo cleanly separate the country names, split the Variables column using the Custom Split and select “(”.\n\n\n\n3\nAfter renaming this newly split column as Country, filter using Selected Values. Exclude Africa, America, Asia, Europe, Oceania and Total Merchandise Imports\n\n\n\n4\nAdd a Pivot node to the Clean 1 node. To pivot just the dates, select all date values from 1976 Apr to 2021 Apr and drag to the central pane.\n\n\n\n5\nThen change format to Date.\n\n\n\n6\nRename to Month and Import (Thousand Dollars).\n\n\n\n7\nWe will now replicate these same steps for the Export file. First, drag spreadsheet T2 into the main window.\n\n\n\n8\nRename T2 to Export. Then copy both the Clean 1 and Pivot nodes and paste them next to the Export node\n\n\n\n9\nTo connect these nodes together, drag the Export node to the Clean 1(1) node over the Add function.\n\n\n\n10\nSince these are export values, replace the Import (Thousand Dollars) to Export (Thousand Dollars) in the Pivot 1(1) node.\n\n\n\n11\nTo combine both the Import and Export data, drag Pivot 1 to Pivot 1(1) to create a new Join node.\n\n\n\n12\nWithin the Join Clauses, match both the Country and Month columns to each other. Ensure that the full Join Type is selected.\n\n\n\n13\nAttach a new Clean node to the Join 1 node. Since there are duplicate columns, merge both Month columns first. Then merge both Country columns.\n\n\n\n14\nWe will now need to pivot the export and import fields. We will add a Pivot node to the Clean 3 node. We then drag the Export and Import columns into the centre pane.\n\n\n\n15\nWe will extract out just the Export and Import words, through the use of the Automatic Split. The newly created column will be renamed as Trade Type.\n\n\n\n16\nMost of the trade value are in thousands. We now need to convert this to its proper format. Taking into account that European Union is the only trading partner whose trade value is in the millions, we will create a calculated field using the following formula.\n\n\n\n17\nTo limit our timeframe, we will add a Clean node. We will then keep only the months between 2011 to 2020.\n\n\n\n18\nAt the same time, we will select our countries of choice by keeping these ten countries using the Filter function.\n\n\n\n19\nAfter keeping only the Trade Value, Trade Type, Country and Month columns, we will then add the Output node. Save the file in a Tableau hyper format for use in Tableau Desktop.\n\n\n\n\n\n\n\n\n6.2. Data Visualisation\nHaving prepared the data, we will now utilize Tableau Desktop for the eventual visualisation.\n\n\n\n\nCode\nNo. &lt;- c(1:32)\nSteps &lt;- c(\"Launch Tableau Desktop by double-clicking on the newly created hyper file. You should get this view. To start creating the first chart (bubble chart), let's click on Sheet 1 at the bottom.\",\n           \"We will need to create a few Calculated Fields. To create the first calculated field, click on the Analysis tab, and choose Calculated Field from the dropdown menu.\",\n           \"First we need to separate the export and import values. We create a calculated field and then rename to Export. Then enter the following formula: IF [Trade Type] = \\\"Export\\\" THEN [Trade Value] END. Ensure that there is a remark indicating that the calculation is valid. Then click OK. To repeat the same step for Import, create a calculated field and then rename to Import. Enter the following formula: IF [Trade Type] = \\\"Import\\\" THEN [Trade Value] END. Thereafter, click OK.\",\n           \"Our last calculated field for this chart will be to create a total trade value, combining both export and import. Name this calculated field as Total Trade Value. Enter the following formula: SUM([Export])+SUM([Import])\",\n           \"Drag Export to Columns, Import to Rows, Country to Detail Marks and Total Trade Value to Size Marks.\",\n           \"To mimic Hans Rosling's yearly animation, we will then drag Month to Pages. Then right-click this and choose Cumulative Year.\",\n           \"To switch on the animation feature, select the Format tab, and choose Animations from the dropdown menu. Then select On.\",\n           \"To fix the axis ranges as the animation plays, first right-click on the vertical Import axis. Choose Edit Axis. Then select Fixed Range and enter 0 and 90,000,000,000. Repeat the same step for the horizontal Export axis by right-clicking on it.\",\n           \"To format the bubbles accordingly, we first resize them by clicking on the Size marks and choosing the centre. Then we change the marks type, by clicking on Automatic and choosing Circle. This will allow us to have a fill colour. To colour this circle, click on the Color marks and choose a dark grey tone (HTML #555555) Within the same window, choose 70% opacity. Overlapping circles will now be seen easily, compared to full opacity. Lastly, to include the Country names to each bubble, drag Country to the Label marks.\",\n           \"A few more formatting to go before we're done with the bubble chart. To remove the gridlines, select the Format tab and click on Lines from the dropdown menu. Within the Rows tab, select None within the Grid Lines. Repeat this step for the Columns tab. To rename the Title, right-click on the Title and select Edit Title. Enter the following phrase. Then select OK.\",\n           \"Dr Hans Rosling's bubble chart featured a large Year label in the centre. We would need to first create this Year label in a separate worksheet and then bring it in during our dashboard stage. First, create a new worksheet, and rename it Bubble Year. Then drag Month to Pages. Then right-click this and choose Cumulative Year. Again drag Month to the Text Marks. Then again, right-click this and choose Cumulative Year.\",\n           \"To realign to middle centre, click on the Text marks. Select Alignment and choose centre of both Horizontal and Vertical alignments. Next to adjust the font colour and size, right-click on the Year(Month) text mark and choose Format. Then choose font size 72, light grey colour (HTML #e6e6e6) and bold. Then lastly, to remove the title, right-click on the Title and select Hide Title.\",\n           \"Now we would need to create a line graph for Total Trade Value. First create a new worksheet and name it Total. Then drag Country and Month to Columns, and Total Trade Value to Rows. Change the date format by right-clicking on Month and choosing Cumulative Year. Drag Month to Pages and repeat the date format change.\",\n           \"Next choose Circle marks. To add labels, drag Total Trade Value to the Text marks. Since the label is too long, right-click on the Total Trade Value text marks and select Format. Within the Numbers dropdown menu, select Number (Custom). Choose zero decimal places and display units as billions.\",\n           \"Click on Format Borders to remove unnecessary lines on the graph. Within the Sheet tab, select None for both the Column and Row Divider. Then select Format Lines, and select None for Grid Lines within the Rows tab.\",\n           \"To give a nice background shade per country, we will add a reference line. Right-click on the horizontal year axis and choose Add Reference Line. From the top, choose Band and Entire Table as Scope. Then for Band From, choose Constant, and 1st Jan 2011 as its Value. Choose None for Label. Then for Band To, choose Constant, and 1st Dec 2020 as its Value. Choose None for Label. Lastly for the Fill, choose a light grey colour (HTML #f5f5f5).\",\n           \"To hide all unnecessary wordings, we start with right-clicking on the Title and Hide Title. Next we will remove the vertical axis by right-clicking on it and unchecking Show Header. Next we will remove the word Country by right-clicking on it and selecting Hide Field Labels for Columns. Lastly to remove the horizontal axis, right-click on it and uncheck Show Header.\",\n           \"To bold the Country name font, right-click on the country and click on Format. Then choose Bold. Separately, change the colour of the circle by selecting on the Color mark and choosing a dark grey colour (HTML #555555).\",\n           \"To ensure that animation applies to this chart, select the Format tab and click on Animations from the dropdown menu. On the right hand side of the chart, an animation pane would appear. Select a Fast speed, check the Show History box, and then click the down arrow button to reveal more options. Select All Marks To Show History For, and show Both marks and trails. You can then play this animation to see how the circles moves across the years.\",\n           \"Now let's finalise its tooltip. Since our intention is to show more digits in the tooltip than on axis, we will need to duplicate the total trade value. First, right-click on the Total Trade Value and select Duplicate. Then drag this Total Trade Value (copy) to the Tooltip marks, together with Export and Import. Then click on the Tooltip marks. Remove the sentence with Year(Month)(2). And then insert AGG(Total Trade Value(copy)) just beside the sentence of Total Trade Value. Add two more sentences (Total Export and Total Import) and insert their respective fields.\",\n           \"Now we would need to create a similar line chart for both Export and Import. Duplicate the sheet Total and rename it as Export Import. Remove AGG(Total Trade Value) from Rows, as well as the Label and Tooltip marks. Then drag Trade Value to Rows, and Trade Type to the Colour marks.\",\n           \"Next to add labels, drag Trade Value to the Text marks. The labels do not appear, since it is too long. We would need to shrink the labels for it to appear. First right-click on the Trade Value text marks and select Format. Within the Numbers dropdown menu, select Number (Custom). Choose zero decimal places and display units as billions.\",\n           \"Next we will need to have the horizontal annual axis reappear, while all other axis and titles would need to be hidden. First, right-click on the Year(Month) on the Columns and Show Header. To remove the repeated Year of Month, right-click on the horizontal annual axis and select Edit Axis. Within the General tab, remove Year of Month within the Title of Axis Titles. Then to just show 2011 and 2020 within the axis, select the Tick Marks tab. Under the Major Tick Marks, select Fixed and choose 1st Jan 2011 and 9 Years interval.\",\n           \"To finalise the tooltip, we would need to create a calculated field to indicate whether the line is an Export or Import line. We create a calculated field named Custom TT, and enter the following formula: IF ISNULL([Import]) THEN \\\"Export\\\" ELSE \\\"Import\\\" END. Then drag Custom TT to the Tooltip marks. Also, drag Export and Import into the Tooltip marks. Then click on the Tooltip marks. Type Total and then insert ATTR(Custom TT). Press Tab on the keyboard and insert both Sum(Export) and Sum(Import). Then click OK.\",\n           \"Now we're ready to create the dashboard. First, create a new dashboard and name it DataViz2. To ensure that our dashboard fits all screen resolutions, we will need to click on Size and choose Automatic. We will then make use of containers to right size our dashboard. The plan is to have three horizontal containers stacked on top of each other. First, drag the Vertical container into the main window. Then drag the Horizontal container over the lower half of the main window such that it gets highlighted. Next drag the Vertical container over the lowest quarter of the main window such that this too gets highlighted. Now we have three horizontal containers, of which the top and bottom are of a vertical nature and the middle is of a horizontal nature. For easy reference, any box with blue boundaries are containers, whereas any box with green boundaries are charts.\",\n           \"Drag the Bubble worksheet to the middle container. Then drag a Vertical container to the right of this Bubble chart. Drag Total into this Vertical container, and then drag Import Export just below this Total chart.\",\n           \"Within the top container, remove Total Trade Value, Year of Month (2020) and Trade Type. Drag Text box to just below the remaining Year of Month within the top container and enter the following text. Resize to font size 16 and bold. Then drag the Year of Month from the top container to the bottom.\",\n           \"Now let's focus on the top container. Let's drag another text box as the sub-Title below the earlier Title. Enter your short paragraph and enlarge font size to 12. We create a line separator by dragging another text box below this sub-title. Without entering any text, click OK. Reduce its Outer Padding to zero, and select its background colour as dark grey (HTML #555555). We then right-click on this text box and Edit Height to 3 pixels.\",\n           \"For the bottom container, drag a Horizontal container below the Year of Month. To create another line separator, drag a Text Box in between this Year of Month and the Horizontal container. Without entering any text, click OK. Reduce its Outer Padding to zero, and select its background colour as dark grey (HTML #555555). We then right-click on this text box and Edit Height to 3 pixels. Below this line, drag a Text Box. Enter details on the source of the data. Drag another Text Box to the right of this data source text box. Enter the reason for this dashboard. Your eventual bottom container should look like this.\",\n           \"For the middle right container. hide both Titles (Total and Export Import). For the Trade Value chart, we'll remove the country names by right-clicking on it and unchecking Show Header. Then remove the vertical Trade Value axis by right-clicking on it and uncheck Show Header. Your eventual arrangement should look like this picture on the right. To create a Title above the top line chart, drag a Text box just above the Country Names. Enter commentary below the Total Merchandise Trade Volumes and click OK. To create a Title below the bottom line chart, drag another Text box below the years. Enter commentary on the Export and Import Volumes. Colour the words Export and Import according to its respective lines (HTML #4e79a7 and #f28e2b). Your arrangement should now look like the picture on the right.\",\n           \"Now focusing our attention on the animation bar. Drag a Text box above this animation bar to provide instructions to the viewer that the charts can be played with this animation bar. Input the necessary instructions in this text, and click OK. Then right-click on the animation bar. Uncheck Show Title, Show Page Readout and Show History Controls. Your animation bar should now look like this pic.\",\n           \"Lastly, while pressing on the Shift button, drag Bubble Year to just above the Bubble Chart. Remove its Title by right-clicking on it and uncheck Show Header. Then to move it behind the Bubble Chart, right-click on it,  move to Floating ORder and select Send to Back. The reason why you can no longer see the Bubble Year, is due to a white background within the Bubble Chart. Right-click on Bubble chart and select Format. Change the colour of the Fill worksheet to None. Your chart should now look like this.\")\nScreenshots &lt;- c(\"img/1.png\",\n                 \"img/2.png\",\n                 \"img/3.png\",\n                 \"img/4.png\",\n                 \"img/5.png\",\n                 \"img/6.png\",\n                 \"img/7.png\",\n                 \"img/8.png\",\n                 \"img/9.png\",\n                 \"img/10.png\",\n                 \"img/11.png\",\n                 \"img/12.png\",\n                 \"img/13.png\",\n                 \"img/14.png\",\n                 \"img/15.png\",\n                 \"img/16.png\",\n                 \"img/17.png\",\n                 \"img/18.png\",\n                 \"img/19.png\",\n                 \"img/20.png\",\n                 \"img/21.png\",\n                 \"img/22.png\",\n                 \"img/23.png\",\n                 \"img/24.png\",\n                 \"img/25.png\",\n                 \"img/26.png\",\n                 \"img/27.png\",\n                 \"img/28.png\",\n                 \"img/29.png\",\n                 \"img/30.png\",\n                 \"img/31.png\",\n                 \"img/32.png\")\n\ntable3 &lt;- data.frame(No.,Steps,Screenshots)\n\n# Add appropriate rmarkdown tagging\ntable3$Screenshots = sprintf(\"![](%s)\", table3$Screenshots)\n\nknitr::kable(table3, booktabs = TRUE,\n  caption = 'Data Visualisation Steps Using Tableau Desktop')\n\n\n\nData Visualisation Steps Using Tableau Desktop\n\n\n\n\n\n\n\nNo.\nSteps\nScreenshots\n\n\n\n\n1\nLaunch Tableau Desktop by double-clicking on the newly created hyper file. You should get this view. To start creating the first chart (bubble chart), let’s click on Sheet 1 at the bottom.\n\n\n\n2\nWe will need to create a few Calculated Fields. To create the first calculated field, click on the Analysis tab, and choose Calculated Field from the dropdown menu.\n\n\n\n3\nFirst we need to separate the export and import values. We create a calculated field and then rename to Export. Then enter the following formula: IF [Trade Type] = “Export” THEN [Trade Value] END. Ensure that there is a remark indicating that the calculation is valid. Then click OK. To repeat the same step for Import, create a calculated field and then rename to Import. Enter the following formula: IF [Trade Type] = “Import” THEN [Trade Value] END. Thereafter, click OK.\n\n\n\n4\nOur last calculated field for this chart will be to create a total trade value, combining both export and import. Name this calculated field as Total Trade Value. Enter the following formula: SUM([Export])+SUM([Import])\n\n\n\n5\nDrag Export to Columns, Import to Rows, Country to Detail Marks and Total Trade Value to Size Marks.\n\n\n\n6\nTo mimic Hans Rosling’s yearly animation, we will then drag Month to Pages. Then right-click this and choose Cumulative Year.\n\n\n\n7\nTo switch on the animation feature, select the Format tab, and choose Animations from the dropdown menu. Then select On.\n\n\n\n8\nTo fix the axis ranges as the animation plays, first right-click on the vertical Import axis. Choose Edit Axis. Then select Fixed Range and enter 0 and 90,000,000,000. Repeat the same step for the horizontal Export axis by right-clicking on it.\n\n\n\n9\nTo format the bubbles accordingly, we first resize them by clicking on the Size marks and choosing the centre. Then we change the marks type, by clicking on Automatic and choosing Circle. This will allow us to have a fill colour. To colour this circle, click on the Color marks and choose a dark grey tone (HTML #555555) Within the same window, choose 70% opacity. Overlapping circles will now be seen easily, compared to full opacity. Lastly, to include the Country names to each bubble, drag Country to the Label marks.\n\n\n\n10\nA few more formatting to go before we’re done with the bubble chart. To remove the gridlines, select the Format tab and click on Lines from the dropdown menu. Within the Rows tab, select None within the Grid Lines. Repeat this step for the Columns tab. To rename the Title, right-click on the Title and select Edit Title. Enter the following phrase. Then select OK.\n\n\n\n11\nDr Hans Rosling’s bubble chart featured a large Year label in the centre. We would need to first create this Year label in a separate worksheet and then bring it in during our dashboard stage. First, create a new worksheet, and rename it Bubble Year. Then drag Month to Pages. Then right-click this and choose Cumulative Year. Again drag Month to the Text Marks. Then again, right-click this and choose Cumulative Year.\n\n\n\n12\nTo realign to middle centre, click on the Text marks. Select Alignment and choose centre of both Horizontal and Vertical alignments. Next to adjust the font colour and size, right-click on the Year(Month) text mark and choose Format. Then choose font size 72, light grey colour (HTML #e6e6e6) and bold. Then lastly, to remove the title, right-click on the Title and select Hide Title.\n\n\n\n13\nNow we would need to create a line graph for Total Trade Value. First create a new worksheet and name it Total. Then drag Country and Month to Columns, and Total Trade Value to Rows. Change the date format by right-clicking on Month and choosing Cumulative Year. Drag Month to Pages and repeat the date format change.\n\n\n\n14\nNext choose Circle marks. To add labels, drag Total Trade Value to the Text marks. Since the label is too long, right-click on the Total Trade Value text marks and select Format. Within the Numbers dropdown menu, select Number (Custom). Choose zero decimal places and display units as billions.\n\n\n\n15\nClick on Format Borders to remove unnecessary lines on the graph. Within the Sheet tab, select None for both the Column and Row Divider. Then select Format Lines, and select None for Grid Lines within the Rows tab.\n\n\n\n16\nTo give a nice background shade per country, we will add a reference line. Right-click on the horizontal year axis and choose Add Reference Line. From the top, choose Band and Entire Table as Scope. Then for Band From, choose Constant, and 1st Jan 2011 as its Value. Choose None for Label. Then for Band To, choose Constant, and 1st Dec 2020 as its Value. Choose None for Label. Lastly for the Fill, choose a light grey colour (HTML #f5f5f5).\n\n\n\n17\nTo hide all unnecessary wordings, we start with right-clicking on the Title and Hide Title. Next we will remove the vertical axis by right-clicking on it and unchecking Show Header. Next we will remove the word Country by right-clicking on it and selecting Hide Field Labels for Columns. Lastly to remove the horizontal axis, right-click on it and uncheck Show Header.\n\n\n\n18\nTo bold the Country name font, right-click on the country and click on Format. Then choose Bold. Separately, change the colour of the circle by selecting on the Color mark and choosing a dark grey colour (HTML #555555).\n\n\n\n19\nTo ensure that animation applies to this chart, select the Format tab and click on Animations from the dropdown menu. On the right hand side of the chart, an animation pane would appear. Select a Fast speed, check the Show History box, and then click the down arrow button to reveal more options. Select All Marks To Show History For, and show Both marks and trails. You can then play this animation to see how the circles moves across the years.\n\n\n\n20\nNow let’s finalise its tooltip. Since our intention is to show more digits in the tooltip than on axis, we will need to duplicate the total trade value. First, right-click on the Total Trade Value and select Duplicate. Then drag this Total Trade Value (copy) to the Tooltip marks, together with Export and Import. Then click on the Tooltip marks. Remove the sentence with Year(Month)(2). And then insert AGG(Total Trade Value(copy)) just beside the sentence of Total Trade Value. Add two more sentences (Total Export and Total Import) and insert their respective fields.\n\n\n\n21\nNow we would need to create a similar line chart for both Export and Import. Duplicate the sheet Total and rename it as Export Import. Remove AGG(Total Trade Value) from Rows, as well as the Label and Tooltip marks. Then drag Trade Value to Rows, and Trade Type to the Colour marks.\n\n\n\n22\nNext to add labels, drag Trade Value to the Text marks. The labels do not appear, since it is too long. We would need to shrink the labels for it to appear. First right-click on the Trade Value text marks and select Format. Within the Numbers dropdown menu, select Number (Custom). Choose zero decimal places and display units as billions.\n\n\n\n23\nNext we will need to have the horizontal annual axis reappear, while all other axis and titles would need to be hidden. First, right-click on the Year(Month) on the Columns and Show Header. To remove the repeated Year of Month, right-click on the horizontal annual axis and select Edit Axis. Within the General tab, remove Year of Month within the Title of Axis Titles. Then to just show 2011 and 2020 within the axis, select the Tick Marks tab. Under the Major Tick Marks, select Fixed and choose 1st Jan 2011 and 9 Years interval.\n\n\n\n24\nTo finalise the tooltip, we would need to create a calculated field to indicate whether the line is an Export or Import line. We create a calculated field named Custom TT, and enter the following formula: IF ISNULL([Import]) THEN “Export” ELSE “Import” END. Then drag Custom TT to the Tooltip marks. Also, drag Export and Import into the Tooltip marks. Then click on the Tooltip marks. Type Total and then insert ATTR(Custom TT). Press Tab on the keyboard and insert both Sum(Export) and Sum(Import). Then click OK.\n\n\n\n25\nNow we’re ready to create the dashboard. First, create a new dashboard and name it DataViz2. To ensure that our dashboard fits all screen resolutions, we will need to click on Size and choose Automatic. We will then make use of containers to right size our dashboard. The plan is to have three horizontal containers stacked on top of each other. First, drag the Vertical container into the main window. Then drag the Horizontal container over the lower half of the main window such that it gets highlighted. Next drag the Vertical container over the lowest quarter of the main window such that this too gets highlighted. Now we have three horizontal containers, of which the top and bottom are of a vertical nature and the middle is of a horizontal nature. For easy reference, any box with blue boundaries are containers, whereas any box with green boundaries are charts.\n\n\n\n26\nDrag the Bubble worksheet to the middle container. Then drag a Vertical container to the right of this Bubble chart. Drag Total into this Vertical container, and then drag Import Export just below this Total chart.\n\n\n\n27\nWithin the top container, remove Total Trade Value, Year of Month (2020) and Trade Type. Drag Text box to just below the remaining Year of Month within the top container and enter the following text. Resize to font size 16 and bold. Then drag the Year of Month from the top container to the bottom.\n\n\n\n28\nNow let’s focus on the top container. Let’s drag another text box as the sub-Title below the earlier Title. Enter your short paragraph and enlarge font size to 12. We create a line separator by dragging another text box below this sub-title. Without entering any text, click OK. Reduce its Outer Padding to zero, and select its background colour as dark grey (HTML #555555). We then right-click on this text box and Edit Height to 3 pixels.\n\n\n\n29\nFor the bottom container, drag a Horizontal container below the Year of Month. To create another line separator, drag a Text Box in between this Year of Month and the Horizontal container. Without entering any text, click OK. Reduce its Outer Padding to zero, and select its background colour as dark grey (HTML #555555). We then right-click on this text box and Edit Height to 3 pixels. Below this line, drag a Text Box. Enter details on the source of the data. Drag another Text Box to the right of this data source text box. Enter the reason for this dashboard. Your eventual bottom container should look like this.\n\n\n\n30\nFor the middle right container. hide both Titles (Total and Export Import). For the Trade Value chart, we’ll remove the country names by right-clicking on it and unchecking Show Header. Then remove the vertical Trade Value axis by right-clicking on it and uncheck Show Header. Your eventual arrangement should look like this picture on the right. To create a Title above the top line chart, drag a Text box just above the Country Names. Enter commentary below the Total Merchandise Trade Volumes and click OK. To create a Title below the bottom line chart, drag another Text box below the years. Enter commentary on the Export and Import Volumes. Colour the words Export and Import according to its respective lines (HTML #4e79a7 and #f28e2b). Your arrangement should now look like the picture on the right.\n\n\n\n31\nNow focusing our attention on the animation bar. Drag a Text box above this animation bar to provide instructions to the viewer that the charts can be played with this animation bar. Input the necessary instructions in this text, and click OK. Then right-click on the animation bar. Uncheck Show Title, Show Page Readout and Show History Controls. Your animation bar should now look like this pic.\n\n\n\n32\nLastly, while pressing on the Shift button, drag Bubble Year to just above the Bubble Chart. Remove its Title by right-clicking on it and uncheck Show Header. Then to move it behind the Bubble Chart, right-click on it, move to Floating ORder and select Send to Back. The reason why you can no longer see the Bubble Year, is due to a white background within the Bubble Chart. Right-click on Bubble chart and select Format. Change the colour of the Fill worksheet to None. Your chart should now look like this.\n\n\n\n\n\n\n\n\n7.2. Derived Insights\n\n\nCode\nNo. &lt;- c(1:6)\nSteps &lt;- c(\"**Overall:** Between 2011 and 2020, Singapore has generally seen flat or increasing trade with some of its top ten trading partners. 2016 was an exceptional year, where there was a sharp decline in trade across the board. This was due to three main reasons, sudden volatility with the China stock market, OPEC oil production cuts as well as Brexit, where UK had decided to leave the EU bloc. Notable trade recovery from 2016 was with EU, US, China and Taiwan. Conversely, trade with its remaining top ten trading partners, of whom are mainly the Asian markets, have stuttered since.\",\n           \"**Mainland China:** From being Singapore's 2nd biggest trading partner in 2011 with a trade worth of $101 billion, China outgrew Malaysia to be Singapore's top trading partner in 2020 with a whopping trade worth of $136 billion. This was due to a burgeoning two-way trade between Singapore and China: between 2011 and 2020, export and imports grew from $54 to $71 billion and from $48 to $65 billion respectively. The trading growth was aided by the 2009 China-Singapore Free Trade Agreement. At the same time, with the ever-present US-China tensions causing turmoil to global trade,  China has increasingly focused their efforts in strengthening trade links with its neighbours. This benefits Singapore even further, with it being a part of the recent Regional Comprehensive Economic Partnership (RCEP) agreement with China.\",\n           \"**Indonesia:** Trade with Indonesia slumped from a high of $82 billion in 2011 to $49 billion in 2020. This is mainly due to a sharp decline in exports from $57 billion in 2011 to $30 billion in 2020.\",\n           \"**United States:** Trade with United States remained flat between 2011 and 2016 at $73 billion. Thereafter, trade grew rapidly to $102 billion, driven mainly by export trade from Singapore. Despite US's withdrawal from the Trans-Pacific Partnership in 2017, Singapore could still rely on its 2004 Free Trade Agreement with US.\",\n           \"**Malaysia:** Malaysia was Singapore's biggest trading partner, before being overtaken by China in 2013. Apart from a 2016 trade dip, trade with Singapore's closest neighbour, Malaysia, has largely remained constant at $115 billion. 2018 saw the start of a dip with it reaching $103 billion in 2020. The drop coincided with Mahathir's return to power in the 2018 elections.\",\n           \"**Taiwan:** Singapore's trade with Taiwan has been on an upward trend; trade grew from $46 billion in 2011 to $75 billion in 2020. This trade growth was due to greater imports fuelled by the  2014 trade agreement between Singapore and Taiwan, Penghu, Kinmen and Matsu (ASTEP).\")\nScreenshots &lt;- c(\"img/insight1.png\",\n                 \"img/insight2.png\",\n                 \"img/insight3.png\",\n                 \"img/insight4.png\",\n                 \"img/insight5.png\",\n                 \"img/insight6.png\")\n\ntable4 &lt;- data.frame(No.,Steps,Screenshots)\n\n# Add appropriate rmarkdown tagging\ntable4$Screenshots = sprintf(\"![](%s)\", table4$Screenshots)\n\nknitr::kable(table4, booktabs = TRUE,\n  caption = 'Derived Insights')\n\n\n\nDerived Insights\n\n\n\n\n\n\n\nNo.\nSteps\nScreenshots\n\n\n\n\n1\nOverall: Between 2011 and 2020, Singapore has generally seen flat or increasing trade with some of its top ten trading partners. 2016 was an exceptional year, where there was a sharp decline in trade across the board. This was due to three main reasons, sudden volatility with the China stock market, OPEC oil production cuts as well as Brexit, where UK had decided to leave the EU bloc. Notable trade recovery from 2016 was with EU, US, China and Taiwan. Conversely, trade with its remaining top ten trading partners, of whom are mainly the Asian markets, have stuttered since.\n\n\n\n2\nMainland China: From being Singapore’s 2nd biggest trading partner in 2011 with a trade worth of $101 billion, China outgrew Malaysia to be Singapore’s top trading partner in 2020 with a whopping trade worth of $136 billion. This was due to a burgeoning two-way trade between Singapore and China: between 2011 and 2020, export and imports grew from $54 to $71 billion and from $48 to $65 billion respectively. The trading growth was aided by the 2009 China-Singapore Free Trade Agreement. At the same time, with the ever-present US-China tensions causing turmoil to global trade, China has increasingly focused their efforts in strengthening trade links with its neighbours. This benefits Singapore even further, with it being a part of the recent Regional Comprehensive Economic Partnership (RCEP) agreement with China.\n\n\n\n3\nIndonesia: Trade with Indonesia slumped from a high of $82 billion in 2011 to $49 billion in 2020. This is mainly due to a sharp decline in exports from $57 billion in 2011 to $30 billion in 2020.\n\n\n\n4\nUnited States: Trade with United States remained flat between 2011 and 2016 at $73 billion. Thereafter, trade grew rapidly to $102 billion, driven mainly by export trade from Singapore. Despite US’s withdrawal from the Trans-Pacific Partnership in 2017, Singapore could still rely on its 2004 Free Trade Agreement with US.\n\n\n\n5\nMalaysia: Malaysia was Singapore’s biggest trading partner, before being overtaken by China in 2013. Apart from a 2016 trade dip, trade with Singapore’s closest neighbour, Malaysia, has largely remained constant at $115 billion. 2018 saw the start of a dip with it reaching $103 billion in 2020. The drop coincided with Mahathir’s return to power in the 2018 elections.\n\n\n\n6\nTaiwan: Singapore’s trade with Taiwan has been on an upward trend; trade grew from $46 billion in 2011 to $75 billion in 2020. This trade growth was due to greater imports fuelled by the 2014 trade agreement between Singapore and Taiwan, Penghu, Kinmen and Matsu (ASTEP)."
  },
  {
    "objectID": "school/capstone_3/index.html",
    "href": "school/capstone_3/index.html",
    "title": "Uncovering Retail Customer Segmentation From Large Transaction Records: A Nuanced Comparison Of Clustering Algorithms Using Rough Set Reduced Dataset",
    "section": "",
    "text": "Capstone Project Final Report\nCapstone Project Poster\nCapstone Project - SAS Report-Out"
  },
  {
    "objectID": "school/capstone_3/index.html#important-links",
    "href": "school/capstone_3/index.html#important-links",
    "title": "Uncovering Retail Customer Segmentation From Large Transaction Records: A Nuanced Comparison Of Clustering Algorithms Using Rough Set Reduced Dataset",
    "section": "",
    "text": "Capstone Project Final Report\nCapstone Project Poster\nCapstone Project - SAS Report-Out"
  },
  {
    "objectID": "school/capstone_3/index.html#abstract",
    "href": "school/capstone_3/index.html#abstract",
    "title": "Uncovering Retail Customer Segmentation From Large Transaction Records: A Nuanced Comparison Of Clustering Algorithms Using Rough Set Reduced Dataset",
    "section": "Abstract",
    "text": "Abstract\nThis project explores the use of Rough Set’s reduct algorithm on synthesizing a dataset down to its most significant variables, and its impact on clustering outcomes. This capstone project also explores and compares the use of non-traditional clustering algorithms such as fuzzy k-means and Gaussian mixture model, against the more traditional k-means clustering. These algorithms would be tested on a retail transaction dataset as an actual real-world application. These algorithms would also be executed primarily by R Code within a SAS Enterprise Miner 14.1 environment. Findings indicate positive contributions and clearer clustering outcomes, from the use of non-traditional algorithms such as fuzzy k-means, Gaussian mixture, and Rough Set’s reduct. It is hoped that these three algorithms would be a welcome addition to a data analyst’s ever burgeoning toolbox."
  },
  {
    "objectID": "school/capstone_3/index.html#figure",
    "href": "school/capstone_3/index.html#figure",
    "title": "Uncovering Retail Customer Segmentation From Large Transaction Records: A Nuanced Comparison Of Clustering Algorithms Using Rough Set Reduced Dataset",
    "section": "Figure",
    "text": "Figure\nFigure 5: Predicted ODA (foreign aid) across a range of differences from average number of anti-NGO laws in an average country; dark line shows average of 500 draws from posterior distribution.\n\n\n\nFigure 5: Predicted ODA (foreign aid) across a range of differences from average number of anti-NGO laws in an average country; dark line shows average of 500 draws from posterior distribution."
  },
  {
    "objectID": "school/capstone_3/index.html#bibtex-citation",
    "href": "school/capstone_3/index.html#bibtex-citation",
    "title": "Uncovering Retail Customer Segmentation From Large Transaction Records: A Nuanced Comparison Of Clustering Algorithms Using Rough Set Reduced Dataset",
    "section": "BibTeX citation",
    "text": "BibTeX citation\n@unpublished{ChaudhryHeiss:2023,\n    Author = {Suparna Chaudhry and Andrew Heiss},\n    Note = {Working paper},\n    Title = {Are Donors Really Responding? Analyzing the Impact of Global Restrictions on {NGO}s},\n    Year = {2023}}"
  },
  {
    "objectID": "school/capstone_3/index.html#introduction",
    "href": "school/capstone_3/index.html#introduction",
    "title": "Uncovering Retail Customer Segmentation From Large Transaction Records: A Nuanced Comparison Of Clustering Algorithms Using Rough Set Reduced Dataset",
    "section": "1. Introduction",
    "text": "1. Introduction\nCluster analysis is an important data mining technique, which segregates data into separate groups, which themselves possess uniquely distinct attributes. Hierarchical and k-means clustering are two such established clustering methods, with differing approaches towards a similar clustering outcome. Since the introduction of both these established methods, data has grown increasingly complex and imprecise.\n\nThough today’s technology allows ease of data recording more so than ever before, data remains disparately stored i.e., held in often unconnected sources with differing structures. Putting them together into a single homogenous dataset becomes a gargantuan task, and this is even before any proper analytical work is performed. The combined dataset invariably contains inherent inconsistencies (such as imprecisions or incompleteness), that traditional and established clustering methods are not designed for.\n\nTwo different approaches were adopted to resolve these shortcomings. The first approach was to address the limitations of existing clustering methods. Gaussian Mixture Model (GMM) was introduced to specifically address k-means and its limitations, namely its fundamental assumptions of sphericity, equal variance, and hard clustering. Given that its methodology remains akin to k-means, GMM’s application is similarly wide-ranging across diverse fields. That said, analysing such imperfect data was not limited to just tweaking existing clustering methods.\n\nThe other approach was to develop entirely new fields, capable of generating meaningful insights from inconsistent data. One novel research field was fuzzy set theory (Zadeh 1965). All objects are said to belong to all different sets, albeit to varying degrees of membership, similar to ‘soft’ clustering. Conversely, this theory also recognizes objects that have binary set membership ie. that they either belong to a set or not. This is similar to ‘hard’ clustering like k-means. With the success of fuzzy set theory in analysing imperfect data, another novel research field was introduced over a decade later that is equally adaptable to recognizing both soft and hard clustering.\n\nRough Set theory (Pawlak 1982) introduces the concept of approximating hard or crisp sets. These approximations are divided into two regions: the lower approximation (positive region) and the upper approximation (negative region). The former (lower approximation) is similar to ‘hard’ clustering, where the objects are unambiguously and positively assigned to a set. Conversely, the latter (upper approximation) recognizes objects that are possibly assigned to a set, similar to ‘soft’ clustering. As such, both fuzzy and rough sets achieve similar outcomes of identifying both ‘hard’ and ‘soft’ sets or clusters, through their own distinctive methodologies.\n\nWhether it be Rough Set or fuzzy set theory, both novel research fields were created out of a need to analyse inconsistent data in a more robust way. This complementary nature has led to numerous works incorporating both theories together. Computationally efficient on managing uncertainty in large complex data, both theories have had major research and applications in machine learning, data mining and other domains (Pięta and Szmuc 2021). Despite their robust applications in a highly technical research field such as artificial intelligence (AI), there have been surprisingly little applications in a much less complex, real-world field such as customer segmentation.\n\nThus, this study aims to understand and compare clustering approaches, both established and novel, specifically on customer segmentation, to the eventual benefit of the everyday data analyst.\n\nThe flow of this capstone report starts off with its problem statement and objectives. It is then followed by a literature review of these topics. An explanation of the capstone’s six step approach is covered within the research design and methods and is then followed with an extensive analysis and results exploration, as well as a subsequent discussion section. This capstone then rounds off its study with its conclusions and contributions as well as the author’s milestones and reflections."
  },
  {
    "objectID": "school/capstone_3/index.html#problem-statement-and-objectives",
    "href": "school/capstone_3/index.html#problem-statement-and-objectives",
    "title": "Uncovering Retail Customer Segmentation From Large Transaction Records: A Nuanced Comparison Of Clustering Algorithms Using Rough Set Reduced Dataset",
    "section": "2. Problem Statement and Objectives",
    "text": "2. Problem Statement and Objectives\nUsing a real-world dataset, this study explored the impact of Rough Set’s reduct variable selection. Two different forms of dataset are fed into each clustering method: traditional variable manipulation (TVM) and the Rough Set-based variable reduction. TVM was completed during the progress report phase, while the Rough Set-based variable reduction has been expanded in this report.\n\nThis study also examined and compared the clustering outcome of k-means (both traditional and fuzzy) and GMM. The comparison would entail the following:\n\nDetailed the different outcomes\nUncovered merits and shortcomings of each clustering methods\nSuggested situations where each approach would excel\n\nLastly, given SAS scholarship’s requirements, this study used JMP Pro and SAS Enterprise Miner (EM) for data manipulation and clustering respectively. Given that these four algorithms (traditional k-means, fuzzy k-means, GMM and Rough Set) are not available within EM, this study explored the use of R and its statistical packages within EM’s environment.\n\nThe table below shows the clustering methods used on the differing dataset.\n\n\n\n\nCode\n`DATA MANIPULATION & VARIABLE SELECTION METHODS` &lt;- c(\"CLUSTERING METHODS\",\n                                                      \"Traditional k-means (TKM)\",\n                                                      \"Fuzzy k-means (FKM)\",\n                                                      \"Gaussian Mixture Models (GMM)\")\n`Traditional Variable Manipulation (TVM)` &lt;- c(\"\",\n                                               \"TKM-T\",\n                                               \"FKM-T\",\n                                               \"GMM-T\")\n`Rough Set Based Variable Reduction (RSVR)` &lt;- c(\"\",\n                                               \"TKM-R\",\n                                               \"FKM-R\",\n                                               \"GMM-R\")\ntable1 &lt;- tibble(`DATA MANIPULATION & VARIABLE SELECTION METHODS`,\n                     `Traditional Variable Manipulation (TVM)`,\n                     `Rough Set Based Variable Reduction (RSVR)`)\nknitr::kable(table1, \n             booktabs = TRUE,\n             caption = 'Clustering Methods',\n             align = \"lcc\") %&gt;%\n  row_spec(1,\n           bold = T,\n           color = \"white\",\n           background = \"#404040\")\n\n\n\n\n\nClustering Methods\n\n\n\n\nDATA MANIPULATION & VARIABLE SELECTION METHODS\n\n\nTraditional Variable Manipulation (TVM)\n\n\nRough Set Based Variable Reduction (RSVR)\n\n\n\n\n\n\nCLUSTERING METHODS\n\n\n\n\n\n\n\n\nTraditional k-means (TKM)\n\n\nTKM-T\n\n\nTKM-R\n\n\n\n\nFuzzy k-means (FKM)\n\n\nFKM-T\n\n\nFKM-R\n\n\n\n\nGaussian Mixture Models (GMM)\n\n\nGMM-T\n\n\nGMM-R\n\n\n\n\n\n(A Breakdown of the Clustering Methods and Variable Selection Techniques Involved)\n\n\n\nIn summary, this study aims to understand whether either GMM, Rough Set-based variable reduction, fuzzy k-means, or all of them would be a welcomed complement to current established clustering methods such as k-means clustering."
  },
  {
    "objectID": "school/capstone_3/index.html#literature-review",
    "href": "school/capstone_3/index.html#literature-review",
    "title": "Uncovering Retail Customer Segmentation From Large Transaction Records: A Nuanced Comparison Of Clustering Algorithms Using Rough Set Reduced Dataset",
    "section": "3. Literature Review",
    "text": "3. Literature Review\nCormack (1971) explains that a traditional clustering approach assigns individual objects into initially unclear classes, where all entities within a class share similarities with one another. The converse is also true. Objects outside of their assigned classes are highly dissimilar to those within their respective classes. A well-regarded clustering methodology is the k-means clustering. Being an established non-hierarchical clustering approach, objects are partitioned such that the squared Euclidean distance between each object and the centroid of the class it resides in, is as small as the centroids of the other classes (Steinley 2006). Garla et al. (2012) and Steinley (2006) explains the iterative k-means algorithm in detail:\n\nSpecify the number of clusters or classes known as k. This number is chosen a priori ie. based on theoretical deduction.\nRandomly select k cluster centres, or initial seeds in the data space, as defined by P-dimensional vectors \\((S_{1}^{(k)},…, S_{P}^{(k)})\\), for 1 ≤ k ≤ K\nAllocate objects to clusters, where the squared Euclidean distance, \\(d^2(i,k)\\), between ith object and the kth seed vector, is at its minimum\n\n\n\n\n\n\n\\[\nd^2(i,k) = \\displaystyle\\sum_{j=1}^P(x_{ij}-S_j^{(k)})^2\n\\]\n\n\n\n( 1 )\n\n\n\nRe-compute new cluster centroids by:\n\n\nCalculate the centroid value for the jth variable in cluster \\(C_k\\)\n\n\n\n\n\n\n\\[\n\\bar{x}_j^{(k)} = \\frac{1}{n_k}\\displaystyle\\sum_{i\\epsilon{C_k}}x_{ij}\n\\]\n\n\n\n( 2 )\n\n\n\nCalculate complete centroid vector for cluster \\(C_k\\) by averaging all centroid values within\n\n\n\n\n\n\n\\[\n\\bar{x}^{(k)} = (\\bar{x}_1^{(k)},\\bar{x}_2^{(k)},...,\\bar{x}_P^{(k)})'.\n\\]\n\n\n\n( 3 )\n\n\n\nRepeat steps 3. and 4. Above until convergence criterion is achieved i.e., the centroids do not change\n\n\n3.1 Gaussian Mixture Model\nRaykov et al. (2016) explains that k-means suffers from sphericity, outliers, and geometric closeness. Sphericity assumes that all clusters, within the dataset, are spherical in nature with the same radius. Conversely, given that k-means heavily relies on linear distance, any outliers that are unusually far away from the rest of the points within the cluster would impair the results of the k-means. Lastly, k-means implicitly assumes each cluster within the dataset has the same volume and fails to take into consideration clusters with differing geometric closeness and densities. These three limitations prevent k-means from clustering improper data in a meaningful way. To overcome this, this capstone is influenced by the work of Garla et al. (2012). There, a comparison was made between k-means, normal mixtures, and probabilistic-D clustering for a B2B segmentation study on customers’ perceptions. Given that normal mixtures model, otherwise known as Gaussian Mixture Model (GMM), is not a common clustering method, this capstone aims to explore this, on top of the traditional and fuzzy k-means.\n\nGaussian Mixture Model (GMM) was introduced to overcome the k-means assumptions of hard clustering, sphericity, outliers, and geometric closeness. It is also suited for an incomplete dataset (Melchior and Goulding 2018). Patel and Kushwaha (2020) denotes each Gaussian or Normal distribution as a continuous probability distribution as follows:\n\n\n\n\n\n\n\\[\nN(X|\\mu,\\sum)=\\frac{1}{(2\\pi)^\\frac{D}{2}\\sqrt{|\\sum|}}exp\\left\\{-\\frac{(X-\\mu)^T\\sum^{-1}(X-\\mu)}{2}\\right\\}\n\\]\n\n\n\n( 4 )\n\n\n\n\nwhere\n\n\n\n\\(\\mu\\) is a D-dimensional mean vector\n\\(\\sum\\) is a D x D covariance matrix, describing the shape of the Gaussian\n\\(|\\sum|\\) denotes the determinant of \\(\\sum\\)\n\n\n\n\n\n\nEach dataset is deemed to contain several different Gaussian (normal) distributions on the same linear plane, with each distribution being regarded as one cluster:\n\n\n\n\n\n\\[\np(X)=\\displaystyle\\sum^K_{k=1}\\pi_kN(X|\\mu_k,{\\sum}_k)\n\\]\n\n\n\n( 5 )\n\n\n\n\nwhere\n\n\n\nK is the number of components or clusters in the mixture model\n\\(\\pi_k\\)π_k denotes the mixing coefficient, giving an estimate of the density of each Gaussian component\n\\(N(X|\\mu_k,{\\sum}k)\\) denotes the Gaussian density as per (5), with mean \\(\\mu_k\\), covariance \\({\\sum}_k\\) and the mixing coefficient \\(\\pi_k\\)\n\n\n\n\n\n\nUnlike traditional k-means which assigns each object to only one cluster, GMM assigns each object a probability of how closely it fits to each distribution. It thus has a ‘fit’ score relating to each cluster and would naturally be included in the cluster with the highest score. While it’s akin to a k-means clustering method in that it uses the mean values, GMM goes one step further by incorporating variance into the methodology. This allows it to identify and separate clusters that are close to each other, overcoming some of its limitations.\n\n\n3.2 Fuzzy Clustering\nIn the traditional deterministic form of k-means clustering, it is impossible to assess the impact of the imprecise data (Chaudhuri and Bhowmik 1998). Given that k-means is a hard clustering methodology, objects are either assigned to a particular set or not. This becomes increasingly difficult with imprecise data, where there is every possibility that an object may not just belong to a single set. For k-means to adopt a soft clustering approach, it would need to be paired with an additional concept. As such, there have been numerous studies to introduce fuzzy logic into the traditional k-means clustering.\n\nBuilding on classical set theory where objects in a set either belong to a set or otherwise, Zadeh (1965) theorised fuzzy sets, where these same objects have degrees of membership to a set. Granted imprecision within a data space, Bezdek (1981) explains the use of fuzzy k-means clustering to ‘soft’ classify its observations into k clusters through a minimisation algorithm. This allows the traditional k-means clustering method to adopt a more flexible approach, especially where imprecise data would generally not fit into a ‘hard’ cluster. Steinley (2006) expresses the fuzzy k-means as follows:\n\n\n\n\n\n\n\\[\nFuzzy\\quad k-means = \\displaystyle\\sum^N_{i=1}\\displaystyle\\sum^K_{k=1}w^r_{ik}d^2(i,k)\n\\]\n\n\n\n( 6 )\n\n\n\n\nwhere\n\n\n\n\\(w_{ik}\\) indicates membership values or the degree to which object i belongs to cluster \\(k\\).\nall \\(w_{ik}\\) sums to unity or 1\nr is a hyper-parameter that controls the fuzziness of the solution and is always greater than or equal to 1. The higher the r value is, the ‘fuzzier’ the solution\n\\(d^2(i,k)\\) is the squared Euclidean distance between the ith point and the kth seed vector\n\n\n\n\n\n\nOn reflection, while k-means clustering also tries to minimise (4), its \\(w_{ik}\\) or membership values are only binary in nature. Separately, while r takes values between 1 and \\(\\infty\\) ie. \\(r\\in \\{1, ∞\\}\\), a larger r results in fuzzier clusters. In the case of m → 1, the membership, \\(w_{ik}\\), converses to either 0 or 1, and Fuzzy k-means would match the behaviour of traditional k-means.\n\n\n3.3 Rough Set Theory\nThis study thus far has considered traditional k-means, fuzzy k-means and GMM as part of the clustering concepts to explore. To efficiently discover clusters, feature selection is an important pre-clustering process. The task of finding important and significant variables within the unsupervised dataset allows for highly efficient processing, without eroding or compromising the overall quality of the dataset. A common feature selection would be to identify non-collinear variables for subsequent k-means clustering. As an alternative feature selection, this capstone explored Rough Set theory’s reduct algorithm.\n\nThe Rough Set theory begins by separating these same objects into indiscernible classes, which can then be constructed as clusters themselves (Nayak et al. 2012). Dagdia et al. (2020) expounds the theory by starting with a tuple \\(S=(U,A)\\), where \\(U=\\{u_1,u_2,…,u_N\\}\\) is a non-empty finite set of N objects aptly named as universe and A is a non-empty set of \\((n+k)\\) attributes. Feature set \\(A=C \\cup D\\) can broken down to two subsets: conditional feature set, \\(C=\\{a_1,a_2,…,a_n\\}\\), is made up of n conditional attributes or predictors, while decision attribute, \\(D=\\{d_1,d_2,…,d_k\\}\\), is made up of k decision attributes or output variables. Each feature \\(a \\in A\\) is defined with a set of possible values \\(V_a\\), or known as the domain of a. For each non-empty subset of attributes \\(P \\subset C\\), a binary relation named P-indiscernibility relation, is a key tenet of the Rough Set theory and is illustrated as follows:\n\n\n\n\n\n\n\\[\nIND(P) = \\{(u_1,u_2) \\in U x U: \\forall_a \\in P, a(u_1) = a(U_2) \\}\n\\]\n\n\n( 7 )\n\n\n\n\nwhere\n\n\n\\(a(u_i)\\) refers to the value of attribute a for the instance \\(u_i\\)\n\n\n\n\n\n\nIf \\((u_1,u_2)∈IND(P)\\), then \\(u_1\\) is indiscernible from \\(u_2\\) by the attributes P. The induced set of equivalence classes \\([u]p\\) ,where \\(u \\in U\\), separates U into different parts or blocks denoted as U/P. The Rough Set theory approximates a target set of objects \\(X \\subseteq U\\) using the equivalence classes induced using P as follows:\n\n\n\n\n\n\n\\[\n\\underline{P}(X)=\\{u:[u]_p \\subseteq X\\}. (lower \\quad approximation)\n\\]\n\n\n\n( 8 )\n\n\n\n\nwhere\n\n\n\\(\\underline{P}(X)\\) denotes the P-lower approximations of X, or positively and certainly assigned to set X\n\n\n\n\n\n\n\n\n\n\n\\[\n\\overline{P}(X)=\\{u:[u]_p \\cap X\\neq 0 \\}. (upper \\quad approximation)\n\\]\n\n\n\n( 9 )\n\n\n\n\nwhere\n\n\n\\(\\overline{P}(X)\\) denotes the P-upper approximations of X, or positively and possibly assigned to set X\n\n\n\n\n\n\nThe difference between the two approximations is called the boundary region and is made up of the set of instances that are not positively, but possibly classified in a certain way. X is said to be a crisp set if the boundary region is an empty set where \\(\\underline{P} (X)= \\overline{P}(X)\\). Failure to meet this condition would yield X as a rough set.\n\nA dependency measure is defined to compare subsets of attributes. For example, the dependency measure of an attribute subset Q on another attribute subset P is given as:\n\n\n\n\n\n\n\\[\n_{\\gamma P}(Q)=\\frac{|POS_P(Q)|}{|U|}\n\\]\n\n\n\n( 10 )\n\n\n\n\nwhere\n\n\n\\(0≤ _{\\gamma P} (Q)≤1\\), \\(\\cup\\) denotes the union operation, \\(||\\) denotes the set cardinality, and \\(POS_P (Q)\\) is defined as:\n\n\n\n\n\n\n\n\n\n\n\\[\nPOS_P(Q) = \\bigcup_{X\\in[u]_Q}\\underline P(X)\n\\]\n\n\n\n( 11 )\n\n\n\\(POS_P(Q)\\) is the positive region of subset Q in relation to subset P, and is the set of all objects of U that can be uniquely classified to blocks of the partition \\([u]_Q\\), by means of P. As \\(_{\\gamma P}(Q)\\) nears to 1, the greater Q depends on P.\n\nThe additional benefit of Rough Set theory is the reduct concept. It is the ability to reduce the number of attributes that objects possess in such a way that it remains distinguishable as before. A subset \\(R\\subseteq C\\) is considered a reduct of C where:\n\n\n\n\n\n\\[\n_{\\gamma R}(D)=_{\\gamma C}(D)\n\\]\n\n\n\n( 12 )\n\n\nand there is no \\(R'\\subseteq R\\) such that \\(_{\\gamma R}(D)=_{\\gamma C}(D)\\).\n\nThese reduct concept, distilling only ‘significant’ attributes, remain a vital part of the cluster generation. Recent research applications of Rough Set theory mainly focus on attribute reduction, rule acquisition and intelligent algorithm (Zhang, Xie, and Wang 2016). Conversely, clustering-specific Rough Set-based research has been few. It was this capstone’s initial direction to venture into this somewhat uncharted field. Upadhyaya, Arora, and Jain (2008) and Singh and Mandal (2017) applied rough set’s reduct algorithm on the attributes within each cluster, post clustering, to reduce each cluster’s insignificant attributes and retain its significant attributes. With this method, each cluster would retain attributes pertinent to its unique cluster characteristics. While this capstone acknowledges this reduct approach as a possible direction to take, nonetheless this capstone’s eventual direction slightly differs. Rather than incorporate the reduct algorithm post clustering, this capstone aims to examine the effects of incorporating a reduct algorithm pre-clustering. By comparing a ‘normal’ dataset with another dataset post reduct, this capstone aims to examine and compare its effects on cluster accuracy."
  },
  {
    "objectID": "school/capstone_3/index.html#research-design-and-methods",
    "href": "school/capstone_3/index.html#research-design-and-methods",
    "title": "Uncovering Retail Customer Segmentation From Large Transaction Records: A Nuanced Comparison Of Clustering Algorithms Using Rough Set Reduced Dataset",
    "section": "4. Research Design and Methods",
    "text": "4. Research Design and Methods\nThis study’s approach are broadly classified into the following six sequential steps, of which steps 3 to 6 are encapsulated in Figure 1 below.\n\nOverall Intent of Study\nOriginal Data Overview and Table Selection\nData Manipulation\nPre-Clustering\nEmploying Relevant Clustering Methods\nClustering Result Comparison\n\n\nFigure 1 below shows the workflow steps beginning from data manipulation to pre-clustering techniques to actual clustering applications, before finally comparing the cluster output against its original household data.\n\n\n\n\n\nFigure 1: Capstone Workflow\n\n\nTo add, the completion of steps 1 to 3 i.e., overall intent of study, original data overview and table selection as well as data manipulation were covered in the earlier progress report. At the same time, the traditional variable manipulation part of step 4 was similarly completed. Excluding the household ID variable, this traditional variable manipulation part identified 16 non-collinear variables from the overall 28 variables as per Figure 2. These non-collinear variables are crucial assumptions within the k-means algorithm.\n\n\n\n\n\nFigure 2: 16 Non-Collinear Variables Kept as Inputs For k-Means; Rest Are Rejected\n\n\nThus, this final report would continue from the Rough Set variable reduction part of step 4.\n\n\n\n\n\nFigure 3: Capstone Workflow with emphasis on current step\n\n\n\n4.1 Pre-Clustering (Rough Set Variable Reduction)\nThis sub-section implements Rough Set’s reduct feature selection algorithm onto the base dataset. Though Rough Set algorithm exists within a multitude of programming language packages (R, C++, and Java) (Riza et al. 2014), Enterprise Miner does not include Rough Set reduct within its standard software. As a workaround, Enterprise Miner has an Open-Source Integration (OSI) Node that allows for both R and Python scripts to run within the EM environment. Since Rough Set already exists as an R package, using this node allows for Rough Set to be run within EM. Similarly, R packages of k-means, fuzzy k-means and Gaussian Mixture Models algorithms would also be executed with the Open-Source Integration node in similar fashion.\n\nTo ensure successful R integration with EM, compatibility of both versions need to be carefully considered. This capstone uses EM version 14.1. As such, SAS recommend 64-Bit R versions between 2.13.0 and 3.2.5. At the time of this capstone, the latest stable R version is 4.2.2. Since EM version 14.1 only recognizes R version 3.2.5 and nothing later, it must be said that certain R packages, considered useful to this capstone, may have limited usage on this earlier R version, or may even find itself completely unusable. Nonetheless, it is this capstone’s intention to find workable alternatives to ultimately deliver meaningful analysis and insights. Lastly, for housekeeping purposes and to ensure a streamlined and coherent report, all R codes within this capstone will be included in the appendix.\n\nAs seen in Figure 4 below, the Open-Source Integration Node is attached to the Data_All Node containing the base dataset of 28 input variables and 1 ID variable. The Rough Set reduct R code, using the RoughSets R package, is then entered within the Code Editor of the OSI Node. The Rough Set reduct R code used within this node closely follows Riza et al. (2014).\n\n\n\n\n\nFigure 4: 16 Attaching Open-Source Integration Node to Data_All Node\n\n\nThe base dataset is first converted as a decision table. Unlike R’s usual dataframe or tibble type, a decision table has attribute descriptions, types of attributes and an index of the decision attribute. The household_ID is classified as an index of the decision attribute, with a nominal property. As a pre-processing step, this decision table is then discretized or converted from real-valued attributes into nominal attributes. This step maintains the discernibility between objects in information systems. Of the many different discretization approaches, there is one option for either global (discretizing values over the whole continuous attribute) or local (discretizing values over localized regions). @Riza et al. (2014) used global discretization, and thus this capstone followed suit. The eventual discretized values are then applied back to the decision table to generate a new decision table. Though there are other methods (quickreduct.rst, quickreduct.frst), feature selection is performed using the greedy.heuristic.superreduct method. The R code detailing this whole Rough Set reduct process is included in Appendix 5, and 6.\n\nFigure 5 shows the Rough Set reduct output. Out of 28 possible input variables to choose from, the R code within the OSI Node chose only one input variable: M_DISCOUNT_USED_PER_ACTIVE_WEEKEND. This meant that the amount of discounts on an active weekend was THE attribute that is akin to the entire set of shopping consumer behaviour attributes.\n\n\n\n\n\nFigure 5: Rough Set Reduct Output from Data_All\n\n\nUnfortunately, given that clustering algorithms require a minimum of two variables, at least one other input is required. To meet this requirement, this capstone then removed the ‘M_DISCOUNT_USED_PER_ACTIVE_WEEKEND’ input variable from the Data_All Node. Since the OSI Node takes in the whole data node regardless of rejected variables, filtering out the ‘M_DISCOUNT_USED_PER_ACTIVE_WEEKEND’ input variable would need to be done within the OSI Node.\n\n\n\n\n\nFigure 6: 16 Attaching Open-Source Integration Node (Removed One Input and Redid Reduct) to Data_All Node\n\n\nFigure 6 shows the second run of Rough Set reduct output. Out of 27 possible input variables to choose from, the R code within the OSI Node chose only one input variable as before. This time, the input variable ‘M_DISCOUNT_USED_PER_ACTIVE_WEEKDAY’ was chosen. Thus, the reduct dataset (originally from the base dataset) would now comprise of one ID variable (household_ID), and two input variables (M_DISCOUNT_USED_PER_ACTIVE_WEEKDAY and M_DISCOUNT_USED_PER_ACTIVE_WEEKEND).\n\n\n\n\n\nFigure 7: Rough Set Reduct Output from Data_All(Excluding M_DISCOUNT_USED_PER_ACTIVE_WEEKEND)\n\n\n\n\n4.2 Employing Relevant Clustering Methods – k-Means Clustering\nAs mentioned in the progress report, four out of the eight clustering outputs are generated from k-means clustering. The outputs differ due to:\n\nType of Dataset Used:\n\n16 non-collinear input variables and 1 ID variable\n2 Rough Set reduct variables and 1 ID variable\n\nWhether dataset was normalized (using the Transform Variables Node) before k-means clustering was performed\n\n\n\n\n\n\nFigure 8: Capstone Workflow with emphasis on current step\n\n\nIt’s worth noting that since k-means is a distance-based algorithm, its input variables need to be scaled in a similar fashion. This scaling will be done in the R code. This scale function, already within base R, centres and scales the values, using either standard deviation or root mean square.\n\nSeparately, the clustering algorithm in the Cluster Node within SAS EM is closer to hierarchical clustering, than k-means clustering. The following details were lifted from SAS EM 14.1’s Reference Help, with emphasis on the hierarchical clustering.\n\n\n::: {.grid .style=“background-color: #F2F2F2”} “The Automatic setting (default) configures SAS Enterprise Miner to automatically determine the optimum number of clusters to create.\n\nWhen the Automatic setting is selected, the value in the Maximum Number of Clusters property in the Number of Clusters section is not used to set the maximum number of clusters. Instead, SAS Enterprise Miner first makes a preliminary clustering pass, beginning with the number of clusters that is specified as the Preliminary Maximum value in the Selection Criterion properties.\n\nAfter the preliminary pass completes, the multivariate means of the clusters are used as inputs for a second pass that uses agglomerative, hierarchical algorithms to combine and reduce the number of clusters. Then, the smallest number of clusters that meets all four of the following criteria is selected.” :::\nSince k-means clustering is not found within SAS EM, similar to Rough Set, this section will focus on using R code within OSI to perform k-means clustering. R already has k-means clustering built into its base stats package. To determine the optimal number of k clusters, this capstone would use the silhouette coefficient, with values between -1 and +1. It is a measure of how similar a data-point is within-cluster (cohesion), compared to other clusters (separation). Rousseeuw (1987) first defines a value of s(i) in the case of dissimilarities for object i. Object i is then assigned to cluster A. When cluster A has other objects apart from i, the following can be computed:\n\n\n\n\n\n\n\na(i) = average dissimilarity distance of i to all other objects within cluster A\n\n\n( 13 )\n\n\n\nConsidering another cluster C, which is different from cluster A, the following can be computed:\n\n\n\n\n\n\n\nd(i, C) = average dissimilarity distance of i to all objects of cluster C\n\n\n( 14 )\n\n\n\nAfter computing d(i, C) for all clusters \\(C ≠ A\\), the smallest of these numbers are then selected, and denoted by:\n\n\n\n\n\n\n\nb(i) = minimum d(i, C), where \\(C ≠ A\\)\n\n\n( 15 )\n\n\n\nThus, the number s(i) is obtained by combining a(i) and b(i) as follows:\n\n\n\n\n\n\n\n\\[\ns(i) =\n  \\begin{cases}\n   1 - b(i)/a(i) & \\quad \\text{if a(i) &lt; b(i)}\\\\\n   0             & \\quad \\text{if a(i) = b(i)  ,  or}\\\\\n   b(i)/a(i)-1   & \\quad \\text{if a(i) &gt; b(i)}\n  \\end{cases}\n\\]\n\n\n\n\n( 16 )\n\n\n\n\n\n\n\n\\[\ns(i) = \\frac{b(i) - a(i)}{max\\{a(i), b(i)\\}}\n\\]\n\n\n\n( 17 )\n\n\nA silhouette coefficient close to +1 shows that the object i has been assigned to an appropriate cluster, while a silhouette coefficient close to -1 shows that the same object has been misclassified to its current cluster. A silhouette coefficient close to zero would mean that object i lies equally far from two clusters.\n\nThis section covers both the TKM-N-T and TKM-A-T outputs, using the dataset with 16 non-collinear input variables and one ID variable, as shown in the EM diagram below.\n\n\n\n\n\nFigure 9: EM Diagram for TKM-N-T and TKM-A-T outputs (Using 16 non-collinear inputs and 1 ID variable)\n\n\nTKM-N-T (Traditional k-means clustering on normalized form of 16 non-collinear inputs) First, the optimal number of clusters needs to be computed. Information on the transformation of these 16 non-collinear inputs, within the Transform Variables Node, is included in Appendix 3. These are then ingested into the OSI Node called ‘OSI11-TKM-N-T-Explore.’ The number 12 here refers to the node number within the EM Workspace and allows for ease of retrieval of .csv and plot files. Setting the seed value to 1234, and then calling the ‘cluster’ and ‘purrr’ packages, a function to record the silhouette index of a particular cluster k is created, and then looped through cluster counts 2 to 50. This silhouette index is then plotted within EM, as seen below.\n\n\n\n\n\n\n\nFigure 10: Silhouette Index for TKM-N-T output\n\n\n\n\nThe silhouette index starts out strongest at k = 2, and then goes downwards sharply by k = 6, before gradually decreasing by k = 50. It’s worth noting that the silhouette index within the graph is closer to zero than to 1. This might mean that the objects within the dataset are more evenly spread out, with little cluster concentrations. For now, we will continue to regard k = 2 as its most optimized cluster count.\n\nAnother OSI node, called ‘OSI12-TKM-N-T-2-Clusters,’ is now attached to the same Transform Variable Node. This time, instead of exploring, we will generate the k = 2 cluster output. Apart from the same R code above, a plot is created to compare the histograms of each cluster against overall for all variables. Analysis of these results will be covered in a later section."
  }
]